<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Xeoner&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Xeoner&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Xeoner&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="some description"><meta property="og:type" content="blog"><meta property="og:title" content="Xeoner&#039;s Blog"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Xeoner&#039;s Blog"><meta property="og:description" content="some description"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="Xeoner"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Xeoner's Blog","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Xeoner"},"publisher":{"@type":"Organization","name":"Xeoner's Blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"some description"}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link data-pjax rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-M6KHR911M6" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-M6KHR911M6');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Xeoner&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-10-06T01:36:26.000Z" title="2023. 10. 6. 오전 10:36:26">2023-10-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-06T01:56:58.308Z" title="2023. 10. 6. 오전 10:56:58">2023-10-06</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Game/">Game</a></span><span class="level-item">a minute read (About 172 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/10/06/music2motion-project/">music2motion_project</a></p><div class="content"><h2 id="게임에-딥러닝-적용하기-1"><a href="#게임에-딥러닝-적용하기-1" class="headerlink" title="게임에 딥러닝 적용하기 1"></a>게임에 딥러닝 적용하기 1</h2><h3 id="데이터-준비"><a href="#데이터-준비" class="headerlink" title="데이터 준비"></a>데이터 준비</h3><p>  게임에 사용될 데이터 소개<br>  음악이 플레이되는 동안 디바이스를 리듬에 맞춰 신나게 흔드는 데이터  </p>
<ul>
<li>샘플러 앱 모션 입력값 Setter 에 임계치 이상의 값이 들어오는 로직 적용<br>  <a target="_blank" rel="noopener" href="https://github.com/melody-sigma/LOLMusiK/blob/7604ec093619e3e68a46e9814c89f1ba5baf2237/Laputa1_1.csv">csv data link</a>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">TimeStep, accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z, gyro_w</span><br><span class="line">2.047982, -0.716, -0.788, -0.106, -0.247, 0.276, -0.038, -0.928</span><br><span class="line">2.581315, -0.948, 1.532, -1.504, 0.298, 0.315, 0.463, -0.773</span><br><span class="line">2.986644, -1.214, 2.212, -0.735, 0.465, 0.346, 0.701, -0.416</span><br><span class="line">3.071995, -0.765, 0.496, -0.480, 0.461, 0.274, 0.685, -0.493</span><br><span class="line">3.455986, -0.756, 0.031, -0.393, 0.433, 0.235, 0.704, -0.511</span><br><span class="line">4.799977, -1.122, -1.067, -0.935, -0.256, -0.328, -0.905, 0.087</span><br><span class="line">5.141315, -3.009, -0.402, -2.298, -0.220, -0.489, -0.843, -0.032</span><br><span class="line">6.655986, 0.034, -0.171, 0.688, -0.239, -0.668, -0.694, 0.121</span><br><span class="line">9.002653, -0.078, 0.931, 3.289, -0.315, -0.892, -0.242, -0.218</span><br><span class="line">9.066644, -1.996, -0.112, -0.943, -0.564, -0.627, -0.443, -0.304</span><br><span class="line">10.19732, 0.473, -0.018, 0.035, -0.081, -0.661, -0.726, -0.172</span><br><span class="line">10.32533, -0.627, -0.682, -1.279, -0.292, -0.586, -0.740, 0.156</span><br><span class="line">10.79465, -1.628, -0.304, -0.207, -0.489, -0.731, -0.463, -0.112</span><br><span class="line">11.56265, -0.507, 0.267, 0.217, -0.067, -0.658, -0.697, -0.277</span><br><span class="line">11.64798, -0.146, 1.522, 0.850, -0.206, -0.642, -0.736, -0.063</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="모션-데이터를-게임에-적용하기"><a href="#모션-데이터를-게임에-적용하기" class="headerlink" title="모션 데이터를 게임에 적용하기"></a>모션 데이터를 게임에 적용하기</h3><p>  유니티 스크립트로 파일 로더를 만들거나<br>  언리얼 블루프린트를 이용하여 게임 그래픽에 적용<br>  to be continued…</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-15T23:28:46.000Z" title="2023. 3. 16. 오전 8:28:46">2023-03-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-05-27T22:25:59.924Z" title="2023. 5. 28. 오전 7:25:59">2023-05-28</time></span><span class="level-item"><a class="link-muted" href="/categories/Game/">Game</a></span><span class="level-item">4 minutes read (About 525 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/16/%EA%B2%8C%EC%9E%84%EA%B0%9C%EB%B0%9C-%EC%9D%BC%EA%B8%B02/">게임개발-일기2</a></p><div class="content"><h2 id="게임-개발-2번째-포스트"><a href="#게임-개발-2번째-포스트" class="headerlink" title="게임 개발 2번째 포스트"></a>게임 개발 2번째 포스트</h2><p>  데이터 샘플링 방법에 관한 스터디<br>  <a target="_blank" rel="noopener" href="https://forum.unity.com/threads/solved-how-to-sample-transform-position-with-a-precision-of-a-frequency-of-60hz-with-unity3d.442160/">https://forum.unity.com/threads/solved-how-to-sample-transform-position-with-a-precision-of-a-frequency-of-60hz-with-unity3d.442160/</a></p>
<p>  간단한 방법 : FixedUpdate 설정<br>  약간 더 정확한 방법 : Realtime 타이머를 만들어서 Multi-Thread locking을 사용(타이머와 샘플링 로직을 스레드별로 분리) </p>
<p>  두둥~ 드. 디. 어 데이터 추출!!<br>  앗…!</p>
<h3 id="원래-데이터"><a href="#원래-데이터" class="headerlink" title="원래 데이터"></a>원래 데이터</h3><p>  <img src="/%EA%B2%8C%EC%9E%84%EA%B0%9C%EB%B0%9C-%EC%9D%BC%EA%B8%B02/%EC%95%88%EB%93%9C%EC%84%BC%EC%84%9C%EC%83%98%ED%94%8C%EB%A7%81_%EC%A4%91%EB%B3%B5.png" alt="샘플링 데이터">  </p>
<ul>
<li>하지만 뭔가 비약했었다. 샘플링한 디바이스 센서 값들이 중복이 있었다.<br>  그냥 Input.accel.y 단순히 이렇게 샘플링하면 초당 프레임 레이트가 저하되고 데이터가<br>  밀리거나 싱크가 어긋나서 많은 데이터 중복이 발생한다.  <blockquote>
<p>일반적인 텍스트 크롤링보다 구현 난이도 있는 편.</p>
</blockquote>
</li>
</ul>
<h3 id="개선된-데이터"><a href="#개선된-데이터" class="headerlink" title="개선된 데이터"></a>개선된 데이터</h3><p>  <img src="/%EA%B2%8C%EC%9E%84%EA%B0%9C%EB%B0%9C-%EC%9D%BC%EA%B8%B02/%EC%95%88%EB%93%9C%EB%A1%9C%EC%9D%B4%EB%93%9C_%EC%84%BC%EC%84%9C%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%83%98%ED%94%8C%EB%A7%81.png" alt="샘플링 데이터2">  </p>
<ul>
<li>멀티스레드 like한 코루틴으로 타이머 로직을 만들어 샘플링한 데이터다. 약간 보기 좋고 깔끔하다.<br>  다행히 데이터 중복현상도 거의 없어졌다.  유니티 프로젝트 셋팅만 건들면 더 좋아질 것 같다.  <blockquote>
<p>시간은 Audio Source 속성의 playtime이 아닌 TimeStep 개념으로 접근해야 데이터 샘플링이 더욱 수월하다.</p>
</blockquote>
</li>
</ul>
<h3 id="과연-어디에-쓰이는-데이터일까"><a href="#과연-어디에-쓰이는-데이터일까" class="headerlink" title="과연 어디에 쓰이는 데이터일까?"></a>과연 어디에 쓰이는 데이터일까?</h3><ul>
<li>Anomaly Event Detection, Game Play Pattern generation(Audio Dynamics …)에도 쓰이는 데이터이다.<br>  이벤트 감지에 활용?!.<br>  <img src="/%EA%B2%8C%EC%9E%84%EA%B0%9C%EB%B0%9C-%EC%9D%BC%EA%B8%B02/%EC%84%BC%EC%84%9C%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%83%98%ED%94%8C%EB%A7%81.png" alt="샘플링 데이터3"><br>  <img src="/%EA%B2%8C%EC%9E%84%EA%B0%9C%EB%B0%9C-%EC%9D%BC%EA%B8%B02/%EC%84%BC%EC%84%9C%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%83%98%ED%94%8C%EB%A7%81_2.png" alt="샘플링 데이터5">  </li>
<li>데이터 학습할때 절.대.로 데이터 전체를 그대로 넣으면 안된다.<br>  분명 메모리 OOM이 날 것이다.<br>  저번 포스트에서 나온 개념인<br>  Data Windowing를 써야 하는 이유이다.  (<em>Batch size보다 우선.</em>)</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-10-16T09:23:41.000Z" title="2022. 10. 16. 오후 6:23:41">2022-10-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-12-02T13:00:51.747Z" title="2024. 12. 2. 오후 10:00:51">2024-12-02</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">a minute read (About 153 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/10/16/rl-video-summ/">rl-video-summ</a></p><div class="content"><h3 id="비디오-요약-모델"><a href="#비디오-요약-모델" class="headerlink" title="비디오 요약 모델"></a>비디오 요약 모델</h3><p>  현실에서 하이라이트 장면 편집하려고 하면 편집 프로그램이 너무 비싸다.<br>  간단히 설계할 수 있는 YasuoNet 같은 지도학습 모델은 데이터셋의 하이라이트를 일일이 라벨링이 들어간다.<br>  만약 시간이 긴 비디오 데이터라면?? 말도 안됨. </p>
<p>  GPU 성능이 무어의 법칙으로 점차 향상되어 비디오에도 강화학습을 적용할 수 있을지 않을까 상상만 했었는데<br>  이미 5년전…  </p>
<p>  <img src="/rl-video-summ/pipeline.jpg" alt="강화학습"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-05T07:26:50.000Z" title="2022. 4. 5. 오후 4:26:50">2022-04-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-12-02T12:54:20.175Z" title="2024. 12. 2. 오후 9:54:20">2024-12-02</time></span><span class="level-item">a minute read (About 123 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/05/hello-world/">Hello World</a></p><div class="content"><p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-05T07:26:50.000Z" title="2022. 4. 5. 오후 4:26:50">2022-04-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-12-02T12:54:46.263Z" title="2024. 12. 2. 오후 9:54:46">2024-12-02</time></span><span class="level-item">a few seconds read (About 102 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/05/bbackcheem/">bbackcheem</a></p><div class="content"><h2 id="우분투-동적-라이브러리-빌드-후-안될때…-feat-CUDA"><a href="#우분투-동적-라이브러리-빌드-후-안될때…-feat-CUDA" class="headerlink" title="우분투 동적 라이브러리 빌드 후 안될때…(feat. CUDA)"></a>우분투 동적 라이브러리 빌드 후 안될때…(feat. CUDA)</h2><p>  CUDA 세팅중 ldd 의존성 체크에서 libc10.so 파일이 링크 확인할 때</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IFS=<span class="string">&#x27;:&#x27;</span> ; <span class="keyword">for</span> i <span class="keyword">in</span> <span class="variable">$LD_LIBRARY_PATH</span>; <span class="keyword">do</span> <span class="built_in">ls</span> -l <span class="variable">$i</span>/libc10.so 2&gt;/dev/null;<span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<p>  여기에서 아무것도 리턴 안되면 링크가 안되어 있는 상태임.<br>  해결책 </p>
<ol>
<li>export LD_LIBRARY_PATH &#x3D; &#x2F;somepath&#x2F;bin:$LD_LIBRARY_PATH</li>
<li>ld.so.conf.d&#x2F;someconf.conf 셋팅</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-09-06T05:54:41.000Z" title="2021. 9. 6. 오후 2:54:41">2021-09-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-09-11T23:29:06.695Z" title="2023. 9. 12. 오전 8:29:06">2023-09-12</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">9 minutes read (About 1397 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/06/Multi-GPU/">Multi-GPU</a></p><div class="content"><h2 id="작곡-GAN-x-Multi-GPU"><a href="#작곡-GAN-x-Multi-GPU" class="headerlink" title="작곡 GAN x Multi-GPU"></a>작곡 GAN x Multi-GPU</h2><h3 id="작곡-GAN-만들어보기"><a href="#작곡-GAN-만들어보기" class="headerlink" title="작곡 GAN 만들어보기"></a>작곡 GAN 만들어보기</h3><ol>
<li>음악 작곡 전처리 방법 : midi파일을 입출력으로 사용</li>
<li>pypianoroll로 각 트랙의 악기, 키, 코드, 화음 계산 반영</li>
</ol>
<h3 id="네트워크-구조"><a href="#네트워크-구조" class="headerlink" title="네트워크 구조"></a>네트워크 구조</h3><ol>
<li><p>Generator<br>  input : Single Melody track (Timestep, n_pitch, n_tracks)<br>  Latent noise Vector z: (2, 8, 512)<br>  U-Net 구조로 되어있음.<br>  <img src="/Multi-GPU/%EB%A9%80%ED%8B%B0%EC%A7%80%ED%93%A8_01.png" alt="U-net">     </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">def</span> <span class="title function_">_conv2d</span>(<span class="params">layer_input, filters, f_size=<span class="number">4</span>, bn=<span class="literal">True</span></span>):</span><br><span class="line">      <span class="comment"># 다운샘플링</span></span><br><span class="line">      d = keras.layers.Conv2D(filters, kernel_size=f_size, strides=<span class="number">2</span>,</span><br><span class="line">                                padding=<span class="string">&#x27;same&#x27;</span>)(layer_input)</span><br><span class="line">      d = keras.layers.LeakyReLU(alpha=<span class="number">0.2</span>)(d)</span><br><span class="line">      <span class="keyword">if</span> bn:</span><br><span class="line">          d = keras.layers.BatchNormalization(momentum=<span class="number">0.8</span>)(d)</span><br><span class="line">      <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_deconv2d</span>(<span class="params">layer_input, pre_input, filters, f_size=<span class="number">4</span>, dropout_rate=<span class="number">0</span></span>):</span><br><span class="line">        <span class="comment"># 업 샘플링</span></span><br><span class="line">        u = keras.layers.UpSampling2D(size=<span class="number">2</span>)(layer_input)</span><br><span class="line">        u = keras.layers.Conv2D(filters, kernel_size=f_size, strides=<span class="number">1</span>,</span><br><span class="line">                                  padding=<span class="string">&#x27;same&#x27;</span>)(u)</span><br><span class="line">        u = keras.layers.BatchNormalization(momentum=<span class="number">0.8</span>)(u)</span><br><span class="line">        u = keras.layers.ReLU()(u)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> dropout_rate:</span><br><span class="line">            u = keras.layers.Dropout(dropout_rate)(u)</span><br><span class="line">            </span><br><span class="line">        u = keras.layers.Concatenate()([u, pre_input])</span><br><span class="line">        <span class="keyword">return</span> u</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_generator</span>(<span class="params">condition_input_shape=(<span class="params"><span class="number">32</span>, <span class="number">128</span>, <span class="number">1</span></span>), filters=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">                        instruments=<span class="number">4</span>, latent_shape=(<span class="params"><span class="number">2</span>, <span class="number">8</span>, <span class="number">512</span></span>)</span>):</span><br><span class="line">        <span class="comment"># 제네레이터 블록</span></span><br><span class="line">        c_input = keras.layers.Input(shape=condition_input_shape)</span><br><span class="line">        z_input = keras.layers.Input(shape=latent_shape)</span><br><span class="line"></span><br><span class="line">        d1 = _conv2d(c_input, filters, bn=<span class="literal">False</span>)</span><br><span class="line">        d2 = _conv2d(d1, filters * <span class="number">2</span>)</span><br><span class="line">        d3 = _conv2d(d2, filters * <span class="number">4</span>)</span><br><span class="line">        d4 = _conv2d(d3, filters * <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        d4 = keras.layers.Concatenate(axis=-<span class="number">1</span>)([d4, z_input])</span><br><span class="line"></span><br><span class="line">        u4 = _deconv2d(d4, d3, filters * <span class="number">4</span>)</span><br><span class="line">        u5 = _deconv2d(u4, d2, filters * <span class="number">2</span>)</span><br><span class="line">        u6 = _deconv2d(u5, d1, filters)</span><br><span class="line"></span><br><span class="line">        u7 = keras.layers.UpSampling2D(size=<span class="number">2</span>)(u6)</span><br><span class="line">        output = keras.layers.Conv2D(instruments, kernel_size=<span class="number">4</span>, strides=<span class="number">1</span>,</span><br><span class="line">                                  padding=<span class="string">&#x27;same&#x27;</span>, activation=<span class="string">&#x27;tanh&#x27;</span>)(u7)  <span class="comment"># 32, 128, 4</span></span><br><span class="line"></span><br><span class="line">        generator = keras.models.Model([c_input, z_input], output, name=<span class="string">&#x27;Generator&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> generator</span><br><span class="line">    ```  </span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> Discriminator  </span><br><span class="line">  ![U-net](Multi-GPU/멀티지퓨_03.png)  </span><br><span class="line">    </span><br><span class="line">    ```python</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_discriminator_layer</span>(<span class="params">layer_input, filters, f_size=<span class="number">4</span></span>):</span><br><span class="line">      </span><br><span class="line">          <span class="comment"># input:  [batch_size, in_channels, H, W]</span></span><br><span class="line">          <span class="comment"># output: [batch_size, out_channels, H/2, W/2]</span></span><br><span class="line">      </span><br><span class="line">      d = keras.layers.Conv2D(filters, kernel_size=f_size, strides=<span class="number">2</span>,</span><br><span class="line">                                padding=<span class="string">&#x27;same&#x27;</span>)(layer_input)</span><br><span class="line">      <span class="comment"># Discriminator는 BatchNorm을 쓰지 않습니다!!</span></span><br><span class="line">      d = keras.layers.LeakyReLU(alpha=<span class="number">0.2</span>)(d) </span><br><span class="line">      <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_discriminator</span>(<span class="params">pianoroll_shape=(<span class="params"><span class="number">32</span>, <span class="number">128</span>, <span class="number">4</span></span>), filters=<span class="number">64</span></span>):</span><br><span class="line">        <span class="comment"># WGAN Discriminator(비평자)</span></span><br><span class="line">        </span><br><span class="line">        condition_input_shape = (<span class="number">32</span>,<span class="number">128</span>,<span class="number">1</span>)</span><br><span class="line">        groundtruth_pianoroll = keras.layers.Input(shape=pianoroll_shape)</span><br><span class="line">        condition_input = keras.layers.Input(shape=condition_input_shape)</span><br><span class="line">        combined_imgs = keras.layers.Concatenate(axis=-<span class="number">1</span>)([groundtruth_pianoroll, condition_input])</span><br><span class="line"></span><br><span class="line">        d1 = _build_discriminator_layer(combined_imgs, filters)</span><br><span class="line">        d2 = _build_discriminator_layer(d1, filters * <span class="number">2</span>)</span><br><span class="line">        d3 = _build_discriminator_layer(d2, filters * <span class="number">4</span>)</span><br><span class="line">        d4 = _build_discriminator_layer(d3, filters * <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        x = keras.layers.Flatten()(d4)</span><br><span class="line">        logit = keras.layers.Dense(<span class="number">1</span>)(x)</span><br><span class="line"></span><br><span class="line">        discriminator = keras.models.Model([groundtruth_pianoroll,condition_input], logit,</span><br><span class="line">                                              name=<span class="string">&#x27;Discriminator&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> discriminator</span><br></pre></td></tr></table></figure></li>
<li><p>GAN모델 loss함수와 굴리는 방법</p>
<p><em>Generator Loss 함수 :</em></p>
<ul>
<li><p>Discriminator loss함수와 반대의 함수를 사용한다. 제네레이터는 pianoroll을 가능한 한 더 리얼하게 만들어야 하기때문.</p>
<ul>
<li>$\frac{1}{m} \sum_{i&#x3D;1}^{m} -D_w(G(z^{i}|c^{i})|c^{i})$</li>
</ul>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generator_loss</span>(<span class="params">discriminator_fake_output</span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot; Wasserstein GAN loss</span></span><br><span class="line"><span class="string">(Generator)  -D(G(z|c))</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">return</span> -tf.reduce_mean(discriminator_fake_output)</span><br></pre></td></tr></table></figure></li>
</ul>
<p><em>Discriminator Loss 함수:</em></p>
<ul>
<li><p>진짜 Pianoroll과 생성된 pianoroll 분포의 거리를 최대화 하기 위해 Wasserstein loss 함수를 사용.</p>
<ul>
<li>$\frac{1}{m} \sum_{i&#x3D;1}^{m} [D_w(G(z^{i}|c^{i})|c^{i}) - D_w(x^{i}|c^{i})]$</li>
</ul>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">wasserstein_loss</span>(<span class="params">discriminator_real_output, discriminator_fake_output</span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot; Wasserstein GAN loss</span></span><br><span class="line"><span class="string">(Discriminator)  D(G(z|c)) - D(x|c)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">return</span> tf.reduce_mean(discriminator_fake_output) - tf.reduce_mean(</span><br><span class="line">  discriminator_real_output)</span><br></pre></td></tr></table></figure>
</li>
<li><p>gradient penalty loss(aka WGAN-GP loss)를 사용한다.<br> 그 이유는 D에 대한 gradient를 적절히 컨트롤 하는데 적합하기 떄문이고 G의 최적화에 도움을 준다.</p>
<ul>
<li>$\frac{1}{m} \sum_{i&#x3D;1}^{m}(\lVert \nabla_{\hat{x}^i}D_w(\hat{x}^i|c^{i}) \rVert_2 -  1)^2 $</li>
</ul>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_penalty</span>(<span class="params">discriminator, x, fake_x</span>):</span><br><span class="line"></span><br><span class="line">c = tf.expand_dims(x[..., <span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">batch_size = x.get_shape().as_list()[<span class="number">0</span>]</span><br><span class="line">eps_x = tf.random.uniform(</span><br><span class="line">    [batch_size] + [<span class="number">1</span>] * (<span class="built_in">len</span>(x.get_shape()) - <span class="number">1</span>))  <span class="comment"># B, 1, 1, 1, 1</span></span><br><span class="line">inter = eps_x * x + (<span class="number">1.0</span> - eps_x) * fake_x</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> g:</span><br><span class="line">    g.watch(inter)</span><br><span class="line">    disc_inter_output = discriminator((inter,c), training=<span class="literal">True</span>)</span><br><span class="line">grads = g.gradient(disc_inter_output, inter)</span><br><span class="line">slopes = tf.sqrt(<span class="number">1e-8</span> + tf.reduce_sum(</span><br><span class="line">    tf.square(grads),</span><br><span class="line">    axis=tf.<span class="built_in">range</span>(<span class="number">1</span>, grads.get_shape().ndims)))</span><br><span class="line">gradient_penalty = tf.reduce_mean(tf.square(slopes - <span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> gradient_penalty</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="GAN-모델-돌리는-방법"><a href="#GAN-모델-돌리는-방법" class="headerlink" title="GAN 모델 돌리는 방법"></a>GAN 모델 돌리는 방법</h3><ol>
<li><p>G와 D에 Adam 최적화 함수를 쓴다.(정교하게 하려면 SGD 를 써야 한다.)  </p>
</li>
<li><p>체크포인트를 써서 매번 모델을 저장한다.</p>
</li>
<li><p>돌려보는 함수.<br>  G를 돌리는 함수 부분  </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generator_train_step</span>(<span class="params">x, condition_track_idx=<span class="number">0</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment">############################################</span></span><br><span class="line">    <span class="comment"># G를 업데이트 한다.: maximize D(G(z|c))</span></span><br><span class="line">    <span class="comment">############################################</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 조건부 트랙을 뽑아서 real batch pianoroll을 만든다.</span></span><br><span class="line"></span><br><span class="line">    c = tf.expand_dims(x[..., condition_track_idx], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># latent vectors의 batch data를 만든다.</span></span><br><span class="line">    z = tf.random.truncated_normal([BATCH_SIZE, <span class="number">2</span>, <span class="number">8</span>, <span class="number">512</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        fake_x = generator((c, z), training=<span class="literal">True</span>)</span><br><span class="line">        fake_output = discriminator((fake_x,c), training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># G 결과물의 Loss 계산한다.</span></span><br><span class="line">        gen_loss = generator_loss(fake_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># G의 gradient를 계산한다.</span></span><br><span class="line">    gradients_of_generator = tape.gradient(gen_loss,</span><br><span class="line">                                          generator.trainable_variables)</span><br><span class="line">    <span class="comment"># 제네레이터를 업데이트 한다.</span></span><br><span class="line">    generator_optimizer.apply_gradients(</span><br><span class="line">        <span class="built_in">zip</span>(gradients_of_generator, generator.trainable_variables))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gen_loss</span><br></pre></td></tr></table></figure>

<p> D를 돌리는 함수 부분  </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">discriminator_train_step</span>(<span class="params">x, condition_track_idx=<span class="number">0</span></span>):</span><br><span class="line"></span><br><span class="line"><span class="comment">############################################################################</span></span><br><span class="line"><span class="comment">#(2) D를 업데이트: (D(x|c)) + (1 - D(G(z|c))|c) + GradientPenality() 를 최대화</span></span><br><span class="line"><span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 조건부 트랙을 뽑아서 real batch pianoroll을 만든다.</span></span><br><span class="line">c = tf.expand_dims(x[..., condition_track_idx], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># latent vectors의 batch data를 만든다.</span></span><br><span class="line">z = tf.random.truncated_normal([BATCH_SIZE, <span class="number">2</span>, <span class="number">8</span>, <span class="number">512</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 훼이크 pianoroll을 만든다.</span></span><br><span class="line">fake_x = generator((c, z), training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># D의 파라미터들 업데이트</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    real_output = discriminator((x,c), training=<span class="literal">True</span>)</span><br><span class="line">    fake_output = discriminator((fake_x,c), training=<span class="literal">True</span>)</span><br><span class="line">    discriminator_loss =  wasserstein_loss(real_output, fake_output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># real, fake batch의 gradient를 계산</span></span><br><span class="line">grads_of_discriminator = tape.gradient(discriminator_loss,</span><br><span class="line">                                           discriminator.trainable_variables)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    gp_loss = compute_gradient_penalty(discriminator, x, fake_x)</span><br><span class="line">    gp_loss *= <span class="number">10.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># real, fake batch의 GP-loss를 계산</span></span><br><span class="line">grads_gp = tape.gradient(gp_loss, discriminator.trainable_variables)</span><br><span class="line">gradients_of_discriminator = [g + ggp <span class="keyword">for</span> g, ggp <span class="keyword">in</span></span><br><span class="line">                              <span class="built_in">zip</span>(grads_of_discriminator, grads_gp)</span><br><span class="line">                              <span class="keyword">if</span> ggp <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># D를 업데이트 해준다.</span></span><br><span class="line">discriminator_optimizer.apply_gradients(</span><br><span class="line">    <span class="built_in">zip</span>(gradients_of_discriminator, discriminator.trainable_variables))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> discriminator_loss + gp_loss</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="Multi-GPU-돌리는-방법-소개"><a href="#Multi-GPU-돌리는-방법-소개" class="headerlink" title="Multi-GPU 돌리는 방법 소개"></a>Multi-GPU 돌리는 방법 소개</h3><ol>
<li>텐서플로 MirroredStrategy 를 사용(계산 용이성)</li>
<li>어떻게 돌리는지?? <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Strategy 만들고</span></span><br><span class="line">strategy = tf.distribute.MirroredStrategy()</span><br><span class="line">FLAG = <span class="literal">True</span></span><br><span class="line"><span class="keyword">if</span> strategy.num_replicas_in_sync  &gt; <span class="number">1</span> <span class="keyword">and</span> FLAG:</span><br><span class="line">    MULTIPLE_BATCH = strategy.num_replicas_in_sync</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&#x27;분산환경 사용 &gt;&gt; GPU: <span class="subst">&#123;MULTIPLE_BATCH&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&#x27;분산환경 미사용&#x27;</span>)</span><br><span class="line">  MULTIPLE_BATCH = <span class="number">1</span></span><br><span class="line"><span class="comment"># 2. 모델을 strategy 안에 포함시킴.</span></span><br><span class="line"><span class="comment"># 중요한 부분 : 학습 함수에서 나온 loss를 self.strategy.run에 넣고 strategy.reduce</span></span><br><span class="line"><span class="comment"># 배치 사이즈를 num_replica_in_sync 갯수만큼 곱한다.</span></span><br><span class="line"><span class="comment"># 출력을 포함한 자세한 코드는 My GitHub...</span></span><br></pre></td></tr></table></figure></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-07-15T10:43:57.000Z" title="2021. 7. 15. 오후 7:43:57">2021-07-15</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-07-26T08:26:34.391Z" title="2022. 7. 26. 오후 5:26:34">2022-07-26</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">11 minutes read (About 1699 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/15/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/">논문 리뷰</a></p><div class="content"><h2 id="딥러닝-논문-리뷰"><a href="#딥러닝-논문-리뷰" class="headerlink" title="딥러닝 논문 리뷰"></a>딥러닝 논문 리뷰</h2><p>  RNN의 고질적인 문제점… Sequence가 길면<br>  i번째 output을 만들기 위해 그 이전의 i-1번째 hidden state를 사용한다.<br>  Long Term Dependency problem…<br>  <img src="/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/%ED%8A%B8%ED%8F%AC%EB%A8%B8_01.png" alt="이런거"><br>  <img src="/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/%ED%8A%B8%ED%8F%AC%EB%A8%B8_pre.png" alt="Gradient X"><br>  이걸 해결하기 위해서는 Recurrent Layer 대신<br>  Attention Mechanism을 쓰면 Sequence 길이에 상관없이 input &#x2F; output의 Dependency를 보다  정확히 감지… RNN을 완전히 제거해야 함.  </p>
<h3 id="트랜스포머를-써야-하는-이유"><a href="#트랜스포머를-써야-하는-이유" class="headerlink" title="트랜스포머를 써야 하는 이유?"></a>트랜스포머를 써야 하는 이유?</h3><blockquote>
<p>현재 Video Understanding 모델에서도 시도되고 있음(TimesFormer; 기존 프레임 단위로 쪼갠 CNN모델에 비해서 메모리 절감과 Inference 속도 향상)<br>  Timesformer 은 효과적인 프레임별 델타값만 감지 : 비디오를 Patch단위로 분석함    </p>
</blockquote>
<ul>
<li>Positional Encoding을 시간축에 확장한 모델, 시간축과 공간축 전체를 어텐션 하면 Cost가 너무 크기 때문<br>  밀리초 단위로 결판이 나는 게임 판독에도 정말 용이할것으로 기대됨.</li>
</ul>
<h3 id="트랜스포머의-특징"><a href="#트랜스포머의-특징" class="headerlink" title="트랜스포머의 특징"></a>트랜스포머의 특징</h3><ul>
<li><p>RNN 계열은 순서대로만 처리 가능해서 학습 속도가 느림.<br>  하지만 트랜스포머는 병렬 처리가 가능…How?  </p>
</li>
<li><p>Encoder-Decoder 모델을 통해서 병렬 처리  </p>
<blockquote>
<p>ENcoder에서는 각각 position에 대해 Attention만 하고,<br>  Decoder에서는 Masking 메커니즘으로 병렬 처리가 가능</p>
</blockquote>
</li>
</ul>
<p>  Encoder는 input Sequence를 다른 표현으로 치환  </p>
<ul>
<li>Decoder에서는 ENcoder으로부터 Output Sequence를 하나씩 생성  </li>
<li>각각 step에서 다음 symbol을 만들 때 이전에 만들어진 output을 쓴다.(자기 회귀적인 특성)  <blockquote>
<p>ex : “여기는 어디 나는 누구” 라는 문장에서 “여기는 어디” 라는 symbol으로 “나는 누구” 를 만들 수 있습니다.</p>
</blockquote>
</li>
</ul>
<h3 id="Transformer-전체-구조"><a href="#Transformer-전체-구조" class="headerlink" title="Transformer 전체 구조"></a>Transformer 전체 구조</h3><p>  <img src="/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/transformer.png" alt="이것이 바로 트랜스포머">   </p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>  <img src="/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8_%EC%9D%B8%EC%BD%94%EB%8D%94.png" alt="인코더"></p>
<ul>
<li><p>Input Embedding은 Time Embedding, 자연어에 쓰이는 Word Embedding 등 여러 종류가 있음  </p>
<p>그 중에서 Time Embedding을 소개.<br>$$\mathbf{t} 2 \mathbf{v}(\tau)[i]&#x3D;\left{\begin{array}{ll}<br>\omega_{i} \tau+\varphi_{i}, &amp; \text { if } i&#x3D;0 \<br>\mathcal{F}\left(\omega_{i} \tau+\varphi_{i}\right), &amp; \text { if } 1 \leq i \leq k<br>\end{array}\right.$$</p>
<p>ax+b처럼 생긴 저 식에 함수를 넣어서 시간별 정보를 실어야 함.<br>i는 Timestep…  시퀀스의 시작점은 그냥 ax+b만을 쓴다.<br>전체 시퀀스 데이터에 적용하려면 주기성을 갖는 함수(파장, 주기, 주파수 등)를 사용해야 하는데<br>relu를 사용하면 주기성 정보가 없으니…당연히 안될듯..(!) </p>
<p>y &#x3D; wx + b concat sin(wx+b)</p>
<p>각 타임스텝별로 주기성 정보를 주입  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.Keras.Layer</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Time2Vector</span>(<span class="title class_ inherited__">Layer</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, seq_len, **kwargs</span>):</span><br><span class="line">      <span class="built_in">super</span>(Time2Vector, <span class="variable language_">self</span>).__init__()</span><br><span class="line">      <span class="variable language_">self</span>.seq_len = seq_len</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">      <span class="string">&#x27;&#x27;&#x27;shape (batch, seq_len) 형태로 가중치와 Bias 초기화 &#x27;&#x27;&#x27;</span></span><br><span class="line">      <span class="variable language_">self</span>.weights_linear = <span class="variable language_">self</span>.add_weight(name=<span class="string">&#x27;weight_linear&#x27;</span>,shape=(<span class="built_in">int</span>(<span class="variable language_">self</span>.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      <span class="variable language_">self</span>.bias_linear = <span class="variable language_">self</span>.add_weight(name=<span class="string">&#x27;bias_linear&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(<span class="variable language_">self</span>.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      <span class="variable language_">self</span>.weights_periodic = <span class="variable language_">self</span>.add_weight(name=<span class="string">&#x27;weight_periodic&#x27;</span>,shape=(<span class="built_in">int</span>(<span class="variable language_">self</span>.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      <span class="variable language_">self</span>.bias_periodic = <span class="variable language_">self</span>.add_weight(name=<span class="string">&#x27;bias_periodic&#x27;</span>,shape=(<span class="built_in">int</span>(<span class="variable language_">self</span>.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">      <span class="string">&#x27;&#x27;&#x27;주기성, 선형 시간별 특징을 계산&#x27;&#x27;&#x27;</span></span><br><span class="line">      x = tf.math.reduce_mean(x[:,:,:], axis=-<span class="number">1</span>) <span class="comment"># 입력 Feature 차원 슬라이싱</span></span><br><span class="line">      time_linear = <span class="variable language_">self</span>.weights_linear * x + <span class="variable language_">self</span>.bias_linear <span class="comment"># 선형 시간 특징</span></span><br><span class="line">      time_linear = tf.expand_dims(time_linear, axis=-<span class="number">1</span>) <span class="comment"># 차원 추가 (batch, seq_len, 1)</span></span><br><span class="line"></span><br><span class="line">      time_periodic = tf.math.sin(tf.multiply(x, <span class="variable language_">self</span>.weights_periodic) + <span class="variable language_">self</span>.bias_periodic)</span><br><span class="line">      time_periodic = tf.expand_dims(time_periodic, axis=-<span class="number">1</span>) <span class="comment"># 차원 추가 (batch, seq_len, 1)</span></span><br><span class="line">      <span class="keyword">return</span> tf.concat([time_linear, time_periodic], axis=-<span class="number">1</span>) <span class="comment"># shape = (batch, seq_len, 2)</span></span><br></pre></td></tr></table></figure></li>
<li><p>positional Encoding<br>$$\begin{array}{l}<br>P E_{(\text {pos } 2 i)}&#x3D;\sin \left(\frac{\text { pos }}{10000^{\frac{2 i}{d_{\text {model }}}}}\right) \<br>P E_{(\text {pos 2i+1) }}&#x3D;\cos \left(\frac{\text { pos }}{10000^{\frac{2 i}{d_<br>{\text {model }}}}} \right)<br>\end{array}$$</p>
<p>positional Encoding이 계산되는 과정 : Time Embedded Input + positional vector -&gt; Time Embedded vector</p>
<p>positional Encoding을 해야 하는 이유 :<br>이후 Attention Layer의 Q, K, V에 보다 명확한 Sequence에 대한 정보를 반영하기 위해(sequence vector간의 거리 확실화)</p>
<p>ex &gt; embedding size 512일때 : (-1~1의 값으로 이루어진) sin 함수로 처리된 256사이즈 벡터 + cos로 처리된 256사이즈 벡터 생성</p>
<p><img src="/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/TF_PE.png" alt="20*256사이즈"></p>
<ul>
<li>Attention<br>Luong Attention, Badanau Attention, self-Attention 등 Task에 따라 다름.<br>그 중에서 Self-Attention …</li>
</ul>
<p>@ : matmul (벡터 내적곱)<br>T : 전치행렬(Transpose)<br>^1&#x2F;2 : 루트</p>
<p>Attention(Q, K, V) &#x3D; softmax( Q @ K.T &#x2F; d_k^1&#x2F;2) @ V<br>V를 곱하는 이유 : softmax된 스코어에 value를 곱해서<br>관련이 없는 시퀀스에다 1e-4 같은 작은 스코어를 곱해 없앤다.  </p>
<p>이렇게 Attention score가 계산된다.</p>
<ul>
<li>Multi-Head Attention<br>n_head * Concat( softmax( Q @ K.T &#x2F; d_k^1&#x2F;2) @ V ) @ W0<br>feed forward Neural Network( tf.keras.layers.Dense )에 입력을 주기 위해<br>하나의 행렬로 head마다 계산된 softmax된 스코어를 전부 합치고 W0를 곱한다.  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> |---Residual connect----|                  |---Residual connect-------|</span><br><span class="line">Attention ----&gt; LayerNormalize --&gt; FeedForwardNeuralNetwork -&gt; LayerNorm </span><br><span class="line"> |                                                                     |</span><br><span class="line"> |&lt;------------- N repeat --------------------------------------------&gt;|</span><br><span class="line"> |----------------------Encoder #1-------------------------------------|</span><br></pre></td></tr></table></figure></li>
</ul>
<p>멀티헤드 어텐션의 장점 : 모델이 서로 다른 위치에서 서로 다른 표현 부분 공간 정보에 공동으로 관여</p>
<blockquote>
<p>커버로스, 히드라, 자쿰은 머리가 여러개라 위치가 다양한 여러 플레이어를 인식하고 동시다발로 데미지를 가함. </p>
</blockquote>
<p><img src="/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/%ED%8A%B8%ED%8F%AC_%EC%84%A4%EB%AA%85.png" alt="TransformerAttn"></p>
</li>
</ul>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>  디코더 구성</p>
<ul>
<li>Output에 대해 Right Shift 하고 Embedding, Positional Encoding 이 이루어짐.</li>
</ul>
<ul>
<li><p>Right shifted 된 입력을 받는 이유 : 디코더는 이전 시퀀스에 대한 토큰과 어텐션 스코어를 기반으로 다음 시퀀스를 예측하는 방식으로 작동된다. 시퀀스의 처음 토큰 이전에 시작임을 알리기 위한 특정 토큰이 삽입되어 없는 시퀀스에 대한 예측 에러을 방지해 준다<br>   Masked Attention은 다음 시퀀스와의 유사성을 무시하기 위해 Masking을 적용</p>
</li>
<li><p>이후는 Encoder와 같은 구조의 레이어로 쌓여 있음.  디코더 에서는 인코더와는 다르게<br>   Masked Multi-Head Attention layer가 추가되었다.</p>
</li>
</ul>
<ul>
<li>Masked Attention의 원리: 시계열 데이터를 예로 들자면</li>
</ul>
<p>  오늘 5000원 받았는데 이것저것 샀다.<br>  그래서 내일은 오늘보다 잔액이 감소할 것이다.</p>
<p>  Masked Attention의 시계열 데이터를 계산하는 과정은..  </p>
<p>  미래를 예측하기 위해 현재의 값만 알려주고(미래 시퀀스와의 유사도를 계산하면 안되서…)<br>  미래의 값을 가려버리는 방식으로 학습..<br>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">오늘의 잔고   내일 잔고   모레 잔고  글피 잔고 </span><br><span class="line">5000원          -inf      -inf        -inf</span><br><span class="line">5000원          3000원    -inf        -inf</span><br><span class="line">5000원          3000원     2000원     -inf</span><br><span class="line">5000원          3000원     2000원     1000원</span><br><span class="line"></span><br><span class="line">                         |---Residual connect----|            |---Residual connect-------|</span><br><span class="line">MaskedAttn-LayerNorm--&gt; Attention ----&gt; LayerNormalize --&gt; FeedForwardNeuralNetwork -&gt; LayerNorm </span><br><span class="line">  |                                                                                       |</span><br><span class="line">  |&lt;------------------------------- N repeat --------------------------------------------&gt;|</span><br><span class="line">  |&lt;---------------------------------------Decoder #1-------------------------------------|</span><br></pre></td></tr></table></figure></p>
<h3 id="Final-Output-Layer"><a href="#Final-Output-Layer" class="headerlink" title="Final Output Layer"></a>Final Output Layer</h3><ul>
<li>분류기(softmax), 회귀예측(Linear) 레이어를 붙여서 태스크에 맞게 바꿔끼면 된다.</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-06-17T08:34:17.000Z" title="2021. 6. 17. 오후 5:34:17">2021-06-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-05-27T22:12:01.671Z" title="2023. 5. 28. 오전 7:12:01">2023-05-28</time></span><span class="level-item"><a class="link-muted" href="/categories/Game/">Game</a></span><span class="level-item">4 minutes read (About 581 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/06/17/%EA%B2%8C%EC%9E%84%EA%B0%9C%EB%B0%9C-%EC%9D%BC%EA%B8%B0/">게임개발 일기</a></p><div class="content"><h2 id="게임개발-후기"><a href="#게임개발-후기" class="headerlink" title="게임개발 후기"></a>게임개발 후기</h2><p>데이터를 모으기 위한 Android 앱 만들기<br>앱을 최적화하려면 라이브러리 특성, 자료 구조를 알아야 가능.  </p>
<h3 id="앱-최적화-시도"><a href="#앱-최적화-시도" class="headerlink" title="앱 최적화 시도"></a>앱 최적화 시도</h3><p>StreanWriter에 WriteLine을 써서 로깅을 하게된다면<br>일반적인 File.CreateText에 비해서 리소스를 겁나게(!) 많이 먹는 현상이 발생한다.<br>계속 버퍼가 열려 있으면 오버헤드가 나기 십상이기 때문이다.<br>단순한 데이터일 경우에는 단순한 라이브러리를 써야 한다.<br>그렇지 않으면 리소스 소모 급증과 엄청난 메모리 사고가 발생한다.<br>리소스 소모량 급증을 막기 위해서는 StreamWriter 대신 문자열을 계속   더해주는 StringBuilder를 사용하자.  </p>
<h3 id="라이브러리-하나만-바꿨을-뿐인데…"><a href="#라이브러리-하나만-바꿨을-뿐인데…" class="headerlink" title="라이브러리 하나만 바꿨을 뿐인데…"></a>라이브러리 하나만 바꿨을 뿐인데…</h3><p><img src="/%EA%B2%8C%EC%9E%84%EA%B0%9C%EB%B0%9C-%EC%9D%BC%EA%B8%B0/%EC%B5%9C%EC%A0%81%ED%99%94_%EC%9D%B4%ED%9B%84.png" alt="최적화 예시"><br>최적화 이후에는 예전보다 cpu 점유 시간이 엄청나게 개선되었다.</p>
<p>그 이전에는 엄청난 렉, 끊김 크리티컬.<br>(update 함수에 WriteLine을 쓰게되면<br> 화면 프레임이 초당 10프레임으로 낮아지고 렉, 싱크 어긋남 크리티컬.)</p>
<h3 id="게임개발-앞서-선형대수는-필수"><a href="#게임개발-앞서-선형대수는-필수" class="headerlink" title="게임개발 앞서 선형대수는 필수"></a>게임개발 앞서 선형대수는 필수</h3><p>  로컬과 월드 좌표계를 헷갈리면 매우 큰일난다.<br>  뿐만 아니라 Collider, 속도 가속도 등 물리지식도 필수..<br>  캐릭터가 이상하게 움직이는 버그도 나기 십상이다.<br>  (to be continue…)</p>
<h2 id="최적화-방법은…"><a href="#최적화-방법은…" class="headerlink" title="최적화 방법은…"></a>최적화 방법은…</h2><p>API 문서를 일단 보면서 각 함수의 특징을 잘 알고 코딩을 해야 한다.<br>로직은 최대한 단순화(여러 가지 변수를 컨트롤 하려면 GameManager를 두는 방법)<br>특히 실행중 로직에 절대 Find() SendMessage() 같은 함수를 쓰면 최소 1000배 느려진다.</p>
<p>메모리 힙 재할당은 렉의 근본 원인… (GC 발동을 최대한 막아야 한다 !!)  </p>
<ol>
<li>문자열 속성에 자주 접근하면 그만큼 속도 저하</li>
<li>유니티에서 클래스는 참조 형식이라 GC 대상임.  </li>
<li>Destroy 자주 쓰지 말고 오브젝트 풀링…</li>
</ol>
<p>CPU는 나눗셈이 곱셈보다 더 느림.<br> –&gt; 제곱근(루트)를 계산하는 경우는 1&#x2F;2를 제곱한다.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-30T06:14:16.000Z" title="2021. 1. 30. 오후 3:14:16">2021-01-30</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-05T06:21:31.797Z" title="2022. 4. 5. 오후 3:21:31">2022-04-05</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Dacon/">Dacon</a></span><span class="level-item">2 minutes read (About 366 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/30/Dacon%ED%9B%84%EA%B8%B0/">Dacon후기</a></p><div class="content"><h2 id="저번에-이은-데이콘-도전-후기2"><a href="#저번에-이은-데이콘-도전-후기2" class="headerlink" title="저번에 이은 데이콘 도전 후기2"></a>저번에 이은 데이콘 도전 후기2</h2><p>  순위 발표 순간…<br>  ?!?!?!?!<br>  <img src="/Dacon%ED%9B%84%EA%B8%B0/dacon01.PNG" alt="순위 발표"></p>
<p>  <img src="/Dacon%ED%9B%84%EA%B8%B0/dacon02.PNG" alt="자세한 순위"></p>
<p>  1458 팀 중 15위… 와 이거 실화?!</p>
<p>  모델링은 Conv-LSTM</p>
<p>  전처리 방법은 설명력 높은 변수, </p>
<p>  단위면적당 일사량 &#x3D; 산란일사량 + 직접 일사량</p>
<p>  GHI &#x3D; DHI + DNI * \(\cos(\theta))\)  </p>
<p>  하루의 전체 시간 중에서</p>
<p>  해가 뜨고 질 때까지만 계산하는 것이였다.  </p>
<p>  이것이 바로 그 방법들…</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GHI 계산 방법</span></span><br><span class="line">temp[<span class="string">&#x27;NoSun&#x27;</span>] = np.where((temp[<span class="string">&#x27;DHI&#x27;</span>] &gt; <span class="number">0</span>) | (temp[<span class="string">&#x27;DNI&#x27;</span>] &gt; <span class="number">0</span>), <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">  </span><br><span class="line">temp[<span class="string">&#x27;sunny&#x27;</span>] = temp.groupby([<span class="string">&#x27;Day&#x27;</span>, temp.NoSun.cumsum()])[<span class="string">&#x27;NoSun&#x27;</span>].apply(<span class="keyword">lambda</span> x: (x ^ <span class="number">1</span>).cumsum())</span><br><span class="line"></span><br><span class="line">temp[<span class="string">&#x27;long&#x27;</span>] = temp[<span class="string">&#x27;sunny&#x27;</span>].cummax()</span><br><span class="line"></span><br><span class="line">temp[<span class="string">&#x27;angle&#x27;</span>] = ((temp[<span class="string">&#x27;sunny&#x27;</span>] / temp[<span class="string">&#x27;long&#x27;</span>]) * <span class="number">180</span>) - <span class="number">90</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">temp[<span class="string">&#x27;GHI&#x27;</span>] = temp[<span class="string">&#x27;DHI&#x27;</span>] + temp[<span class="string">&#x27;DNI&#x27;</span>] * temp[<span class="string">&#x27;angle&#x27;</span>].apply(<span class="keyword">lambda</span> x: np.cos(np.pi * (x / <span class="number">180</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataSet windowing 하는 방법</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">windowed_dataset</span>(<span class="params">x, y, window_size, batch_size, shuffle, shuffle_size</span>):</span><br><span class="line">  ds_x = tf.data.Dataset.from_tensor_slices(x)</span><br><span class="line">  ds_x = ds_x.window(window_size, shift = <span class="number">1</span>, stride = <span class="number">1</span>, drop_remainder=<span class="literal">True</span>)</span><br><span class="line">  ds_x = ds_x.flat_map(<span class="keyword">lambda</span> x: x.batch(window_size))</span><br><span class="line">  </span><br><span class="line">  ds_y = tf.data.Dataset.from_tensor_slices(y)</span><br><span class="line">  ds_y = ds_y.window(window_size, shift = <span class="number">1</span>, stride = <span class="number">1</span>, drop_remainder=<span class="literal">True</span>)</span><br><span class="line">  ds_y = ds_y.flat_map(<span class="keyword">lambda</span> x: x.batch(window_size))</span><br><span class="line">  </span><br><span class="line">  ds = tf.data.Dataset.<span class="built_in">zip</span>((ds_x, ds_y))</span><br><span class="line">  <span class="keyword">if</span> shuffle:</span><br><span class="line">      ds = ds.shuffle(shuffle_size)</span><br><span class="line">  <span class="keyword">return</span> ds.batch(batch_size).prefetch(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">window_data_pred</span>(<span class="params">data, window_size, batch_size</span>):</span><br><span class="line">    ds = tf.data.Dataset.from_tensor_slices(data)</span><br><span class="line">    ds = ds.window(window_size, shift = <span class="number">1</span>, stride = <span class="number">1</span>) <span class="comment">#, drop_remainder=True) # stride = 1</span></span><br><span class="line">    ds = ds.flat_map(<span class="keyword">lambda</span> x: x.batch(window_size))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ds.padded_batch(batch_size, padded_shapes=(<span class="literal">None</span>, <span class="number">7</span>)).prefetch(<span class="number">1</span>) <span class="comment"># 7은 Test Set의 특징 개수</span></span><br></pre></td></tr></table></figure>
<h3 id="Test-dataset을-drop-remainder-True-하면-안되는-이유"><a href="#Test-dataset을-drop-remainder-True-하면-안되는-이유" class="headerlink" title="Test dataset을 drop_remainder&#x3D;True 하면 안되는 이유?"></a>Test dataset을 drop_remainder&#x3D;True 하면 안되는 이유?</h3><p>  Test Set의 정보가 sequence length만큼 없어짐.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-03T04:05:29.000Z" title="2021. 1. 3. 오후 1:05:29">2021-01-03</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-05T06:20:21.669Z" title="2022. 4. 5. 오후 3:20:21">2022-04-05</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">5 minutes read (About 677 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/03/%EC%86%8C%EC%86%8C%ED%95%9C-%EC%97%B0%EA%B5%AC%EC%95%84%EB%8B%8C-%EC%97%B0%EA%B5%AC/">소소한 연구아닌 연구</a></p><div class="content"><h2 id="넘사벽-난이도인-트랜스포머-모델링"><a href="#넘사벽-난이도인-트랜스포머-모델링" class="headerlink" title="넘사벽 난이도인 트랜스포머 모델링"></a>넘사벽 난이도인 트랜스포머 모델링</h2><p>  시계열 데이터를 병렬 처리한다는 점에서는 가장 좋지만<br>  그만큼 모델링 하기가 까다롭다.</p>
<p>  model input : (Batch, Sequence, feature_num)</p>
<p>  Transformer는 input &#x2F; output을 잘못 설계하다간 sequence 정보가 날아갈 수도 있고<br>  그냥 데이터 자체가 8:45 가 될수가 있다.</p>
<p>  입력 데이터를 Seqneuce로 Packing 해주어야 학습이 가능한 데이터가 된다.</p>
<p>  Sequence Packing 방법은 파이썬 문법으로 비교적 쉽게 되지만…</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df_train = train.values</span><br><span class="line">train_seq = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(seq_len, <span class="built_in">len</span>(df_train)+<span class="number">1</span>):</span><br><span class="line">  train_seq.append(df_train[i - seq_len: i])</span><br><span class="line">train_seq = np.array(train_seq)</span><br></pre></td></tr></table></figure>

<p>  Sequence unpacking 은 numpy 연산을 써야 비교적 쉬움..<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">out = np.vstack(r <span class="keyword">for</span> r <span class="keyword">in</span> train_seq)</span><br><span class="line">out = out.unique(out, axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<p>  <img src="/%EC%86%8C%EC%86%8C%ED%95%9C-%EC%97%B0%EA%B5%AC%EC%95%84%EB%8B%8C-%EC%97%B0%EA%B5%AC/%ED%8F%AC%ED%8A%B8%ED%8F%B4%EB%A6%AC%EC%98%A41.PNG" alt="packing &amp; unpacking"></p>
<p>  <img src="/%EC%86%8C%EC%86%8C%ED%95%9C-%EC%97%B0%EA%B5%AC%EC%95%84%EB%8B%8C-%EC%97%B0%EA%B5%AC/%ED%8F%AC%ED%8A%B8%ED%8F%B4%EB%A6%AC%EC%98%A42.PNG" alt="unpacking2"></p>
<h3 id="다변량-회귀에서-얻은-교훈-1"><a href="#다변량-회귀에서-얻은-교훈-1" class="headerlink" title="다변량 회귀에서 얻은 교훈 1"></a>다변량 회귀에서 얻은 교훈 1</h3><p>  입력 데이터를 받아서 Time Embedding…</p>
<p>  시간별 Positional Encoding 방법을 소개하지.</p>
<p>  y &#x3D; wx + b concat sin(wx+b)</p>
<p>  선형의 시간 특징과 주기성의 특징에다 sin을 적용한 특징임.</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Time2Vector</span>(<span class="title class_ inherited__">Layer</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, seq_len, **kwargs</span>):</span><br><span class="line">      <span class="built_in">super</span>(Time2Vector, <span class="variable language_">self</span>).__init__()</span><br><span class="line">      <span class="variable language_">self</span>.seq_len = seq_len</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">      <span class="string">&#x27;&#x27;&#x27;shape (batch, seq_len) 형태로 가중치와 Bias 초기화 &#x27;&#x27;&#x27;</span></span><br><span class="line">      <span class="variable language_">self</span>.weights_linear = <span class="variable language_">self</span>.add_weight(name=<span class="string">&#x27;weight_linear&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(<span class="variable language_">self</span>.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      <span class="variable language_">self</span>.bias_linear = <span class="variable language_">self</span>.add_weight(name=<span class="string">&#x27;bias_linear&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(<span class="variable language_">self</span>.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      <span class="variable language_">self</span>.weights_periodic = <span class="variable language_">self</span>.add_weight(name=<span class="string">&#x27;weight_periodic&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(<span class="variable language_">self</span>.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      <span class="variable language_">self</span>.bias_periodic = <span class="variable language_">self</span>.add_weight(name=<span class="string">&#x27;bias_periodic&#x27;</span>,</span><br><span class="line">                                  shape=(<span class="built_in">int</span>(<span class="variable language_">self</span>.seq_len),),</span><br><span class="line">                                  initializer=<span class="string">&#x27;uniform&#x27;</span>,</span><br><span class="line">                                  trainable=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">      <span class="string">&#x27;&#x27;&#x27;주기성, 선형 시간별 특징을 계산&#x27;&#x27;&#x27;</span></span><br><span class="line">      x = tf.math.reduce_mean(x[:,:,:], axis=-<span class="number">1</span>) <span class="comment"># 입력 Feature 차원 슬라이싱</span></span><br><span class="line">      time_linear = <span class="variable language_">self</span>.weights_linear * x + <span class="variable language_">self</span>.bias_linear <span class="comment"># 선형 시간 특징</span></span><br><span class="line">      time_linear = tf.expand_dims(time_linear, axis=-<span class="number">1</span>) <span class="comment"># 차원 추가 (batch, seq_len, 1)</span></span><br><span class="line"></span><br><span class="line">      time_periodic = tf.math.sin(tf.multiply(x, <span class="variable language_">self</span>.weights_periodic) + <span class="variable language_">self</span>.bias_periodic)</span><br><span class="line">      time_periodic = tf.expand_dims(time_periodic, axis=-<span class="number">1</span>) <span class="comment"># 차원 추가 (batch, seq_len, 1)</span></span><br><span class="line">      <span class="keyword">return</span> tf.concat([time_linear, time_periodic], axis=-<span class="number">1</span>) <span class="comment"># shape = (batch, seq_len, 2)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="트랜스포머의-특징"><a href="#트랜스포머의-특징" class="headerlink" title="트랜스포머의 특징"></a>트랜스포머의 특징</h3><p>  시계열 데이터 분류, 회귀 문제에서는<br>  Decoder가 빠져있는 Self Attention을 사용한다.  (Fine Tunning)</p>
<p>  Decoder는 챗봇이나 Auto-Encoder같은 Encoder-Decoder 구조에 사용되는 경향이 있다.</p>
<p>  참고 논문 : <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=lE1AB4stmX">https://openreview.net/forum?id=lE1AB4stmX</a></p>
<p>  논문에서는 n_layer_num, padding_mask 를 쓰는 특징이 있다. 이 파라미터까지 쓸 수 있으면 좋겠지만<br>  GPU 메모리가 8기가밖에 안되서 논문 파라미터보다는 작게 설정을 해야 원활이 가능하다.<br>  그리고 dataset을 window 하면 GPU RAM 사용량을 더 낮출 수 있다.</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">Previous</a></div><div class="pagination-next"><a href="/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="MELoDy-sigmax"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">MELoDy-sigmax</p><p class="is-size-6 is-block">Musical Data blog</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Seoul</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">23</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">12</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Competition/"><span class="level-start"><span class="level-item">Competition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Game/"><span class="level-start"><span class="level-item">Game</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/"><span class="level-start"><span class="level-item">Study</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Study/Dacon/"><span class="level-start"><span class="level-item">Dacon</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/Game/"><span class="level-start"><span class="level-item">Game</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/Kaggle-Dacon/"><span class="level-start"><span class="level-item">Kaggle, Dacon</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-10-06T01:36:26.000Z">2023-10-06</time></p><p class="title"><a href="/2023/10/06/music2motion-project/">music2motion_project</a></p><p class="categories"><a href="/categories/Study/">Study</a> / <a href="/categories/Study/Game/">Game</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-15T23:28:46.000Z">2023-03-16</time></p><p class="title"><a href="/2023/03/16/%EA%B2%8C%EC%9E%84%EA%B0%9C%EB%B0%9C-%EC%9D%BC%EA%B8%B02/">게임개발-일기2</a></p><p class="categories"><a href="/categories/Game/">Game</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-16T09:23:41.000Z">2022-10-16</time></p><p class="title"><a href="/2022/10/16/rl-video-summ/">rl-video-summ</a></p><p class="categories"><a href="/categories/Study/">Study</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-05T07:26:50.000Z">2022-04-05</time></p><p class="title"><a href="/2022/04/05/hello-world/">Hello World</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-05T07:26:50.000Z">2022-04-05</time></p><p class="title"><a href="/2022/04/05/bbackcheem/">bbackcheem</a></p></div></article></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">September 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/07/"><span class="level-start"><span class="level-item">July 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/06/"><span class="level-start"><span class="level-item">June 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Audio/"><span class="tag">Audio</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DL/"><span class="tag">DL</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dacon/"><span class="tag">Dacon</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dev/"><span class="tag">Dev</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Algebra/"><span class="tag">Linear Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Try/"><span class="tag">Try</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unity/"><span class="tag">Unity</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/XR/"><span class="tag">XR</span><span class="tag">2</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Xeoner&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2024 Xeoner</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2024</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>