{"posts":[{"title":"Daconí›„ê¸°","text":"ì €ë²ˆì— ì´ì€ ë°ì´ì½˜ ë„ì „ í›„ê¸°2 ìˆœìœ„ ë°œí‘œ ìˆœê°„â€¦ ?!?!?!?! 1458 íŒ€ ì¤‘ 15ìœ„â€¦ ì™€ ì´ê±° ì‹¤í™”?! ëª¨ë¸ë§ì€ Conv-LSTM ì „ì²˜ë¦¬ ë°©ë²•ì€ ì„¤ëª…ë ¥ ë†’ì€ ë³€ìˆ˜, ë‹¨ìœ„ë©´ì ë‹¹ ì¼ì‚¬ëŸ‰ = ì‚°ë€ì¼ì‚¬ëŸ‰ + ì§ì ‘ ì¼ì‚¬ëŸ‰ GHI = DHI + DNI * \\(\\cos(\\theta)\\) í•˜ë£¨ì˜ ì „ì²´ ì‹œê°„ ì¤‘ì—ì„œ í•´ê°€ ëœ¨ê³  ì§ˆ ë•Œê¹Œì§€ë§Œ ê³„ì‚°í•˜ëŠ” ê²ƒì´ì˜€ë‹¤. ì´ê²ƒì´ ë°”ë¡œ ê·¸ ë°©ë²•ë“¤â€¦ 12345678910111213141516171819202122232425262728293031323334# GHI ê³„ì‚° ë°©ë²•temp['NoSun'] = np.where((temp['DHI'] &gt; 0) | (temp['DNI'] &gt; 0), 0, 1) temp['sunny'] = temp.groupby(['Day', temp.NoSun.cumsum()])['NoSun'].apply(lambda x: (x ^ 1).cumsum())temp['long'] = temp['sunny'].cummax()temp['angle'] = ((temp['sunny'] / temp['long']) * 180) - 90temp['GHI'] = temp['DHI'] + temp['DNI'] * temp['angle'].apply(lambda x: np.cos(np.pi * (x / 180)))# DataSet windowing í•˜ëŠ” ë°©ë²•def windowed_dataset(x, y, window_size, batch_size, shuffle, shuffle_size): ds_x = tf.data.Dataset.from_tensor_slices(x) ds_x = ds_x.window(window_size, shift = 1, stride = 1, drop_remainder=True) ds_x = ds_x.flat_map(lambda x: x.batch(window_size)) ds_y = tf.data.Dataset.from_tensor_slices(y) ds_y = ds_y.window(window_size, shift = 1, stride = 1, drop_remainder=True) ds_y = ds_y.flat_map(lambda x: x.batch(window_size)) ds = tf.data.Dataset.zip((ds_x, ds_y)) if shuffle: ds = ds.shuffle(shuffle_size) return ds.batch(batch_size).prefetch(1)def window_data_pred(data, window_size, batch_size): ds = tf.data.Dataset.from_tensor_slices(data) ds = ds.window(window_size, shift = 1, stride = 1) #, drop_remainder=True) # stride = 1 ds = ds.flat_map(lambda x: x.batch(window_size)) return ds.padded_batch(batch_size, padded_shapes=(None, 7)).prefetch(1) # 7ì€ Test Setì˜ íŠ¹ì§• ê°œìˆ˜ Test datasetì„ drop_remainder=True í•˜ë©´ ì•ˆë˜ëŠ” ì´ìœ ? Test Setì˜ ì •ë³´ê°€ sequence lengthë§Œí¼ ì—†ì–´ì§.","link":"/2021/01/30/Dacon%ED%9B%84%EA%B8%B0/"},{"title":"Daconë„ì „ê¸°","text":"ë°ì´ì½˜ ë„ì „ í›„ê¸° í¥ë¯¸ë¡­ì§€ë§Œ ì‰½ì§€ëŠ” ì•Šë‹¤. ë§ ê·¸ëŒ€ë¡œ ìƒˆë¡œìš´ íŒŒìƒë³€ìˆ˜, ëª¨ë¸ì„ ë§ì´ ë§Œë“¤ë©´ ì ìˆ˜ê°€ ì˜¤ë¥´ê¸´ í•œë‹¤. ë°ì´ì½˜_ì—°ìŠµë¬¸ì œ ì—°ìŠµìš©ì´ê¸´ í•œë° BERTëª¨ë¸ í­ê²©ìœ¼ë¡œ ì–‘ë¯¼í•™ì‚´ ì´ ë²Œì¨ë¶€í„° ì‹œì‘ë˜ì—ˆâ€¦ã…ã„·ã„· ë²„íŠ¸ëª¨ë¸ ì•ˆì“°ê³ ë„ ìˆœìœ„ ì˜¬ë¦´ìˆ˜ëŠ” ìˆë‹¤. ì§€ê¸ˆë¶€í„° ê·¸ ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. ë°ì´ì½˜ ìˆœìœ„ë¥¼ ì˜¬ë¦´ìˆ˜ ìˆëŠ” ë°©ë²• - ì°½ì˜ì ì¸ modeling, K-cross Validation, parameter searching VDCNNì€ GPUê°€ ì˜ ë²„í…¨ì£¼ë©´ 0.86ê¹Œì§€ëŠ” ë§ˆêµ¬ë§ˆêµ¬ ì˜¬ë¦´ìˆ˜ ìˆìŒ. ì´ê²ƒì´ ë°”ë¡œ VDCNNì´ë‹¹! í•™ìŠµ ëª¨ë¸ì„ ê·¸ë¦¼ìœ¼ë¡œ ë³´ê¸° ì—¬ê¸°ì— ì—¬ëŸ¬ ê°€ì§€ ëª¨ë¸ Ensemble í•´ë³´ë©´ 0.88ì€ ê°€ëŠ¥í•´ë³´ì¼ ê²ƒ ê°™ë„¤ìš”. ì¶”ê°€ë¡œ ëª¨ë¸ Ensemble ì„ ì‹œë„","link":"/2020/12/06/Dacon%EB%8F%84%EC%A0%84%EA%B8%B0/"},{"title":"ì¹˜ì—´í•œ-ë°ì´ì½˜í›„ê¸°","text":"ë°ì´ì½˜ ìµœê·¼ ëŒ€íšŒ í›„ê¸° - ë°ì´ì½˜ ì–´ë‚˜ë” ë²„ì „ í¥ë¯¸ë¡­ì§€ë§Œ quantile lossâ€¦ ë³µì¡í•˜ì§€ë§Œ í’€ì´ë²•ì€ ìˆë‹¤. ê·¸ëƒ¥ quantile ë³„ë¡œ ëŒë¦¬ì§€ ë§ê³  1.0ì— ëŒ€í•œ ê°’ì„ ì˜ˆì¸¡í•œ ë’¤ 0.5 ~ 1.0 ë²”ìœ„ë¡œ í•˜ë©´ ë;; loss function ìˆ˜ì‹êµ¬í˜„ì€ ì‰½ì§€ë§Œ lossë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ ê´€ê±´ì´ë‹¤. ì§„ì§œ 4.1ì—ì„œ ì¤„ì–´ë“¤ì§€ ì•ŠëŠ” lossâ€¦ ê³¼ì—° ì–´ë–»ê²Œ í•˜ë©´ ì¤„ì¼ìˆ˜ ìˆì„ê¹Œâ€¦ ë‹¤ë³€ëŸ‰ íšŒê·€ ì™œ ì´ë ‡ê²Œ ì–´ë ¤ìš¸ê¹Œìš”? - ë°ì´ì½˜ ëŒ€íšŒì—ì„œ ì–»ì€ ì—„ì²­ë‚œ êµí›ˆ loss ê³„ì‚°í• ë•Œ ë‹¤ë¥¸ target ê°’ê³¼ x_trainê°’ì´ ì•„ë‹Œ ë‹¤ë¥¸ Element ë¼ë¦¬ ê³„ì‚°ë˜ì–´ì„œ ì˜¤íˆë ¤ lossê°€ í­ì¦í•˜ê¸°ë„ í•œë‹¤ ì´ë ‡ê²Œ í•˜ë©´ 8:45 ëœë‹¤. Wandbë¡œ ì‹œê°í™” ì•ˆí–ˆìœ¼ë©´ í°ì¼ë‚¬ì—ˆì„ë“¯ ì‹¶ì—ˆë‹¤. Loss ì‹œê°í™” íˆ´ë¡œ ë³´ë‹ˆ ì´ˆë°˜ì—ì„œ ìˆ˜ë ´í•˜ì§€ ì•ŠëŠ” ë¬¸ì œë‚˜ ì•„ì˜ˆ uìë¡œ ë¶ˆê·œì¹™ì ìœ¼ë¡œ íŠ€ëŠ” í˜„ìƒë„ ë°œìƒâ€¦. ë¶„ëª… window datasetìœ¼ë¡œ ë¶€í•˜ë¶„ì‚°ì„ í•˜ë©´ ë°ì´í„° ì²˜ë¦¬ê°€ ìˆ˜ì›”í•  ê²ƒ ê°™ì§€ë§Œâ€¦ì´ë ‡ê²Œ ë˜ë©´ ë—;;; ì±„ì ì„ í•´ë³´ë‹ˆ ê·¹ì•…ì˜ Lossê°€ ë‚˜ì˜¤ê¸°ë„ í•œë‹¤. ì—ì¸¡ labelê³¼ train testì˜ shapeì„ í™•ì¸ ë˜ í™•ì¸.. ë–„ì—ëŠ” window_size, buffer sizeë¥¼ ì˜ ì¡°ì ˆí•´ì•¼ í•œë‹¤. window_sizeë¥¼ ë„ˆë¬´ ë‚®ì¶”ë©´ ìì¹«í•˜ë©´ GPU ì‚¬ìš©ë¥ ì€ í•œìë¦¬ì— ë¨¸ë¬¼ê³  ì‹œê°„ì€ ì˜¤ë˜ ê±¸ë¦°ë‹¤. í•˜ì§€ë§Œ GPU RAM ì‚¬ìš©ëŸ‰ì€ ì¤„ì–´ë“œëŠ” ì´ì ì€ ìˆë‹¤. window datasetì„ í•˜ë‹¤ë³´ë©´ ë°°ì¹˜ í¬ê¸°ê°€ ì•ˆ ë§ì•„ì„œ ì˜ˆì¸¡ì´ ì•ˆë ë•Œê°€ ìˆëŠ”ë° ì´ëŸ´ë•â€¦ Shuffle Buffer size ì¡°ì ˆì€ í•„ìˆ˜, batch Sizeì— ì£¼ì˜í•˜ì—¬ ì¡°ì ˆí•œë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´(BATCH SIZEê°€ êµ‰ì¥íˆ ë†’ë‹¤ë©´) 1WARNING:tensorflow:5 out of the last 5 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7f2570221d30&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. ì´ëŸ° ê²½ê³ ê°€ ë‚˜ì˜¨ë‹¤.. -&gt; batch sizeë¥¼ ë°˜ë“œì‹œ ì¡°ì ˆí•˜ê±°ë‚˜ batch paddingâ€¦","link":"/2021/01/02/Dacon%ED%9B%84%EA%B8%B0_2/"},{"title":"GANí”„ë¡œì íŠ¸_try","text":"Style GAN toy í”„ë¡œì íŠ¸StyleGANì˜ íŠ¹ì§• ì´ë¯¸ì§€ë¥¼ Styleì˜ ì¡°í•©ìœ¼ë¡œ ë³´ê³ Generatorì˜ ê° Layerë§ˆë‹¤ Style ì •ë³´ë¥¼ ì…íˆëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ í•©ì„±ì´ ë•Œ ê° Layerì—ì„œ ì¶”ê°€ë˜ëŠ” Styleì€ ì´ë¯¸ì§€ì˜ Coarse Feature(í¬ì¦ˆ, ì„±ë³„ ë“±)ë¶€í„°Fine Detail(ë¨¸ë¦¬ìƒ‰, í”¼ë¶€í†¤ ë“±)ê¹Œì§€ê°ê¸° ë‹¤ë¥¸ Levelì˜ Visual ì†ì„±ë“¤ì„ ì¡°ì ˆ ê°€ëŠ¥StyleGANì€ ìƒê°ë³´ë‹¤ ì•ˆì •ì ì´ê³  ë†’ì€ í€„ë¦¬í‹°ì˜ ì´ë¯¸ì§€ ìƒì„± ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°(Module) GANì´ë€ ì–´ë–¤ ê²ƒì¼ê¹Œ??? Instance Norm? Generator êµ¬ì¡° ì„¤ëª… ì™¼ìª½ì´ Traditional Network, ì˜¤ë¥¸ìª½ì´ ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ Style-gased Generator. ì™¼ìª½ ë„¤íŠ¸ì›Œí¬ì™€ ì˜¤ë¥¸ìª½ì— Synthesis Networkê°€ ë˜‘ê°™ì€ êµ¬ì¡°ë¥¼ ê°–ê³  ìˆì§€ë§Œ,ì´ì „ GANì—ì„œëŠ” Latent zë¥¼ ë°”ë¡œ Inputìœ¼ë¡œ ë„£ì–´ì¤¬ë˜ ê²ƒê³¼ëŠ” ë‹¤ë¥´ê²Œ,StyleGANì—ì„œëŠ” í•™ìŠµëœ Constant, (w) ê°’ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•¨. ìƒˆë¡­ê²Œ Mapping Networkì™€ Noiseê°€ ì¶”ê°€ë¨.. Wë¥¼ Featureì— ë§¤í•‘í•˜ëŠ” ê²½ìš°WëŠ” Zì²˜ëŸ¼ ê³ ì •ëœ ë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠìŒ. Sampling densityëŠ” í•™ìŠµëœ Piecewise Continuous Mapping f(z)(fëŠ” Mapping Network ì…ë‹ˆë‹¤)ì— ì˜í•´ ì •í•´ì§. ë”°ë¼ì„œ, Warping(í‹€ì–´ì§)ì´ ë§ì´ ì¼ì–´ë‚˜ì§€ ì•ŠìŒ.ê·¸ë ‡ê¸° ë•Œë¬¸ì— Factors of variationì€ ë”ìš± Linearí•˜ê³ , Disentangled (ì–½íˆì§€ ì•ŠìŒ).ì´ê²ƒì´ ë°”ë¡œ zë¥¼ ê³§ë°”ë¡œ Featureì— ë§¤í•‘í•˜ëŠ” ê²ƒë³´ë‹¤ wì— ë§¤í•‘í•˜ëŠ” ê²ƒì˜ ì¥ì ì…ë‹ˆë‹¤ ê¸°ì¡´ì˜ Generator (a)ëŠ”Input Latent Vector (z)ê°€ ì§ì ‘ Convolution, Upsampling ë“±ì„ ê±°ì³ ì´ë¯¸ì§€ë¡œ ë³€í™˜ë˜ëŠ” êµ¬ì¡°. Style-based Generator (b) ì˜ ê²½ìš°,(z)ê°€ Fully-connected Layerë¡œ êµ¬ì„±ëœ Mapping Networkì„ ê±°ì³Intermediate Latent Vector (w) ë¨¼ì € ë³€í™˜. (w)ëŠ” Constant Tensorê°€ ì´ë¯¸ì§€ë¡œ ë³€í™˜ë˜ëŠ” ê³¼ì •ì—ì„œìŠ¤íƒ€ì¼ì„ ì…íˆëŠ” ì—­í• ì„ ìˆ˜í–‰. ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±. Style Transferë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°€ëŠ¥ì¼€í•˜ëŠ” Adaptive Instance Norm Synthesis Network (í•©ì„± ë„¤íŠ¸ì›Œí¬)zë¥¼ ì¤‘ê°„ latent space Wì— ë§¤í•‘ì„ í•œ ë’¤ì— ì´ wëŠ” â€œAâ€ë¥¼ ê±°ì³ì„œ style, y=(ys,yb)ë¡œ ë³€í˜•ë¨. ì´ë•Œ AëŠ” í•™ìŠµëœ affine transform ì„. ê·¸ë¦¬ê³  ì´ styleë“¤ì€AdaIN(adaptive instance normalization) opeartionì„ control í•¨. AdaINì€ style transferë¥¼ í•  ë•Œ ë§ì´ ì“°ì´ëŠ” ë°©ë²•ìœ¼ë¡œ, ì„ì˜ì˜ style transferë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°€ëŠ¥í•˜ê²Œ í•¨.ì—¬ê¸°ì„œ feature map xiëŠ”normalized ëœ ë‹¤ìŒì—, styleë¡œ ë³€í™˜ëœ ë‘ yë¡œ scaled, biased ë¨. (styleì´ ì…í˜€ì§)ì´ ê³¼ì •ì„ ë§¤ layer ë§ˆë‹¤ ë°˜ë³µí•¨. ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ë°©ë²•ì€ scale-specific control ì„ ê°€ëŠ¥í•˜ê²Œ í•¨.","link":"/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/04/05/hello-world/"},{"title":"rl-video-summ","text":"ë¹„ë””ì˜¤ ìš”ì•½ ëª¨ë¸ í˜„ì‹¤ì—ì„œ í•˜ì´ë¼ì´íŠ¸ ì¥ë©´ í¸ì§‘í•˜ë ¤ê³  í•˜ë©´ í¸ì§‘ í”„ë¡œê·¸ë¨ì´ ë„ˆë¬´ ë¹„ì‹¸ë‹¤. ê°„ë‹¨íˆ ì„¤ê³„í•  ìˆ˜ ìˆëŠ” YasuoNet ê°™ì€ ì§€ë„í•™ìŠµ ëª¨ë¸ì€ ë°ì´í„°ì…‹ì˜ í•˜ì´ë¼ì´íŠ¸ë¥¼ ì¼ì¼ì´ ë¼ë²¨ë§ì´ ë“¤ì–´ê°„ë‹¤. ë§Œì•½ ì‹œê°„ì´ ê¸´ ë¹„ë””ì˜¤ ë°ì´í„°ë¼ë©´?? ë§ë„ ì•ˆë¨. GPU ì„±ëŠ¥ì´ ë¬´ì–´ì˜ ë²•ì¹™ìœ¼ë¡œ ì ì°¨ í–¥ìƒë˜ì–´ ë¹„ë””ì˜¤ì—ë„ ê°•í™”í•™ìŠµì„ ì ìš©í•  ìˆ˜ ìˆì„ì§€ ì•Šì„ê¹Œ ìƒìƒë§Œ í–ˆì—ˆëŠ”ë° ì´ë¯¸ 5ë…„ì „â€¦","link":"/2022/10/16/rl-video-summ/"},{"title":"music2motion_project","text":"ê²Œì„ì— ë”¥ëŸ¬ë‹ ì ìš©í•˜ê¸° 1ë°ì´í„° ì¤€ë¹„ ê²Œì„ì— ì‚¬ìš©ë  ë°ì´í„° ì†Œê°œ ìŒì•…ì´ í”Œë ˆì´ë˜ëŠ” ë™ì•ˆ ë””ë°”ì´ìŠ¤ë¥¼ ë¦¬ë“¬ì— ë§ì¶° ì‹ ë‚˜ê²Œ í”ë“œëŠ” ë°ì´í„° ìƒ˜í”ŒëŸ¬ ì•± ëª¨ì…˜ ì…ë ¥ê°’ Setter ì— ì„ê³„ì¹˜ ì´ìƒì˜ ê°’ì´ ë“¤ì–´ì˜¤ëŠ” ë¡œì§ ì ìš© 12345678910111213141516TimeStep, accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z, gyro_w2.047982, -0.716, -0.788, -0.106, -0.247, 0.276, -0.038, -0.9282.581315, -0.948, 1.532, -1.504, 0.298, 0.315, 0.463, -0.7732.986644, -1.214, 2.212, -0.735, 0.465, 0.346, 0.701, -0.4163.071995, -0.765, 0.496, -0.480, 0.461, 0.274, 0.685, -0.4933.455986, -0.756, 0.031, -0.393, 0.433, 0.235, 0.704, -0.5114.799977, -1.122, -1.067, -0.935, -0.256, -0.328, -0.905, 0.0875.141315, -3.009, -0.402, -2.298, -0.220, -0.489, -0.843, -0.0326.655986, 0.034, -0.171, 0.688, -0.239, -0.668, -0.694, 0.1219.002653, -0.078, 0.931, 3.289, -0.315, -0.892, -0.242, -0.2189.066644, -1.996, -0.112, -0.943, -0.564, -0.627, -0.443, -0.30410.19732, 0.473, -0.018, 0.035, -0.081, -0.661, -0.726, -0.17210.32533, -0.627, -0.682, -1.279, -0.292, -0.586, -0.740, 0.15610.79465, -1.628, -0.304, -0.207, -0.489, -0.731, -0.463, -0.11211.56265, -0.507, 0.267, 0.217, -0.067, -0.658, -0.697, -0.27711.64798, -0.146, 1.522, 0.850, -0.206, -0.642, -0.736, -0.063 ëª¨ì…˜ ë°ì´í„°ë¥¼ ê²Œì„ì— ì ìš©í•˜ê¸° ìœ ë‹ˆí‹° ìŠ¤í¬ë¦½íŠ¸ë¡œ íŒŒì¼ ë¡œë”ë¥¼ ë§Œë“¤ê±°ë‚˜ ì–¸ë¦¬ì–¼ ë¸”ë£¨í”„ë¦°íŠ¸ë¥¼ ì´ìš©í•˜ì—¬ ê²Œì„ ê·¸ë˜í”½ì— ì ìš© to be continuedâ€¦","link":"/2023/10/06/music2motion-project/"},{"title":"Multi-GPU","text":"ì‘ê³¡ GAN x Multi-GPUì‘ê³¡ GAN ë§Œë“¤ì–´ë³´ê¸° ìŒì•… ì‘ê³¡ ì „ì²˜ë¦¬ ë°©ë²• : midiíŒŒì¼ì„ ì…ì¶œë ¥ìœ¼ë¡œ ì‚¬ìš© pypianorollë¡œ ê° íŠ¸ë™ì˜ ì•…ê¸°, í‚¤, ì½”ë“œ, í™”ìŒ ê³„ì‚° ë°˜ì˜ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° Generator input : Single Melody track (Timestep, n_pitch, n_tracks) Latent noise Vector z: (2, 8, 512) U-Net êµ¬ì¡°ë¡œ ë˜ì–´ìˆìŒ. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384 def _conv2d(layer_input, filters, f_size=4, bn=True): # ë‹¤ìš´ìƒ˜í”Œë§ d = keras.layers.Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input) d = keras.layers.LeakyReLU(alpha=0.2)(d) if bn: d = keras.layers.BatchNormalization(momentum=0.8)(d) return d def _deconv2d(layer_input, pre_input, filters, f_size=4, dropout_rate=0): # ì—… ìƒ˜í”Œë§ u = keras.layers.UpSampling2D(size=2)(layer_input) u = keras.layers.Conv2D(filters, kernel_size=f_size, strides=1, padding='same')(u) u = keras.layers.BatchNormalization(momentum=0.8)(u) u = keras.layers.ReLU()(u) if dropout_rate: u = keras.layers.Dropout(dropout_rate)(u) u = keras.layers.Concatenate()([u, pre_input]) return u def build_generator(condition_input_shape=(32, 128, 1), filters=64, instruments=4, latent_shape=(2, 8, 512)): # ì œë„¤ë ˆì´í„° ë¸”ë¡ c_input = keras.layers.Input(shape=condition_input_shape) z_input = keras.layers.Input(shape=latent_shape) d1 = _conv2d(c_input, filters, bn=False) d2 = _conv2d(d1, filters * 2) d3 = _conv2d(d2, filters * 4) d4 = _conv2d(d3, filters * 8) d4 = keras.layers.Concatenate(axis=-1)([d4, z_input]) u4 = _deconv2d(d4, d3, filters * 4) u5 = _deconv2d(u4, d2, filters * 2) u6 = _deconv2d(u5, d1, filters) u7 = keras.layers.UpSampling2D(size=2)(u6) output = keras.layers.Conv2D(instruments, kernel_size=4, strides=1, padding='same', activation='tanh')(u7) # 32, 128, 4 generator = keras.models.Model([c_input, z_input], output, name='Generator') return generator ``` 2. Discriminator ![U-net](/img/Multi-GPU/ë©€í‹°ì§€í“¨_03.png) ```python def _build_discriminator_layer(layer_input, filters, f_size=4): # input: [batch_size, in_channels, H, W] # output: [batch_size, out_channels, H/2, W/2] d = keras.layers.Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input) # DiscriminatorëŠ” BatchNormì„ ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤!! d = keras.layers.LeakyReLU(alpha=0.2)(d) return d def build_discriminator(pianoroll_shape=(32, 128, 4), filters=64): # WGAN Discriminator(ë¹„í‰ì) condition_input_shape = (32,128,1) groundtruth_pianoroll = keras.layers.Input(shape=pianoroll_shape) condition_input = keras.layers.Input(shape=condition_input_shape) combined_imgs = keras.layers.Concatenate(axis=-1)([groundtruth_pianoroll, condition_input]) d1 = _build_discriminator_layer(combined_imgs, filters) d2 = _build_discriminator_layer(d1, filters * 2) d3 = _build_discriminator_layer(d2, filters * 4) d4 = _build_discriminator_layer(d3, filters * 8) x = keras.layers.Flatten()(d4) logit = keras.layers.Dense(1)(x) discriminator = keras.models.Model([groundtruth_pianoroll,condition_input], logit, name='Discriminator') return discriminator GANëª¨ë¸ lossí•¨ìˆ˜ì™€ êµ´ë¦¬ëŠ” ë°©ë²• Generator Loss í•¨ìˆ˜ : Discriminator lossí•¨ìˆ˜ì™€ ë°˜ëŒ€ì˜ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. ì œë„¤ë ˆì´í„°ëŠ” pianorollì„ ê°€ëŠ¥í•œ í•œ ë” ë¦¬ì–¼í•˜ê²Œ ë§Œë“¤ì–´ì•¼ í•˜ê¸°ë•Œë¬¸. $\\frac{1}{m} \\sum_{i=1}^{m} -D_w(G(z^{i}|c^{i})|c^{i})$ 12345def generator_loss(discriminator_fake_output):&quot;&quot;&quot; Wasserstein GAN loss(Generator) -D(G(z|c))&quot;&quot;&quot;return -tf.reduce_mean(discriminator_fake_output) Discriminator Loss í•¨ìˆ˜: ì§„ì§œ Pianorollê³¼ ìƒì„±ëœ pianoroll ë¶„í¬ì˜ ê±°ë¦¬ë¥¼ ìµœëŒ€í™” í•˜ê¸° ìœ„í•´ Wasserstein loss í•¨ìˆ˜ë¥¼ ì‚¬ìš©. $\\frac{1}{m} \\sum_{i=1}^{m} [D_w(G(z^{i}|c^{i})|c^{i}) - D_w(x^{i}|c^{i})]$ 123456def wasserstein_loss(discriminator_real_output, discriminator_fake_output):&quot;&quot;&quot; Wasserstein GAN loss(Discriminator) D(G(z|c)) - D(x|c)&quot;&quot;&quot;return tf.reduce_mean(discriminator_fake_output) - tf.reduce_mean( discriminator_real_output) gradient penalty loss(aka WGAN-GP loss)ë¥¼ ì‚¬ìš©í•œë‹¤. ê·¸ ì´ìœ ëŠ” Dì— ëŒ€í•œ gradientë¥¼ ì ì ˆíˆ ì»¨íŠ¸ë¡¤ í•˜ëŠ”ë° ì í•©í•˜ê¸° ë–„ë¬¸ì´ê³  Gì˜ ìµœì í™”ì— ë„ì›€ì„ ì¤€ë‹¤. $\\frac{1}{m} \\sum_{i=1}^{m}(\\lVert \\nabla_{\\hat{x}^i}D_w(\\hat{x}^i|c^{i}) \\rVert_2 - 1)^2 $ 123456789101112131415161718def compute_gradient_penalty(discriminator, x, fake_x):c = tf.expand_dims(x[..., 0], -1)batch_size = x.get_shape().as_list()[0]eps_x = tf.random.uniform( [batch_size] + [1] * (len(x.get_shape()) - 1)) # B, 1, 1, 1, 1inter = eps_x * x + (1.0 - eps_x) * fake_xwith tf.GradientTape() as g: g.watch(inter) disc_inter_output = discriminator((inter,c), training=True)grads = g.gradient(disc_inter_output, inter)slopes = tf.sqrt(1e-8 + tf.reduce_sum( tf.square(grads), axis=tf.range(1, grads.get_shape().ndims)))gradient_penalty = tf.reduce_mean(tf.square(slopes - 1.0))return gradient_penalty GAN ëª¨ë¸ ëŒë¦¬ëŠ” ë°©ë²• Gì™€ Dì— Adam ìµœì í™” í•¨ìˆ˜ë¥¼ ì“´ë‹¤.(ì •êµí•˜ê²Œ í•˜ë ¤ë©´ SGD ë¥¼ ì¨ì•¼ í•œë‹¤.) ì²´í¬í¬ì¸íŠ¸ë¥¼ ì¨ì„œ ë§¤ë²ˆ ëª¨ë¸ì„ ì €ì¥í•œë‹¤. ëŒë ¤ë³´ëŠ” í•¨ìˆ˜. Gë¥¼ ëŒë¦¬ëŠ” í•¨ìˆ˜ ë¶€ë¶„ 1234567891011121314151617181920212223242526272829@tf.functiondef generator_train_step(x, condition_track_idx=0): ############################################ # Gë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤.: maximize D(G(z|c)) ############################################ # ì¡°ê±´ë¶€ íŠ¸ë™ì„ ë½‘ì•„ì„œ real batch pianorollì„ ë§Œë“ ë‹¤. c = tf.expand_dims(x[..., condition_track_idx], -1) # latent vectorsì˜ batch dataë¥¼ ë§Œë“ ë‹¤. z = tf.random.truncated_normal([BATCH_SIZE, 2, 8, 512]) with tf.GradientTape() as tape: fake_x = generator((c, z), training=True) fake_output = discriminator((fake_x,c), training=False) # G ê²°ê³¼ë¬¼ì˜ Loss ê³„ì‚°í•œë‹¤. gen_loss = generator_loss(fake_output) # Gì˜ gradientë¥¼ ê³„ì‚°í•œë‹¤. gradients_of_generator = tape.gradient(gen_loss, generator.trainable_variables) # ì œë„¤ë ˆì´í„°ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤. generator_optimizer.apply_gradients( zip(gradients_of_generator, generator.trainable_variables)) return gen_loss Dë¥¼ ëŒë¦¬ëŠ” í•¨ìˆ˜ ë¶€ë¶„ 1234567891011121314151617181920212223242526272829303132333435363738394041@tf.functiondef discriminator_train_step(x, condition_track_idx=0):#############################################################################(2) Dë¥¼ ì—…ë°ì´íŠ¸: (D(x|c)) + (1 - D(G(z|c))|c) + GradientPenality() ë¥¼ ìµœëŒ€í™”############################################################################# ì¡°ê±´ë¶€ íŠ¸ë™ì„ ë½‘ì•„ì„œ real batch pianorollì„ ë§Œë“ ë‹¤.c = tf.expand_dims(x[..., condition_track_idx], -1)# latent vectorsì˜ batch dataë¥¼ ë§Œë“ ë‹¤.z = tf.random.truncated_normal([BATCH_SIZE, 2, 8, 512])# í›¼ì´í¬ pianorollì„ ë§Œë“ ë‹¤.fake_x = generator((c, z), training=False)# Dì˜ íŒŒë¼ë¯¸í„°ë“¤ ì—…ë°ì´íŠ¸with tf.GradientTape() as tape: real_output = discriminator((x,c), training=True) fake_output = discriminator((fake_x,c), training=True) discriminator_loss = wasserstein_loss(real_output, fake_output)# real, fake batchì˜ gradientë¥¼ ê³„ì‚°grads_of_discriminator = tape.gradient(discriminator_loss, discriminator.trainable_variables)with tf.GradientTape() as tape: gp_loss = compute_gradient_penalty(discriminator, x, fake_x) gp_loss *= 10.0# real, fake batchì˜ GP-lossë¥¼ ê³„ì‚°grads_gp = tape.gradient(gp_loss, discriminator.trainable_variables)gradients_of_discriminator = [g + ggp for g, ggp in zip(grads_of_discriminator, grads_gp) if ggp is not None]# Dë¥¼ ì—…ë°ì´íŠ¸ í•´ì¤€ë‹¤.discriminator_optimizer.apply_gradients( zip(gradients_of_discriminator, discriminator.trainable_variables))return discriminator_loss + gp_loss Multi-GPU ëŒë¦¬ëŠ” ë°©ë²• ì†Œê°œ í…ì„œí”Œë¡œ MirroredStrategy ë¥¼ ì‚¬ìš©(ê³„ì‚° ìš©ì´ì„±) ì–´ë–»ê²Œ ëŒë¦¬ëŠ”ì§€?? 12345678910111213# 1. Strategy ë§Œë“¤ê³ strategy = tf.distribute.MirroredStrategy()FLAG = Trueif strategy.num_replicas_in_sync &gt; 1 and FLAG: MULTIPLE_BATCH = strategy.num_replicas_in_sync print(f'ë¶„ì‚°í™˜ê²½ ì‚¬ìš© &gt;&gt; GPU: {MULTIPLE_BATCH}')else: print(f'ë¶„ì‚°í™˜ê²½ ë¯¸ì‚¬ìš©') MULTIPLE_BATCH = 1# 2. ëª¨ë¸ì„ strategy ì•ˆì— í¬í•¨ì‹œí‚´.# ì¤‘ìš”í•œ ë¶€ë¶„ : í•™ìŠµ í•¨ìˆ˜ì—ì„œ ë‚˜ì˜¨ lossë¥¼ self.strategy.runì— ë„£ê³  strategy.reduce# ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ num_replica_in_sync ê°¯ìˆ˜ë§Œí¼ ê³±í•œë‹¤.# ì¶œë ¥ì„ í¬í•¨í•œ ìì„¸í•œ ì½”ë“œëŠ” My GitHub...","link":"/2021/09/06/Multi-GPU/"},{"title":"bbackcheem","text":"ìš°ë¶„íˆ¬ ë™ì  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¹Œë“œ í›„ ì•ˆë ë•Œâ€¦(feat. CUDA) CUDA ì„¸íŒ…ì¤‘ ldd ì˜ì¡´ì„± ì²´í¬ì—ì„œ libc10.so íŒŒì¼ì´ ë§í¬ í™•ì¸í•  ë•Œ 1IFS=':' ; for i in $LD_LIBRARY_PATH; do ls -l $i/libc10.so 2&gt;/dev/null;done ì—¬ê¸°ì—ì„œ ì•„ë¬´ê²ƒë„ ë¦¬í„´ ì•ˆë˜ë©´ ë§í¬ê°€ ì•ˆë˜ì–´ ìˆëŠ” ìƒíƒœì„. í•´ê²°ì±… export LD_LIBRARY_PATH = /somepath/bin:$LD_LIBRARY_PATH ld.so.conf.d/someconf.conf ì…‹íŒ…","link":"/2022/04/05/bbackcheem/"},{"title":"ê²Œì„ê°œë°œ ì¼ê¸°","text":"ê²Œì„ê°œë°œ í›„ê¸°ë°ì´í„°ë¥¼ ëª¨ìœ¼ê¸° ìœ„í•œ Android ì•± ë§Œë“¤ê¸°ì•±ì„ ìµœì í™”í•˜ë ¤ë©´ ë¼ì´ë¸ŒëŸ¬ë¦¬ íŠ¹ì„±, ìë£Œ êµ¬ì¡°ë¥¼ ì•Œì•„ì•¼ ê°€ëŠ¥. ì•± ìµœì í™” ì‹œë„StreanWriterì— WriteLineì„ ì¨ì„œ ë¡œê¹…ì„ í•˜ê²Œëœë‹¤ë©´ì¼ë°˜ì ì¸ File.CreateTextì— ë¹„í•´ì„œ ë¦¬ì†ŒìŠ¤ë¥¼ ê²ë‚˜ê²Œ(!) ë§ì´ ë¨¹ëŠ” í˜„ìƒì´ ë°œìƒí•œë‹¤.ê³„ì† ë²„í¼ê°€ ì—´ë ¤ ìˆìœ¼ë©´ ì˜¤ë²„í—¤ë“œê°€ ë‚˜ê¸° ì‹­ìƒì´ê¸° ë•Œë¬¸ì´ë‹¤.ë‹¨ìˆœí•œ ë°ì´í„°ì¼ ê²½ìš°ì—ëŠ” ë‹¨ìˆœí•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¨ì•¼ í•œë‹¤.ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë¦¬ì†ŒìŠ¤ ì†Œëª¨ ê¸‰ì¦ê³¼ ì—„ì²­ë‚œ ë©”ëª¨ë¦¬ ì‚¬ê³ ê°€ ë°œìƒí•œë‹¤.ë¦¬ì†ŒìŠ¤ ì†Œëª¨ëŸ‰ ê¸‰ì¦ì„ ë§‰ê¸° ìœ„í•´ì„œëŠ” StreamWriter ëŒ€ì‹  ë¬¸ìì—´ì„ ê³„ì† ë”í•´ì£¼ëŠ” StringBuilderë¥¼ ì‚¬ìš©í•˜ì. ë¼ì´ë¸ŒëŸ¬ë¦¬ í•˜ë‚˜ë§Œ ë°”ê¿¨ì„ ë¿ì¸ë°â€¦ìµœì í™” ì´í›„ì—ëŠ” ì˜ˆì „ë³´ë‹¤ cpu ì ìœ  ì‹œê°„ì´ ì—„ì²­ë‚˜ê²Œ ê°œì„ ë˜ì—ˆë‹¤. ê·¸ ì´ì „ì—ëŠ” ì—„ì²­ë‚œ ë ‰, ëŠê¹€ í¬ë¦¬í‹°ì»¬.(update í•¨ìˆ˜ì— WriteLineì„ ì“°ê²Œë˜ë©´ í™”ë©´ í”„ë ˆì„ì´ ì´ˆë‹¹ 10í”„ë ˆì„ìœ¼ë¡œ ë‚®ì•„ì§€ê³  ë ‰, ì‹±í¬ ì–´ê¸‹ë‚¨ í¬ë¦¬í‹°ì»¬.) ê²Œì„ê°œë°œ ì•ì„œ ì„ í˜•ëŒ€ìˆ˜ëŠ” í•„ìˆ˜ ë¡œì»¬ê³¼ ì›”ë“œ ì¢Œí‘œê³„ë¥¼ í—·ê°ˆë¦¬ë©´ ë§¤ìš° í°ì¼ë‚œë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼ Collider, ì†ë„ ê°€ì†ë„ ë“± ë¬¼ë¦¬ì§€ì‹ë„ í•„ìˆ˜.. ìºë¦­í„°ê°€ ì´ìƒí•˜ê²Œ ì›€ì§ì´ëŠ” ë²„ê·¸ë„ ë‚˜ê¸° ì‹­ìƒì´ë‹¤. (to be continueâ€¦) ìµœì í™” ë°©ë²•ì€â€¦API ë¬¸ì„œë¥¼ ì¼ë‹¨ ë³´ë©´ì„œ ê° í•¨ìˆ˜ì˜ íŠ¹ì§•ì„ ì˜ ì•Œê³  ì½”ë”©ì„ í•´ì•¼ í•œë‹¤.ë¡œì§ì€ ìµœëŒ€í•œ ë‹¨ìˆœí™”(ì—¬ëŸ¬ ê°€ì§€ ë³€ìˆ˜ë¥¼ ì»¨íŠ¸ë¡¤ í•˜ë ¤ë©´ GameManagerë¥¼ ë‘ëŠ” ë°©ë²•)íŠ¹íˆ ì‹¤í–‰ì¤‘ ë¡œì§ì— ì ˆëŒ€ Find() SendMessage() ê°™ì€ í•¨ìˆ˜ë¥¼ ì“°ë©´ ìµœì†Œ 1000ë°° ëŠë ¤ì§„ë‹¤. ë©”ëª¨ë¦¬ í™ ì¬í• ë‹¹ì€ ë ‰ì˜ ê·¼ë³¸ ì›ì¸â€¦ (GC ë°œë™ì„ ìµœëŒ€í•œ ë§‰ì•„ì•¼ í•œë‹¤ !!) ë¬¸ìì—´ ì†ì„±ì— ìì£¼ ì ‘ê·¼í•˜ë©´ ê·¸ë§Œí¼ ì†ë„ ì €í•˜ ìœ ë‹ˆí‹°ì—ì„œ í´ë˜ìŠ¤ëŠ” ì°¸ì¡° í˜•ì‹ì´ë¼ GC ëŒ€ìƒì„. Destroy ìì£¼ ì“°ì§€ ë§ê³  ì˜¤ë¸Œì íŠ¸ í’€ë§â€¦ CPUëŠ” ë‚˜ëˆ—ì…ˆì´ ê³±ì…ˆë³´ë‹¤ ë” ëŠë¦¼. â€“&gt; ì œê³±ê·¼(ë£¨íŠ¸)ë¥¼ ê³„ì‚°í•˜ëŠ” ê²½ìš°ëŠ” 1/2ë¥¼ ì œê³±í•œë‹¤.","link":"/2021/06/17/%EA%B2%8C%EC%9E%84%EA%B0%9C%EB%B0%9C-%EC%9D%BC%EA%B8%B0/"},{"title":"ê²Œì„ê°œë°œ-ì¼ê¸°2","text":"ê²Œì„ ê°œë°œ 2ë²ˆì§¸ í¬ìŠ¤íŠ¸ ë°ì´í„° ìƒ˜í”Œë§ ë°©ë²•ì— ê´€í•œ ìŠ¤í„°ë”” https://forum.unity.com/threads/solved-how-to-sample-transform-position-with-a-precision-of-a-frequency-of-60hz-with-unity3d.442160/ ê°„ë‹¨í•œ ë°©ë²• : FixedUpdate ì„¤ì • ì•½ê°„ ë” ì •í™•í•œ ë°©ë²• : Realtime íƒ€ì´ë¨¸ë¥¼ ë§Œë“¤ì–´ì„œ Multi-Thread lockingì„ ì‚¬ìš©(íƒ€ì´ë¨¸ì™€ ìƒ˜í”Œë§ ë¡œì§ì„ ìŠ¤ë ˆë“œë³„ë¡œ ë¶„ë¦¬) ë‘ë‘¥~ ë“œ. ë””. ì–´ ë°ì´í„° ì¶”ì¶œ!! ì•—â€¦! ì›ë˜ ë°ì´í„° í•˜ì§€ë§Œ ë­”ê°€ ë¹„ì•½í–ˆì—ˆë‹¤. ìƒ˜í”Œë§í•œ ë””ë°”ì´ìŠ¤ ì„¼ì„œ ê°’ë“¤ì´ ì¤‘ë³µì´ ìˆì—ˆë‹¤. ê·¸ëƒ¥ Input.accel.y ë‹¨ìˆœíˆ ì´ë ‡ê²Œ ìƒ˜í”Œë§í•˜ë©´ ì´ˆë‹¹ í”„ë ˆì„ ë ˆì´íŠ¸ê°€ ì €í•˜ë˜ê³  ë°ì´í„°ê°€ ë°€ë¦¬ê±°ë‚˜ ì‹±í¬ê°€ ì–´ê¸‹ë‚˜ì„œ ë§ì€ ë°ì´í„° ì¤‘ë³µì´ ë°œìƒí•œë‹¤. ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ë³´ë‹¤ êµ¬í˜„ ë‚œì´ë„ ìˆëŠ” í¸. ê°œì„ ëœ ë°ì´í„° ë©€í‹°ìŠ¤ë ˆë“œ likeí•œ ì½”ë£¨í‹´ìœ¼ë¡œ íƒ€ì´ë¨¸ ë¡œì§ì„ ë§Œë“¤ì–´ ìƒ˜í”Œë§í•œ ë°ì´í„°ë‹¤. ì•½ê°„ ë³´ê¸° ì¢‹ê³  ê¹”ë”í•˜ë‹¤. ë‹¤í–‰íˆ ë°ì´í„° ì¤‘ë³µí˜„ìƒë„ ê±°ì˜ ì—†ì–´ì¡Œë‹¤. ìœ ë‹ˆí‹° í”„ë¡œì íŠ¸ ì…‹íŒ…ë§Œ ê±´ë“¤ë©´ ë” ì¢‹ì•„ì§ˆ ê²ƒ ê°™ë‹¤. ì‹œê°„ì€ Audio Source ì†ì„±ì˜ playtimeì´ ì•„ë‹Œ TimeStep ê°œë…ìœ¼ë¡œ ì ‘ê·¼í•´ì•¼ ë°ì´í„° ìƒ˜í”Œë§ì´ ë”ìš± ìˆ˜ì›”í•˜ë‹¤. ê³¼ì—° ì–´ë””ì— ì“°ì´ëŠ” ë°ì´í„°ì¼ê¹Œ? Anomaly Event Detection, Game Play Pattern generation(Audio Dynamics â€¦)ì—ë„ ì“°ì´ëŠ” ë°ì´í„°ì´ë‹¤. ì´ë²¤íŠ¸ ê°ì§€ì— í™œìš©?!. ë°ì´í„° í•™ìŠµí• ë•Œ ì ˆ.ëŒ€.ë¡œ ë°ì´í„° ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ ë„£ìœ¼ë©´ ì•ˆëœë‹¤. ë¶„ëª… ë©”ëª¨ë¦¬ OOMì´ ë‚  ê²ƒì´ë‹¤. ì €ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ ë‚˜ì˜¨ ê°œë…ì¸ Data Windowingë¥¼ ì¨ì•¼ í•˜ëŠ” ì´ìœ ì´ë‹¤. (Batch sizeë³´ë‹¤ ìš°ì„ .)","link":"/2023/03/16/%EA%B2%8C%EC%9E%84%EA%B0%9C%EB%B0%9C-%EC%9D%BC%EA%B8%B02/"},{"title":"ë”¥ëŸ¬ë‹-ì…ë¬¸ê³¼-ì¤€ë¹„3","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°-3ì°¨ì›ìˆ˜ ëŠ˜ë¦¬ê¸°, ì¤„ì´ê¸°(TF2.x)1234567x = tf.expand_dims(x, 1)x.shape # (x.shape, 1)x[..., tf.newaxis].shape # (x.shape, 1)np.squeeze(x[0]).shape # x.shape ì°¨ì› ì¤„ì´ê¸° TF2.x LayersConvolution filters : layerì—ì„œ ì¶œë ¥ë ë•Œ ëª‡ê°œì˜ filter kernel_size : filter(weight) ì˜ ì‚¬ì´ì¦ˆ strides : ëª‡ ê°œì˜ pixelë§Œí¼ skipí•˜ë©´ì„œ sliding window í•  ê²ƒì¸ì§€ padding : same, zero activation : í™œì„±í™” í•¨ìˆ˜(Linear functionì€ ì¸µì„ ìŒ“ëŠ” ì˜ë¯¸ê°€ ì—†ë‹¤) to be Continuedâ€¦","link":"/2020/11/23/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%843/"},{"title":"ë…¼ë¬¸ ë¦¬ë·°","text":"ë”¥ëŸ¬ë‹ ë…¼ë¬¸ ë¦¬ë·° RNNì˜ ê³ ì§ˆì ì¸ ë¬¸ì œì â€¦ Sequenceê°€ ê¸¸ë©´ ië²ˆì§¸ outputì„ ë§Œë“¤ê¸° ìœ„í•´ ê·¸ ì´ì „ì˜ i-1ë²ˆì§¸ hidden stateë¥¼ ì‚¬ìš©í•œë‹¤. Long Term Dependency problemâ€¦ ì´ê±¸ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” Recurrent Layer ëŒ€ì‹  Attention Mechanismì„ ì“°ë©´ Sequence ê¸¸ì´ì— ìƒê´€ì—†ì´ input / outputì˜ Dependencyë¥¼ ë³´ë‹¤ ì •í™•íˆ ê°ì§€â€¦ RNNì„ ì™„ì „íˆ ì œê±°í•´ì•¼ í•¨. íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì¨ì•¼ í•˜ëŠ” ì´ìœ ? í˜„ì¬ Video Understanding ëª¨ë¸ì—ì„œë„ ì‹œë„ë˜ê³  ìˆìŒ(TimesFormer; ê¸°ì¡´ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ìª¼ê°  CNNëª¨ë¸ì— ë¹„í•´ì„œ ë©”ëª¨ë¦¬ ì ˆê°ê³¼ Inference ì†ë„ í–¥ìƒ) Timesformer ì€ íš¨ê³¼ì ì¸ í”„ë ˆì„ë³„ ë¸íƒ€ê°’ë§Œ ê°ì§€ : ë¹„ë””ì˜¤ë¥¼ Patchë‹¨ìœ„ë¡œ ë¶„ì„í•¨ Positional Encodingì„ ì‹œê°„ì¶•ì— í™•ì¥í•œ ëª¨ë¸, ì‹œê°„ì¶•ê³¼ ê³µê°„ì¶• ì „ì²´ë¥¼ ì–´í…ì…˜ í•˜ë©´ Costê°€ ë„ˆë¬´ í¬ê¸° ë•Œë¬¸ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ë¡œ ê²°íŒì´ ë‚˜ëŠ” ê²Œì„ íŒë…ì—ë„ ì •ë§ ìš©ì´í• ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë¨. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ íŠ¹ì§• RNN ê³„ì—´ì€ ìˆœì„œëŒ€ë¡œë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•´ì„œ í•™ìŠµ ì†ë„ê°€ ëŠë¦¼. í•˜ì§€ë§Œ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥â€¦How? Encoder-Decoder ëª¨ë¸ì„ í†µí•´ì„œ ë³‘ë ¬ ì²˜ë¦¬ ENcoderì—ì„œëŠ” ê°ê° positionì— ëŒ€í•´ Attentionë§Œ í•˜ê³ , Decoderì—ì„œëŠ” Masking ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥ EncoderëŠ” input Sequenceë¥¼ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ì¹˜í™˜ Decoderì—ì„œëŠ” ENcoderìœ¼ë¡œë¶€í„° Output Sequenceë¥¼ í•˜ë‚˜ì”© ìƒì„± ê°ê° stepì—ì„œ ë‹¤ìŒ symbolì„ ë§Œë“¤ ë•Œ ì´ì „ì— ë§Œë“¤ì–´ì§„ outputì„ ì“´ë‹¤.(ìê¸° íšŒê·€ì ì¸ íŠ¹ì„±) ex : â€œì—¬ê¸°ëŠ” ì–´ë”” ë‚˜ëŠ” ëˆ„êµ¬â€ ë¼ëŠ” ë¬¸ì¥ì—ì„œ â€œì—¬ê¸°ëŠ” ì–´ë””â€ ë¼ëŠ” symbolìœ¼ë¡œ â€œë‚˜ëŠ” ëˆ„êµ¬â€ ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Transformer ì „ì²´ êµ¬ì¡° Encoder Input Embeddingì€ Time Embedding, ìì—°ì–´ì— ì“°ì´ëŠ” Word Embedding ë“± ì—¬ëŸ¬ ì¢…ë¥˜ê°€ ìˆìŒ ê·¸ ì¤‘ì—ì„œ Time Embeddingì„ ì†Œê°œ.$$\\mathbf{t} 2 \\mathbf{v}(\\tau)[i]=\\left{\\begin{array}{ll}\\omega_{i} \\tau+\\varphi_{i}, &amp; \\text { if } i=0 \\\\mathcal{F}\\left(\\omega_{i} \\tau+\\varphi_{i}\\right), &amp; \\text { if } 1 \\leq i \\leq k\\end{array}\\right.$$ ax+bì²˜ëŸ¼ ìƒê¸´ ì € ì‹ì— í•¨ìˆ˜ë¥¼ ë„£ì–´ì„œ ì‹œê°„ë³„ ì •ë³´ë¥¼ ì‹¤ì–´ì•¼ í•¨.iëŠ” Timestepâ€¦ ì‹œí€€ìŠ¤ì˜ ì‹œì‘ì ì€ ê·¸ëƒ¥ ax+bë§Œì„ ì“´ë‹¤.ì „ì²´ ì‹œí€€ìŠ¤ ë°ì´í„°ì— ì ìš©í•˜ë ¤ë©´ ì£¼ê¸°ì„±ì„ ê°–ëŠ” í•¨ìˆ˜(íŒŒì¥, ì£¼ê¸°, ì£¼íŒŒìˆ˜ ë“±)ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ë°reluë¥¼ ì‚¬ìš©í•˜ë©´ ì£¼ê¸°ì„± ì •ë³´ê°€ ì—†ìœ¼ë‹ˆâ€¦ë‹¹ì—°íˆ ì•ˆë ë“¯..(!) y = wx + b concat sin(wx+b) ê° íƒ€ì„ìŠ¤í…ë³„ë¡œ ì£¼ê¸°ì„± ì •ë³´ë¥¼ ì£¼ì… 12345678910111213141516171819202122232425262728293031323334# tf.Keras.Layerclass Time2Vector(Layer): def __init__(self, seq_len, **kwargs): super(Time2Vector, self).__init__() self.seq_len = seq_len def build(self, input_shape): '''shape (batch, seq_len) í˜•íƒœë¡œ ê°€ì¤‘ì¹˜ì™€ Bias ì´ˆê¸°í™” ''' self.weights_linear = self.add_weight(name='weight_linear',shape=(int(self.seq_len),), initializer='uniform', trainable=True) self.bias_linear = self.add_weight(name='bias_linear', shape=(int(self.seq_len),), initializer='uniform', trainable=True) self.weights_periodic = self.add_weight(name='weight_periodic',shape=(int(self.seq_len),), initializer='uniform', trainable=True) self.bias_periodic = self.add_weight(name='bias_periodic',shape=(int(self.seq_len),), initializer='uniform', trainable=True) def call(self, x): '''ì£¼ê¸°ì„±, ì„ í˜• ì‹œê°„ë³„ íŠ¹ì§•ì„ ê³„ì‚°''' x = tf.math.reduce_mean(x[:,:,:], axis=-1) # ì…ë ¥ Feature ì°¨ì› ìŠ¬ë¼ì´ì‹± time_linear = self.weights_linear * x + self.bias_linear # ì„ í˜• ì‹œê°„ íŠ¹ì§• time_linear = tf.expand_dims(time_linear, axis=-1) # ì°¨ì› ì¶”ê°€ (batch, seq_len, 1) time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic) time_periodic = tf.expand_dims(time_periodic, axis=-1) # ì°¨ì› ì¶”ê°€ (batch, seq_len, 1) return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2) positional Encoding$$\\begin{array}{l}P E_{(\\text {pos } 2 i)}=\\sin \\left(\\frac{\\text { pos }}{10000^{\\frac{2 i}{d_{\\text {model }}}}}\\right) \\P E_{(\\text {pos 2i+1) }}=\\cos \\left(\\frac{\\text { pos }}{10000^{\\frac{2 i}{d_{\\text {model }}}}} \\right)\\end{array}$$ positional Encodingì´ ê³„ì‚°ë˜ëŠ” ê³¼ì • : Time Embedded Input + positional vector -&gt; Time Embedded vector positional Encodingì„ í•´ì•¼ í•˜ëŠ” ì´ìœ  :ì´í›„ Attention Layerì˜ Q, K, Vì— ë³´ë‹¤ ëª…í™•í•œ Sequenceì— ëŒ€í•œ ì •ë³´ë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´(sequence vectorê°„ì˜ ê±°ë¦¬ í™•ì‹¤í™”) ex &gt; embedding size 512ì¼ë•Œ : (-1~1ì˜ ê°’ìœ¼ë¡œ ì´ë£¨ì–´ì§„) sin í•¨ìˆ˜ë¡œ ì²˜ë¦¬ëœ 256ì‚¬ì´ì¦ˆ ë²¡í„° + cosë¡œ ì²˜ë¦¬ëœ 256ì‚¬ì´ì¦ˆ ë²¡í„° ìƒì„± AttentionLuong Attention, Badanau Attention, self-Attention ë“± Taskì— ë”°ë¼ ë‹¤ë¦„.ê·¸ ì¤‘ì—ì„œ Self-Attention â€¦ @ : matmul (ë²¡í„° ë‚´ì ê³±)T : ì „ì¹˜í–‰ë ¬(Transpose)^1/2 : ë£¨íŠ¸ Attention(Q, K, V) = softmax( Q @ K.T / d_k^1/2) @ VVë¥¼ ê³±í•˜ëŠ” ì´ìœ  : softmaxëœ ìŠ¤ì½”ì–´ì— valueë¥¼ ê³±í•´ì„œê´€ë ¨ì´ ì—†ëŠ” ì‹œí€€ìŠ¤ì—ë‹¤ 1e-4 ê°™ì€ ì‘ì€ ìŠ¤ì½”ì–´ë¥¼ ê³±í•´ ì—†ì•¤ë‹¤. ì´ë ‡ê²Œ Attention scoreê°€ ê³„ì‚°ëœë‹¤. Multi-Head Attentionn_head * Concat( softmax( Q @ K.T / d_k^1/2) @ V ) @ W0feed forward Neural Network( tf.keras.layers.Dense )ì— ì…ë ¥ì„ ì£¼ê¸° ìœ„í•´í•˜ë‚˜ì˜ í–‰ë ¬ë¡œ headë§ˆë‹¤ ê³„ì‚°ëœ softmaxëœ ìŠ¤ì½”ì–´ë¥¼ ì „ë¶€ í•©ì¹˜ê³  W0ë¥¼ ê³±í•œë‹¤. 12345 |---Residual connect----| |---Residual connect-------|Attention ----&gt; LayerNormalize --&gt; FeedForwardNeuralNetwork -&gt; LayerNorm | | |&lt;------------- N repeat --------------------------------------------&gt;| |----------------------Encoder #1-------------------------------------| ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì˜ ì¥ì  : ëª¨ë¸ì´ ì„œë¡œ ë‹¤ë¥¸ ìœ„ì¹˜ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ í‘œí˜„ ë¶€ë¶„ ê³µê°„ ì •ë³´ì— ê³µë™ìœ¼ë¡œ ê´€ì—¬ ì»¤ë²„ë¡œìŠ¤, íˆë“œë¼, ìì¿°ì€ ë¨¸ë¦¬ê°€ ì—¬ëŸ¬ê°œë¼ ìœ„ì¹˜ê°€ ë‹¤ì–‘í•œ ì—¬ëŸ¬ í”Œë ˆì´ì–´ë¥¼ ì¸ì‹í•˜ê³  ë™ì‹œë‹¤ë°œë¡œ ë°ë¯¸ì§€ë¥¼ ê°€í•¨. Decoder ë””ì½”ë” êµ¬ì„± Outputì— ëŒ€í•´ Right Shift í•˜ê³  Embedding, Positional Encoding ì´ ì´ë£¨ì–´ì§. Right shifted ëœ ì…ë ¥ì„ ë°›ëŠ” ì´ìœ  : ë””ì½”ë”ëŠ” ì´ì „ ì‹œí€€ìŠ¤ì— ëŒ€í•œ í† í°ê³¼ ì–´í…ì…˜ ìŠ¤ì½”ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ì‹œí€€ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë™ëœë‹¤. ì‹œí€€ìŠ¤ì˜ ì²˜ìŒ í† í° ì´ì „ì— ì‹œì‘ì„ì„ ì•Œë¦¬ê¸° ìœ„í•œ íŠ¹ì • í† í°ì´ ì‚½ì…ë˜ì–´ ì—†ëŠ” ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡ ì—ëŸ¬ì„ ë°©ì§€í•´ ì¤€ë‹¤ Masked Attentionì€ ë‹¤ìŒ ì‹œí€€ìŠ¤ì™€ì˜ ìœ ì‚¬ì„±ì„ ë¬´ì‹œí•˜ê¸° ìœ„í•´ Maskingì„ ì ìš© ì´í›„ëŠ” Encoderì™€ ê°™ì€ êµ¬ì¡°ì˜ ë ˆì´ì–´ë¡œ ìŒ“ì—¬ ìˆìŒ. ë””ì½”ë” ì—ì„œëŠ” ì¸ì½”ë”ì™€ëŠ” ë‹¤ë¥´ê²Œ Masked Multi-Head Attention layerê°€ ì¶”ê°€ë˜ì—ˆë‹¤. Masked Attentionì˜ ì›ë¦¬: ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì˜ˆë¡œ ë“¤ìë©´ ì˜¤ëŠ˜ 5000ì› ë°›ì•˜ëŠ”ë° ì´ê²ƒì €ê²ƒ ìƒ€ë‹¤. ê·¸ë˜ì„œ ë‚´ì¼ì€ ì˜¤ëŠ˜ë³´ë‹¤ ì”ì•¡ì´ ê°ì†Œí•  ê²ƒì´ë‹¤. Masked Attentionì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì€.. ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ í˜„ì¬ì˜ ê°’ë§Œ ì•Œë ¤ì£¼ê³ (ë¯¸ë˜ ì‹œí€€ìŠ¤ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ë©´ ì•ˆë˜ì„œâ€¦) ë¯¸ë˜ì˜ ê°’ì„ ê°€ë ¤ë²„ë¦¬ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ.. 1234567891011ì˜¤ëŠ˜ì˜ ì”ê³  ë‚´ì¼ ì”ê³  ëª¨ë ˆ ì”ê³  ê¸€í”¼ ì”ê³  5000ì› -inf -inf -inf5000ì› 3000ì› -inf -inf5000ì› 3000ì› 2000ì› -inf5000ì› 3000ì› 2000ì› 1000ì› |---Residual connect----| |---Residual connect-------|MaskedAttn-LayerNorm--&gt; Attention ----&gt; LayerNormalize --&gt; FeedForwardNeuralNetwork -&gt; LayerNorm | | |&lt;------------------------------- N repeat --------------------------------------------&gt;| |&lt;---------------------------------------Decoder #1-------------------------------------| Final Output Layer ë¶„ë¥˜ê¸°(softmax), íšŒê·€ì˜ˆì¸¡(Linear) ë ˆì´ì–´ë¥¼ ë¶™ì—¬ì„œ íƒœìŠ¤í¬ì— ë§ê²Œ ë°”ê¿”ë¼ë©´ ëœë‹¤.","link":"/2021/07/15/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0/"},{"title":"ë”¥ëŸ¬ë‹-ì…ë¬¸ê³¼-ì¤€ë¹„5","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°-5ì„ í˜•ëŒ€ìˆ˜ ë°°ì›Œë³´ê¸°(í–‰ë ¬ì„ ì•„ë¬´ë¦¬ ê³±í•˜ê³  ë”í•´ë„ ì„ ëª¨ì–‘)Scala : í¬ê¸°ë§Œ ì¡´ì¬í•˜ëŠ” ì–‘Vector : ì†ë„, ìœ„ì¹˜ì´ë™, í˜, ê³µê°„ë’¤í‹€ë¦¼ê³¼ ê°™ì´ í¬ê¸°ì™€ ë°©í–¥ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ì–‘ Norm ? nì°¨ì› ë²¡í„° $$\\vec{x} = (x_1, x_2, \\cdots x_n)$$Norm $$\\lVert x \\rVert = \\sqrt{x_1^1 + x_2^2 + \\cdots + x_n^2}$$ â€œì›ì  Oì—ì„œ ì \\(x_1, x_2, \\cdots, x_n\\) ê¹Œì§€ì˜ ê±°ë¦¬â€ ë‚´ì  ? Inner product, Dot productí–‰ë ¬ë¼ë¦¬ ê³±í•  ë•ŒëŠ” ì°¨ì›ì„ ì£¼ì˜í•œë‹¤. A(m, n) * B(n, m) ë§Œ ê°€ëŠ¥ Transpose: ì „ì¹˜í–‰ë ¬(í–‰ê³¼ ì—´ì„ ë’¤ë°”ê¿ˆ) A.T numpy ì—°ì‚°(Element-wise operation) np.dot(x, y) (aka ë‚´ì , dot-product)ì™€ x * y(element-wise)ëŠ” ì„œë¡œ ë‹¤ë¦„. numpy ë¹„êµ, ë…¼ë¦¬ì—°ì‚°(element-wise operation)numpy Reductions argmax() : ìµœëŒ€ê°’ìˆëŠ” ì¸ë±ìŠ¤ë¥¼ ë¦¬í„´, argmin() : ìµœì†Œê°’ì˜ ì¸ë±ìŠ¤ ë¦¬í„´ np.all, np.any? ALL : Arrayë‚´ ëª¨ë“  ê°’ì´ TRUEì¸ê°€? any : Arrayë‚´ ê°’ì´ í•˜ë‚˜ë¼ë„ TRUEì¸ê°€? np.mean, np.median, np.std ë“± í†µê³„í•¨ìˆ˜ ì‚¬ìš© ê°€ëŠ¥ë”¥ëŸ¬ë‹ì— ëŒ€í•œ í™˜ìƒ ë³µì¡í•œ ë¬¸ì œë„ ì¸µì„ ê¹Šê³  ë„“ê²Œ ìŒ“ìœ¼ë©´ í•´ê²°ëœë‹¤ â€“&gt; Gradient Vanhshing, Initialize fault ìœ¼í•˜í•˜í•°ã…‹ã…‹ã…‹ $$Sigmoid(z) = \\frac{1} {1 + e^{-z}}$$ Sigmoid ë„í•¨ìˆ˜ì˜ ìµœëŒ€ê°’ì€ 1/4 â€¦ â€“&gt; ê·¸ë˜ì„œ Gradient Vanishing ë‚˜ëŠ”ê±°ì„ ã…‡ã…‡ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì´ˆê¸°í™”ì˜ ì¤‘ìš”ì„± $ y = Wx+b $ ì—ì„œ wê°€ 100, bê°€ 50ì´ë¼ë©´ xê°€ 0.01ì´ë”ë¼ë„ yëŠ” 51ì´ ë¨ ì—­ì „íŒŒë•Œ sigmoid í•¨ìˆ˜ í†µê³¼ì‹œí‚¤ë©´ $ Sigmoidâ€™(51) $ ë¦¬í„´ë¨ í•˜ì§€ë§Œ yê°€ 5ë§Œ ë„˜ì–´ë„ $ Sigmoid (y) $ ëŠ” 0ì— ìˆ˜ë ´ â€“&gt; ì´ê²ƒì´ ë°”ë¡œ Gradient Vanishingâ€¦ ê·¸ë˜ì„œ ì…ë ¥ì¸µì˜ ê°€ì¤‘ì¹˜wë¥¼ ëª¨ë‘ 0ìœ¼ë¡œ ë¦¬ì…‹! Forward Propagationë•Œ ë‘ë²ˆì§¸ ì¸µ ë‰´ëŸ°ì— ëª¨ë‘ ê°™ì€ ê°’ì´ ì „ë‹¬ë¨ Backward Propagationë•Œ ë‘ì§¸ ì¸µ ê°€ì¤‘ì¹˜ê°€ ëª¨ë‘ ë˜‘ê°™ì´ ì—…ë°ì´íŠ¸ ==&gt; ì‹ ê²½ë§ í‘œí˜„ë ¥ ì œí•œ BiasëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ëŠ”ê²Œ ì¼ë°˜ì ìœ¼ë¡œ íš¨ìœ¨ì  ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” 2 í‘œì¤€ ì •ê·œë¶„í¬ë¥¼ ì´ìš©í•œ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” Sigmoidí•¨ìˆ˜ì˜ ì¶œë ¥ê°’ì´ ê·¹ë‹¨ì ìœ¼ë¡œ(0 or 1)ì— ì¹˜ìš°ì¹˜ëŠ” í˜„ìƒ â€“&gt; Gradient Vanishing í‘œì¤€í¸ì°¨ë¥¼ 0.01ë¡œ í•˜ëŠ” ì •ê·œë¶„í¬ë¡œ ì´ˆê¸°í™” ê°€ì¤‘ì¹˜ê°€ ëª¨ì—¬ ìˆìŒ =&gt; ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ ì–´ëŠì •ë„ ì™„í™”ë¨ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” 3 Xavierì´ˆê¸°í™” ë°©ë²•(2010) 1w = np.random.randn(n_input, n_output) / (n_input) ** 0.5 Sigmoidì™€ ê°™ì€ Sì í•¨ìˆ˜ì˜ ê²½ìš° ì¶œë ¥ê°’ë“¤ì´ ì •ê·œë¶„í¬ í˜•íƒœì´ì–´ì•¼ ì•ˆì •ì  í•™ìŠµ ê°€ëŠ¥ Sigmoid functionê³¼ Xavier Initë°©ë²•ì„ ì‚¬ìš©í–ˆì„ ê²½ìš° ê·¸ë˜í”„ ReLU ê³„ì—´ í•¨ìˆ˜ì—ëŠ” ì ì ˆí•˜ì§€ ì•ŠìŒlayerë¥¼ ê±°ì³ê°ˆ ìˆ˜ë¡ 0ì— ìˆ˜ë ´(converge) ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” 4 He ì´ˆê¸°í™” ë°©ë²•(2015) 1w = np.random.randn(n_input, n_output) / (n_input / 2) ** 0.5 RELU + He init â€“&gt; 10 layerë¥¼ ê±°ì³ë„ í‘œì¤€í¸ì°¨ê°€ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ Summary ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ëŠ” ë„ˆë¬´ë‚˜ ì¤‘ìš”í•¨ tanhì˜ ê²½ìš° Xavier Init ë°©ë²•ì´ íš¨ìœ¨ì  ReLUê³„ì—´ í•¨ìˆ˜ì—ëŠ” He Init ë°©ë²•ì´ íš¨ìœ¨ì  ìµœê·¼ì—” ëŒ€ë¶€ë¶„ He Initë¥¼ ì£¼ë¡œ ì‚¬ìš©","link":"/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/"},{"title":"ë”¥ëŸ¬ë‹ ì…ë¬¸ê³¼ ì¤€ë¹„2","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°-2Broadcastë‘ê°œì˜ í–‰ë ¬ shapeê°€ ì„œë¡œ ë‹¬ë¼ë„í•œìª½ì˜ ì°¨ì›ì´ ê°™ê±°ë‚˜, ì—°ì‚°í•˜ëŠ” ê°’ì´ í•œ ê°œì¼ë•Œshapeì— ë§ê²Œ ë³µì‚¬í•´ì„œ ì—°ì‚°í•¨ 123456789101112131415161718192021arr = np.arange(6).reshape(-1, 3)# [[0, 1, 2], # [3, 4, 5]]arr + 3# [[3, 4, 5],# [6, 7, 8]]arr * 3# [[0, 3, 6],# [9, 12 15]arr + np.array([1, 2, 3])# [[1, 3, 5],# [4, 6, 8]]np.add(arr, 1)# ëª¨ë“  ì›ì†Œì— 1ì„ ë”í•¨np.multiply(arr, 3)# ëª¨ë“  ì›ì†Œì— 3ì„ ê³±í•¨ argmax, argmin ë°°ì—´ì˜ í° ê°’ì´ë‚˜ ì‘ì€ ê°’ì˜ index return 1234arr = np.array([1, 4, 6, 54, 3, 2])np.argmax(arr) # 54np.argmin(arr) # 1np.unique(arr) # ìœ ì¼í•œ ê°’ ì¶œë ¥","link":"/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%842/"},{"title":"ë”¥ëŸ¬ë‹ ì…ë¬¸ê³¼ ì¤€ë¹„","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°Tensor ì´í•´í•˜ê¸° ì°¨ì› 0ì°¨ì›(ìƒìˆ˜) : Scalarê°’ 1ì°¨ì›(ë¦¬ìŠ¤íŠ¸ ì”Œìš´ ìƒìˆ˜), 2ì°¨ì›(2d), 3ì°¨ì›(3d), 4ì°¨ì›(4-d), nì°¨ì›(n-d) : Tensor Numpyë¡œ Tensor í‘œí˜„ê³¼ ì‘ìš©ì´ ê°€ëŠ¥ 123456import numpy as nparr = np.array([[3, 6, 9], [2, 4, 8]])print(arr.dtype) # dtype('float64')print(arr.shape) # (2, 3)print(arr.size) # 2 * 3 = 6 ì°¨ì› ëŠ˜ë¦¬ê¸°ì™€ ì¤„ì´ê¸° reshape, -1 í™œìš© 123arr.reshape(-1) # 1ì°¨ì›ìœ¼ë¡œ í¼ì¹˜ê¸°arr.reshape(-1, 3) # ì²«ë²ˆì§¸ ì°¨ì›ì€ ì•Œì•„ì„œ, ë‘ë²ˆì§¸ ì°¨ì›ì€ shape 3 Ravel() : arrì˜ ì°¨ì›ì„ 1ë¡œ ë°”ê¿ˆ(==&gt; Flatten) 123arr = np.array([[1, 2, 3], [4, 5, 6]]) # (2, 3)arr.ravel()arr.shape #(6, ) np.expand_dims() : ê°’ì„ ìœ ì§€í•˜ê³  ì°¨ì›ë§Œ ëŠ˜ë¦´ë•Œ 12arr = np.expand_dims(arr, -1) #(6, 1)arr.shape numpy arrayë¥¼ ë¹ ë¥´ê²Œ ì±„ìš°ëŠ” ë°©ë²•! 1234567891011121314# 0ìœ¼ë¡œ ì±„ìš°ê¸°arr2 = np.zeros([3, 4]) # 3 * 4ì˜ 0ì´ ì±„ì›Œì§„ ë°°ì—´one2 = np.ones([3, 4]) # 3 * 4ì˜ 1ë¡œ ì±„ì›Œì§„ ë°°ì—´five2 = np.ones([3, 4]) * 5 # 1ë¡œ ì±„ìš´ ê°’ì— 5ë¥¼ ë‹¤ ê³±í•¨arr2 = np.arange(n, m) # n ~ m-1ê¹Œì§€ì˜ ìˆ˜ë¡œ ë°°ì—´ ì±„ìš°ê¸°# array([n ~ m-1])arr = np.arange(5, 11).reshape(2, -1) # 5 ~ 10 : 6ê°œì˜ ìˆ«ì, (2, 3)arr # array([5, 6, 7] # [8, 9, 10]) ëª¨ì–‘ì´ ë§ì§€ ì•Šìœ¼ë©´ Errorâ€¦ 5, 6, 7, 8, 9ëŠ” 5ê°œì˜ ìˆ«ì 5 * 1 ë§Œ ê°€ëŠ¥í•œ. Index &amp; slicing 123456789101112131415# ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ &amp; ìŠ¬ë¼ì´ì‹±nums = [2, 3, 4, 5, 6]nums[:-1] # ë§ˆì§€ë§‰ ìˆ«ì ì „ê¹Œì§€ í‘œì‹œnums[::-1] # ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ ìˆ«ìë¥¼ ê±°ê¾¸ë¡œ í‘œí˜„nums = [[1, 2, 3], 4, 5, 6, 7]print(nums[0][1]) = 2 # ì²«ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ ì¸ë±ìŠ¤ê°€ 1ì¸ ìˆ«ìarr = np.array([5, 6, 7], [8, 9, 10])print(arr[1, 2]) # 10 --&gt; ì¸ë±ì‹± [í–‰, ì—´]print(arr[1:, 1:]) # [[9, 10]] Boolean Indexing1234data = np.random.randn(3, 3)print(data&lt;=0) # False, Trueë¡œ ë‚˜ì˜´data[data &lt;=0] = 1 # 0 ì´í•˜ì¸ ê²ƒì„ 1ë¡œ ì±„ìš°ë‹¤","link":"/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%84/"},{"title":"ë”¥ëŸ¬ë‹-ì…ë¬¸ê³¼-ì¤€ë¹„4","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°-4ì¸ê³µì‹ ê²½ë§ê³¼ ì†ì‹¤í•¨ìˆ˜ ì¸ê³µì‹ ê²½ë§ì˜ ê¸°ë³¸ êµ¬ì¡° ë‡Œì˜ í•™ìŠµë°©ë²•ì„ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§í•œ ê¸°ê³„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ê¸°ë³¸ êµ¬ì¡° : y = Wx+b $ x_i $ : ì…ë ¥, $ w_i $: ê°€ì¤‘ì¹˜, b : bias, f: í™œì„±í™”í•¨ìˆ˜ u : ê²°í•©(Net), z: ì¶œë ¥ ë‰´ëŸ°ì—ëŠ” ì„ í˜• ê²°í•©ê³¼ í™œì„±í™” í•¨ìˆ˜ ê¸°ëŠ¥ì´ ë“¤ì–´ìˆìŒ ì…ë ¥ì¸µ, ì€ë‹‰ì¸µ, ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±ë¨ ê° ë…¸ë“œì˜ ë‰´ëŸ° ì¶œë ¥ì€ ì§ì ‘ ì „ë‹¬ë˜ëŠ” ì •ë³´ì—ë§Œ ì˜ì¡´í•  ë¿ ë‹¤ë¥¸ ë…¸ë“œì™€ëŠ” ë¬´ê´€ ê·¸ë˜ì„œ? ë³‘ë ¬ì²˜ë¦¬ê°€ ê°€ëŠ¥í•¨. ì†ì‹¤ í•¨ìˆ˜(Loss or Cost function) ì‹ ê²½ë§ì˜ ì¶œë ¥ê°’ê³¼ ì‹¤ì œ ê²°ê³¼ê°’ì˜ ì°¨ì´ë¥¼ ì •ì˜í•˜ëŠ” í•¨ìˆ˜ ì‹ ê²½ë§ í•™ìŠµëª©í‘œëŠ” ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì›€ì§ì—¬ì•¼ í•¨ SGD, Adam ë“±ì˜ í•™ìŠµ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì†ì‹¤ í•¨ìˆ˜ íšŒê·€(Regression) ì œê³± ì˜¤ì°¨(MSE) ì‚¬ìš©, ìµœê·¼ì—ëŠ” rmse, maeì˜ ì¥ì ì´ ìˆëŠ” Huber Loss ì‚¬ìš©í•˜ëŠ” ì¶”ì„¸ Huber Loss?MAE + MSE -&gt; for Time Series Data!! ë¶„ë¥˜(Classification) í™œì„±í™” í•¨ìˆ˜ : softmax, ì†ì‹¤í•¨ìˆ˜ : cross-entropy ì•Œê³ ë¦¬ì¦˜ê³¼ ì—­ì „íŒŒ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ê²½ì‚¬ í•˜ê°•ë²•: ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•˜ì—¬ ì†ì‹¤í•¨ìˆ˜ S($ \\theta $) ê°’ì„ ìµœì í™” gradient(ê¸°ìš¸ê¸°)ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì¼ì • í¬ê¸°ë§Œí¼ ì´ë™í•˜ëŠ” ê²ƒì„ ë°˜ë³µí•˜ì—¬ ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ì„ ìµœì†Œí™”í•˜ëŠ” $ \\theta $ì˜ ê°’ì„ ì°¾ìŒ \\[\\theta = \\theta - \\eta \\nabla_\\theta S(\\theta)\\] ì´ ë–„ $ eta $ ëŠ” ë¯¸ë¦¬ ì •í•´ì§„ learning rate(step size) ì´ê³  ë³´í†µ 1e-3 ~ 1e-4 ì •ë„ë¥¼ ì‚¬ìš© ì—­ì „íŒŒ ê³„ì‚° ê·¸ë˜í”„ ë…¸ë“œëŠ” ì—°ì‚°ì„, ì—£ì§€ëŠ” ë°ì´í„°ì˜ íë¦„ë°©í–¥ sigmoid í•¨ìˆ˜ ì—­ì „íŒŒ í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ë²•(Chain Rule) í–‰ë ¬ì—°ì‚°ê³¼ ì—­ì „íŒŒ 1 ì´ì§„ë¶„ë¥˜ 2-layer NN ì—­ì „íŒŒ to be continuedâ€¦","link":"/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/"},{"title":"ë”¥ëŸ¬ë‹-ì…ë¬¸ê³¼-ì¤€ë¹„6","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°-6ğŸ’¥Remind!! ë”¥ëŸ¬ë‹ì— ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ  ì„ í˜• í•¨ìˆ˜ë¡œëŠ” XORê³¼ ê°™ì€ non-linearí•œ ë¬¸ì œëŠ” í•´ê²°ì´ ì•ˆë¨;; ê·¸ëŸ¬ë©´ Hidden Layerë¥¼ ëŠ˜ë¦¬ë©´ ë˜ì§€ ì•Šì„ê¹Œ? $f(ax+by) = af(x) + bf(y)$ ë¼ëŠ” íŠ¹ì§• ë•Œë¬¸ì— N-layer ê¹Šì´ë¥¼ ì•„ë¬´ë¦¬ ìŒ“ì•„ë„ 1-Layerë¡œ ë™ì‘í•¨. ìµœì í™”(Opt) ì•Œê³ ë¦¬ì¦˜ ê²½ì‚¬í•˜ê°•ë²•(GD)$$\\theta = \\theta - \\eta \\nabla_\\theta S(\\theta)$$ Networkì˜ parameter=$ \\theta $ ë¡œ í• ë•Œ ì†ì‹¤í•¨ìˆ˜ $ J(\\theta) $ì˜ ê°’ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ê¸°ìš¸ê¸°$ \\nabla J(\\theta)$ë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•GDì—ì„œëŠ” Gradientì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì¼ì • í¬ê¸°(lr)ë§Œí¼ ì´ë™í•˜ëŠ” ê²ƒì„ ë°˜ë³µí•˜ì—¬ loss functionì˜ ê°’ì„ ìµœì†Œí™” í•˜ëŠ” $ \\theta $ì˜ ê°’ì„ ì°¾ìŒ, lr(í•™ìŠµë¥ ) $ \\eta $ ëŠ” ë³´í†µ 1e-3 ~ 1e-4 ì‚¬ì´ì—ì„œ ì‚¬ìš©í•¨. ë„ˆë¬´ í¬ë©´ global minimumì„ ì§€ë‚˜ì¹˜ê³  ë„ˆë¬´ ì‘ìœ¼ë©´ Local Minimumì— ë¹ ì§. í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(SGD)ì „ì²´ Training setì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ batch Gradient Descent, ê³„ì‚°ëŸ‰ì´ ë§ì•„ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´mini-batchì— ëŒ€í•´ì„œë§Œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í™•ë¥ ì  GDë¥¼ ì‚¬ìš©í•¨.ê°™ì€ ì‹œê°„ì— ë” ë§ì€ stepë¥¼ ê°ˆ ìˆ˜ ìˆìŒ, ì—¬ëŸ¬ë²ˆ ë°˜ë³µí•  ê²½ìš° batchì˜ ê²°ê³¼ì™€ ë¹„ìŠ·í•¨ GD vs SGDGD : í™•ì‹¤í•œë° ë„ˆë¬´ ëŠë¦¼ | SGD : ì¡°ê¸ˆ í—¤ë©”ì§€ë§Œ ë¹ ë¦„ Momentum : í˜„ì¬ Gradientë¥¼ í†µí•´ ì´ë™í•˜ëŠ” ë°©í–¥ê³¼ ë³„ê°œë¡œ ê³¼ê±°ì˜ ì´ë™ë°©ì‹ì„ ê¸°ì–µí•˜ë©´ì„œ ì¼ì¢…ì˜ ê´€ì„±ì„ ì£¼ëŠ” ë°©ì‹ AdaGrad(Adaptive Gradient) ë§ì´ ë³€í™”í–ˆë˜ ë³€ìˆ˜ë“¤ì€ step sizeë¥¼ ì‘ê²Œ í•˜ëŠ” ê²ƒìì£¼ ë“±ì¥í•˜ê±°ë‚˜ ë³€í™”ë¥¼ ë§ì´ í•œ ë³€ìˆ˜ë“¤ì€ optimumì— ê°€ê¹Œì´ ìˆì„ í™•ë¥ ì´ ë†’ê¸° ë•Œë¬¸ì— ì‘ì€ í¬ê¸°ë¡œ ì´ë™í•˜ë©´ì„œ ë¯¸ì„¸ì¡°ì ˆ ì ê²Œ ë³€í™”í•œ ë³€ìˆ˜ë“¤ì€ ë§ì´ ì´ë™í•´ì•¼í•  í™•ë¥ ì´ ë†’ê¸° ë•Œë¬¸ì— ë¨¼ì € ë¹ ë¥´ê²Œ lossê°’ì„ ì¤„ì´ëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë™í•˜ëŠ” ë°©ì‹í•™ìŠµì„ ê³„ì† ì§„í–‰í•˜ë©´ step sizeê°€ ë„ˆë¬´ ì¤„ì–´ë“œëŠ” ë‹¨ì ì´ ìˆìŒ. RMSPropí•©ì„ ì§€ìˆ˜í‰ê· ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ Adagradì˜ ë‹¨ì ì„ í•´ê²°Gê°€ ë¬´í•œì • ì»¤ì§€ì§€ëŠ” ì•Šìœ¼ë©´ì„œ ìµœê·¼ ë³€í™”ëŸ‰ì˜ ë³€ìˆ˜ê°„ ìƒëŒ€ì ì¸ í¬ê¸° ì°¨ì´ëŠ” ìœ ì§€í•  ìˆ˜ ìˆìŒ. AdamMomentum + RMSProp ì§€ê¸ˆê¹Œì§€ ê³„ì‚°í•´ì˜¨ ê¸°ìš¸ê¸°ì˜ ì§€ìˆ˜í‰ê· ì„ ì €ì¥ rmspropê³¼ ìœ ì‚¬í•˜ê²Œ Gradientì˜ ì œê³±ê°’ì˜ ì§€ìˆ˜í‰ê· ì„ ì €ì¥ Overfitting(ê³¼ì í•©) Training Setì˜ ì§€ì—½ì ì¸ íŠ¹ì„±ê¹Œì§€ ë°˜ì˜í•´ Variance Highë¡œ Trainingë˜ì–´ì„œ Training Setì„ ì•”ê¸°í•´ë²„ë¦¬ëŠ” í˜„ìƒ Test Setì„ ì˜ ì˜ˆì¸¡í•˜ì§€ ëª»í•¨ ì£¼ë¡œ í‘œí˜„ë ¥ì´ ë†’ì€ ëª¨ë¸, ì¦‰ íŒŒë¼ë¯¸í„°ê°€ ë§ì€ ëª¨ë¸ì— ë°œìƒ ì •ê·œí™”(Regularization) ì†ì‹¤í•¨ìˆ˜ì— ê°€ì¤‘ì¹˜ì˜ í¬ê¸°ë¥¼ í¬í•¨ ê°€ì¤‘ì¹˜ê°€ ì‘ì•„ì§€ë„ë¡ í•™ìŠµí•œë‹¤ëŠ” ê²ƒì€ Outlier(Noise)ì˜ ì˜í–¥ì„ ì ê²Œ ë°›ìŒ L2 ì •ê·œí™” Rigde Regression L1 ì •ê·œí™”Sparse Modelì— ì•Œë§ìŒ.. ì‘ì€ ê°€ì¤‘ì¹˜ë“¤ì´ ê±°ì˜ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ì—¬ ëª‡ê°œì˜ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ë“¤ë§Œ ë‚¨ìŒ. Lasso Regression ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•œ ì ì´ ìˆê¸° ë•Œë¬¸ì— Gradient-Base Learningì—ëŠ” ì£¼ì˜.. DropOutê° ë ˆì´ì–´ì˜ ì¼ì • ë¹„ìœ¨ë¡œ ë‰´ëŸ°ì˜ ì¶œë ¥ ê°’ì„ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ë‚˜ë¨¸ì§€ ë‰´ëŸ°ë“¤ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ê³¼ì í•©ì„ íš¨ê³¼ì ìœ¼ë¡œ ì˜ˆë°© ê°€ëŠ¥(Network ë‚´ë¶€ì˜ Ensemble í•™ìŠµìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ) ì—­ì „íŒŒëŠ” ReLUì²˜ëŸ¼ ë™ì‘Forward Propagationë•Œ ì‹œê·¸ë„ì„ í†µê³¼ì‹œí‚¨ ë‰´ëŸ°ì€ Backwardë•Œë„ í†µê³¼ì‹œí‚´dropëœ ë‰´ëŸ°ì€ Backward Propagationë•Œë„ ì‹œê·¸ë„ ì°¨ë‹¨ ë°˜ë©´, TESTë•ŒëŠ” ëª¨ë“  ë‰´ëŸ°ì— ì‹ í˜¸ë¥¼ ì „ë‹¬í•¨ Batch Normalizationí•™ìŠµí•˜ëŠ” ì´ì „ ì¸µì˜ íŒŒë¼ë¯¸í„° ë³€í™”ë¡œ í˜„ì¬ì¸µì˜ ì…ë ¥ ë¶„í¬ê°€ ë°”ë€ŒëŠ” í˜„ìƒì„ ë‚´ë¶€ ê³µë¶„ì‚° ë³€í™”(Internal Covariate Shift)ì´ì „ ì¸µì˜ ì‘ì€ íŒŒë¼ë¯¸í„° ë³€í™”ê°€ ì¦í­ë˜ì–´ ë’· ë ˆì´ì–´ì— í° ì˜í–¥ì„ ë°›ìŒ.ê·¸ë˜ì„œâ€¦ BN(2015) Gradient Vanishing, Explodingì„ ë°©ì§€í•˜ëŠ” ëŒ€í‘œì ì¸ ë°©ë²• ì§ì ‘ì ì¸ ë°©ë²•ì„. Training ê³¼ì • ìì²´ë¥¼ ì•ˆì •í™”ì‹œì¼œ í•™ìŠµì†ë„ë¥¼ ê°€ì†í™” í‰ê· ê³¼ ë¶„ì‚°ì„ ì¡°ì ˆí•˜ëŠ” ê³¼ì •ì´ NN ì•ˆì— í¬í•¨ ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒì´ í•µì‹¬ì  Trainingí• ë•Œê° Mini Batchë§ˆë‹¤ $ \\gamma $ ì™€ $ \\beta $ë¥¼ êµ¬í•˜ê³  ì €ì¥í•´ ë‘  Testí• ë•Œêµ¬í–ˆë˜ $ \\gamma $ ì™€ $ \\beta $ ì˜ í‰ê· ì„ ì‚¬ìš© Data Augmentationì¼ì¢…ì˜ Regularizationì‘ì—…, ë°ì´í„°ê°€ ì ì„ ë•Œ ì‚¬ìš©í•˜ë©´ ë§¤ìš° íš¨ê³¼ì ì¦‰ ë°ì´í„° ë³€í˜•","link":"/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/"},{"title":"ë”¥ëŸ¬ë‹-ì…ë¬¸í•˜ê¸°-9","text":"ì§€ë‚œë²ˆì— ì´ì–´ì„œ ì˜¤ë””ì˜¤ ë”¥ëŸ¬ë‹ 2ë²ˆì§¸2. Sound Representationìœ„ì—ì„œ Samplingëœ discreteí•œ ë°ì´í„°ë¥¼ ì´ì œ ìš°ë¦¬ëŠ” í‘œí˜„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ ìš”ì†Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì €í¬ê°€ ë°ì´í„°ë¥¼ í‘œí˜„í•´ì•¼í• ê¹Œìš”?, ì²«ë²ˆì§¸ëŠ” ì‹œê°„ì˜ íë¦„ì— ë”°ë¼, ê³µê¸°ì˜ íŒŒë™ì˜ í¬ê¸°ë¡œ ë³´ëŠ” Time-domain Representation ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. ë‘ë²ˆì§¸ëŠ” ì‹œê°„ì— ë”°ë¼ì„œ frequencyì˜ ë³€í™”ë¥¼ ë³´ëŠ” Time-Frequency representationì´ ìˆìŠµë‹ˆë‹¤. 2.1. Time domain - WaveformWaveformì˜ ê²½ìš°ì—ëŠ” ì˜¤ë””ì˜¤ì˜ ìì—°ì ì´ í‘œí˜„ì…ë‹ˆë‹¤. ì‹œê°„ì´ xì¶•ìœ¼ë¡œ ê·¸ë¦¬ê³  amplitudeê°€ yì¶•ìœ¼ë¡œ í‘œí˜„ì´ ë©ë‹ˆë‹¤. 1234import librosa.displayfig = plt.figure(figsize = (14,5))librosa.display.waveplot(y[0:10000], sr=sr) &lt;matplotlib.collections.PolyCollection at 0x7fa325708d50&gt; ì •í˜„íŒŒ (Sinusoid)ëª¨ë“  ì‹ í˜¸ëŠ” ì£¼íŒŒìˆ˜(frequency)ì™€ í¬ê¸°(magnitude), ìœ„ìƒ(phase)ì´ ë‹¤ë¥¸ ì •í˜„íŒŒ(sinusolida signal)ì˜ ì¡°í•©ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. í“¨ë¦¬ì— ë³€í™˜ì€ ì¡°í•©ëœ ì •í˜„íŒŒì˜ í•©(í•˜ëª¨ë‹ˆ) ì‹ í˜¸ì—ì„œ ê·¸ ì‹ í˜¸ë¥¼ êµ¬ì„±í•˜ëŠ” ì •í˜„íŒŒë“¤ì„ ê°ê° ë¶„ë¦¬í•´ë‚´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. 1234567sr = 16000 # sample rateT = 2.0 # secondst = np.linspace(0, T, int(T*sr), endpoint=False) # time variablex = 0.5*np.sin(2*np.pi*440*t) # pure sine wave at 440 Hz# y = 0.5*numpy.sin(2*numpy.pi*400*t)ipd.Audio(x, rate=sr) # load a NumPy array 1librosa.display.waveplot(x[:50], sr=sr) &lt;matplotlib.collections.PolyCollection at 0x7fa327a01550&gt; í‘¸ë¦¬ì— ë³€í™˜ (Fourier transform)í‘¸ë¦¬ì— ë³€í™˜(Fourier transform)ì„ ì§ê´€ì ìœ¼ë¡œ ì„¤ëª…í•˜ë©´ í‘¸ë¦¬ì— ë³€í™˜ì€ ì„ì˜ì˜ ì…ë ¥ ì‹ í˜¸ë¥¼ ë‹¤ì–‘í•œ ì£¼íŒŒìˆ˜ë¥¼ ê°–ëŠ” ì£¼ê¸°í•¨ìˆ˜(ë³µìˆ˜ ì§€ìˆ˜í•¨ìˆ˜)ë“¤ì˜ í•©ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ í‘œí˜„í•˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ê° ì£¼ê¸°í•¨ìˆ˜ë“¤ì˜ ì§„í­ì„ êµ¬í•˜ëŠ” ê³¼ì •ì„ í“¨ë¦¬ì— ë³€í™˜ì´ë¼ê³  í•©ë‹ˆë‹¤. ì£¼ê¸°(period): íŒŒë™ì´ í•œë²ˆ ì§„ë™í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„, ë˜ëŠ” ê·¸ ê¸¸ì´, ì¼ë°˜ì ìœ¼ë¡œ siní•¨ìˆ˜ì˜ ì£¼ê¸°ëŠ” \\(2\\pi /w\\)ì…ë‹ˆë‹¤ ì£¼íŒŒìˆ˜(frequency): 1ì´ˆë™ì•ˆì˜ ì§„ë™íšŸìˆ˜ì…ë‹ˆë‹¤. í“¨ë¦¬ì— ë³€í™˜ì˜ ì‹ì„ ì‚´í´ë´…ì‹œë‹¤. $$y(t)=\\sum_{k=-\\infty}^\\infty A_k , \\exp \\left( i\\cdot 2\\pi\\frac{k}{T} t \\right)$$ ì´ ì‹ì„ í•˜ë‚˜ì‹ í•´ì„í•´ë´…ì‹œë‹¤.\\(k\\)ëŠ” \\(-\\infty\\) ~ \\(\\infty\\)ì˜ ë²”ìœ„ë¥¼ ê°€ì§€ê³  ì›€ì§ì…ë‹ˆë‹¤.ì´ê²ƒì€ ì£¼ê¸°í•¨ìˆ˜ë“¤ì˜ ê°¯ìˆ˜ì…ë‹ˆë‹¤. ì–´ë– í•œ ì‹ í˜¸ê°€ ë‹¤ë¥¸ ì£¼ê¸°í•¨ìˆ˜ë“¤ì˜ í•©ìœ¼ë¡œ í‘œí˜„ë˜ëŠ”ë°, ê·¸ ì£¼ê¸°í•¨ìˆ˜ëŠ” ë¬´í•œëŒ€ì˜ ë²”ìœ„ì— ìˆêµ°ìš”. ê·¸ë ‡ë‹¤ë©´ \\(A_k\\)ì€ ê·¸ ì‚¬ì¸í•¨ìˆ˜ì˜ ì§„í­ì´ë¼ê³  í•©ë‹ˆë‹¤. ì´ ì‹ì€ ì‹œê°„ì— ëŒ€í•œ ì…ë ¥ì‹ í˜¸ \\(y_{t}\\)ê°€ \\(\\exp \\left( i\\cdot 2\\pi\\frac{k}{T} t \\right)\\) ì™€ ì§„í­(\\(A_k\\))ì˜ ì„ í˜•ê²°í•©ìœ¼ë¡œ í‘œí˜„ë¨ì„ ë§í•˜ê³  ìˆêµ°ìš”. ìœ„ ê·¸ë¦¼ì„ ë³¸ë‹¤ë©´ ì¡°ê¸ˆ ë” ëª…í™•íˆ ì•Œìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë¶‰ì€ìƒ‰ ë¼ì¸ì´ ì…ë ¥ì‹ í˜¸ \\(y_{t}\\) ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ê°€ ë‹¤ë£¨ê²Œ ë˜ëŠ” ë°ì´í„°ì¸ ìŒì•…ì´ë‚˜ ëª©ì†Œë¦¬ ê°™ì€ ë°ì´í„° ì—­ì‹œ complex toneì…ë‹ˆë‹¤. ì—¬ë ¤ê°œì˜ ì£¼íŒŒìˆ˜ì˜ì—­ì´ í•©ì³ì§„ ê²ƒì´ì£ . ì´ëŸ¬í•œ ì—¬ëŸ¬ê°œì˜ ì£¼íŒŒìˆ˜ ì˜ì—­ì„ ë¶„ë¦¬í•˜ì!ê°€ ì£¼ìš”í•œ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤. íŒŒë€ìƒ‰ ì£¼ê¸°í•¨ìˆ˜ë“¤ì„ ë³´ì‹ ë‹¤ë©´ ì—¬ëŸ¬ê°œì˜ ì£¼ê¸°í•¨ìˆ˜ë“¤ì„ ì°¾ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ ì£¼ê¸°í•¨ìˆ˜ë“¤ì€ ê³ ìœ ì˜ ì£¼íŒŒìˆ˜(frequency)ì™€ ê°•ë„(amplitude)ë¥¼ ê°€ì§€ê³  ìˆê³  ê·¸ê²ƒì´ íŒŒë€ìƒ‰ì˜ ë¼ì¸ë“¤ë¡œ í‘œí˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì§„í­ì— ëŒ€í•œ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.$$A_k = \\frac{1}{T} \\int_{-\\frac{T}{2}}^\\frac{T}{2} f(t) , \\exp \\left( -i\\cdot 2\\pi \\frac{k}{T} t \\right) , dt$$ì—¬ê¸°ì„œ í•˜ë‚˜ì˜ ì˜ë¬¸ì ì´ ë“œì‹¤ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì£¼ê¸°í•¨ìˆ˜ì˜ í•©ìœ¼ë¡œ í‘œí˜„ëœë‹¤ê³  í–ˆëŠ”ë° ì €í¬ê°€ ë³´ê³  ìˆëŠ”ê²ƒì€ \\(\\exp \\left( i\\cdot 2\\pi\\frac{k}{T} t \\right)\\) ì§€ìˆ˜í•¨ìˆ˜ì˜ í˜•íƒœì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì§€ìˆ˜í•¨ìˆ˜ì™€ ì£¼ê¸°í•¨ìˆ˜ ì‚¬ì´ì˜ ì—°ê´€ê´€ê³„ëŠ” ë¬´ì—‡ì¼ê¹Œìš”? ê·¸ ê´€ê³„ë¥¼ ì°¾ì€ ê²ƒì´ ë°”ë¡œ ì˜¤ì¼ëŸ¬ ê³µì‹ì…ë‹ˆë‹¤. $$e^{i\\theta} = \\cos{\\theta} + i\\sin{\\theta}$$ ì´ ì‹ì„ ìœ„ ì‹ì²˜ëŸ¼ í‘œí˜„í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤$$\\exp \\left( i\\cdot 2\\pi\\frac{k}{T} t \\right) = \\cos\\left({2\\pi\\frac{k}{T}}\\right) + i\\sin\\left({2\\pi\\frac{k}{T}}\\right)$$ ì—¬ê¸°ì„œ \\(\\cos{2\\pi\\frac{k}{T}}\\), \\(i\\sin{2\\pi\\frac{k}{T}}\\) í•¨ìˆ˜ëŠ” ì£¼ê¸°ì™€ ì£¼íŒŒìˆ˜ë¥¼ ê°€ì§€ëŠ” ì£¼ê¸°í•¨ìˆ˜ì…ë‹ˆë‹¤. ì¦‰ í“¨ë¦¬ì— ë³€í™˜ì€ ì…ë ¥ singalì´ ì–´ë–¤ê²ƒì¸ì§€ ìƒê´€ì—†ì´ sin, cosê³¼ ê°™ì€ ì£¼ê¸°í•¨ìˆ˜ë“¤ì˜ í•©ìœ¼ë¡œ í•­ìƒ ë¶„í•´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. Fourier Transformì˜ Orthogonal$$y(t)=\\sum_{k=-\\infty}^\\infty A_k , \\exp \\left( i\\cdot 2\\pi\\frac{k}{T} t \\right)$$ ì–´ë– í•œ ì£¼ê¸°í•¨ìˆ˜ë¥¼ ìš°ë¦¬ëŠ” cosê³¼ siní•¨ìˆ˜ë¡œ í‘œí˜„í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ í•œê°€ì§€ ì¬ë°ŒëŠ” ì ì€, ì´ í•¨ìˆ˜ë“¤ì´ ì§êµí•˜ëŠ” í•¨ìˆ˜(orthogonal)ë¼ëŠ” ì ì´ë‹¤.$${ \\exp \\left(i\\cdot 2\\pi\\frac{k}{T} t\\right) } = orthogonal$$ ë²¡í„°ì˜ ì§êµëŠ” í•´ë‹¹ ë²¡í„°ë¥¼ í†µí•´ í‰ë©´ì˜ ëª¨ë“  ì¢Œí‘œë¥¼ í‘œí˜„í• ìˆ˜ ìˆì—ˆë‹¤. í•¨ìˆ˜ì˜ ë‚´ì ì€ ì ë¶„ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ”ë°, ë§Œì•½ êµ¬ê°„ [a,b]ì—ì„œ ì§êµí•˜ëŠ” í•¨ìˆ˜ëŠ” êµ¬ê°„ [a,b]ì˜ ëª¨ë“  í•¨ìˆ˜ë¥¼ í‘œí˜„í• ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ ì¼€ì´ìŠ¤ì—ì„œëŠ” cos, sin í•¨ìˆ˜ê°€ ì‚¬ì‹¤ìƒ ìš°ë¦¬ ì…ë ¥ì‹ í˜¸ì— ëŒ€í•´ì„œ ê¸°ì €ê°€ ë˜ì–´ì£¼ëŠ” í•¨ìˆ˜ë¼ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. DFT (Discrete Fourier Transform)í•œê°€ì§€ ì˜ë¬¸ì ì´ ë“­ë‹ˆë‹¤. ë°”ë¡œ, ìš°ë¦¬ê°€ samplingìœ¼ë¡œ ë“¤ì–´ì˜¨ ë°ì´í„°ëŠ” ë°”ë¡œ ì‹œê°„ì˜ ê°„ê²©ì— ë”°ë¥¸ ì†Œë¦¬ì˜ amplitudeì˜ discreteí•œ ë°ì´í„°ì´ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ìœ„ í‘¸ë¦¬ì— ë³€í™˜ ì‹ì„ Discreteí•œ ì˜ì—­ìœ¼ë¡œ ìƒê°í•´ë´…ì‹œë‹¤. ë§Œì•½ì— ìš°ë¦¬ê°€ ìˆ˜ì§‘í•œ ë°ì´í„° \\(y_{n}\\)ì—ì„œ, ì´ì‚° ì‹œê³„ì—´ ë°ì´í„°ê°€ ì£¼ê¸° \\(N\\)ìœ¼ë¡œ ë°˜ë³µí•œë‹¤ê³  í• ë•Œ, DFTëŠ” ì£¼íŒŒìˆ˜ì™€ ì§„í­ì´ ë‹¤ë¥¸ \\(N\\)ê°œì˜ ì‚¬ì¸ í•¨ìˆ˜ì˜ í•©ìœ¼ë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.$$y_n = \\frac{1}{N} \\sum_{k=0}^{N-1} Y_k \\cdot \\exp \\left( i\\cdot 2\\pi\\frac{k}{N} n \\right)$$ ìœ„ ì‹ì„ ë³´ë©´ kì˜ rangeê°€ 0ë¶€í„° \\(N-1\\)ë¡œ ë³€í™”í–ˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë•Œ Spectrum \\(Y_{k}\\)ë¥¼ ì›ë˜ì˜ ì‹œê³„ì—´ ë°ì´í„°ì— ëŒ€í•œ í“¨ë¦¬ì— ë³€í™˜ê°’ì´ë¼ê³  í•˜ì£ . $$Y_k = \\sum_{n=0}^{N-1} y_n\\cdot \\exp \\left( -i\\cdot 2\\pi\\frac{k}{N} n \\right)$$ \\(y_{n}\\) : input signal \\(n\\) : Discrete time index \\(k\\) : discrete frequency index \\(Y_{k}\\) : kë²ˆì§¸ frequenyì— ëŒ€í•œ Spectrumì˜ ê°’ 123456789def DFT(x): N = len(x) X = np.array([]) nv = np.arange(N) for k in range(N): s = np.exp(1j*2*np.pi*k/N*nv) X = np.append(X, sum(x*np.conjugate(s))) return X STFT (Short-Time Fourier Transform)FFTëŠ” ì‹œê°„ì— íë¦„ì— ë”°ë¼ ì‹ í˜¸ì˜ ìˆ˜íŒŒìˆ˜ê°€ ë³€í–ˆì„ë•Œ, ì–´ëŠ ì‹œê°„ëŒ€ì— ì£¼íŒŒìˆ˜ê°€ ë³€í•˜ëŠ”ì§€ ëª¨ë¥´ê²Œ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œ, STFTëŠ” ì‹œê°„ì˜ ê¸¸ì´ë¥¼ ë‚˜ëˆ ì„œ ì´ì œ í“¨ë¦¬ì— ë³€í™˜ì„ í•˜ê²Œ ë©ë‹ˆë‹¤. ì¦‰ FFTë¥¼ í–ˆì„ë•ŒëŠ” Time dominaì— ëŒ€í•œ ì •ë³´ê°€ ë‚ ì•„ê°€ê²Œ ë˜ëŠ” ê²ƒì´ì£ . ì£¼íŒŒìˆ˜ì˜ íŠ¹ì„±ì´ ì‹œê°„ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” ì‚¬ìš´ë“œë¥¼ ë¶„ì„í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” signal ë°ì´í„°ì— ì í•©í•˜ë‹¤. ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì¼ì •í•œ ì‹œê°„ êµ¬ê°„ (window size)ë¡œ ë‚˜ëˆ„ê³ , ê° êµ¬ê°„ì— ëŒ€í•´ì„œ ìŠ¤í™íŠ¸ëŸ¼ì„ êµ¬í•˜ëŠ” ë°ì´í„°ì´ë‹¤. ì´ëŠ” Time-frequency 2ì°¨ì› ë°ì´í„°ë¡œ í‘œí˜„ì´ ë©ë‹ˆë‹¤. $$X(l,k) = \\sum_{n=0}^{N-1} w(n) x(n+lH)\\exp^{\\frac{-2\\pi k n}{N}}$$ \\(N\\) : FFT size Windowë¥¼ ì–¼ë§ˆë‚˜ ë§ì€ ì£¼íŒŒìˆ˜ ë°´ë“œë¡œ ë‚˜ëˆ„ëŠ”ê°€ ì…ë‹ˆë‹¤. Duration ìƒ˜í”Œë§ ë ˆì´íŠ¸ë¥¼ windowë¡œ ë‚˜ëˆˆ ê°’ì…ë‹ˆë‹¤. $$T= window/SR$$ T(Window) = 5T(Signal), durationì€ ì‹ í˜¸ì£¼ê¸°ë³´ë‹¤ 5ë°° ì´ìƒ ê¸¸ê²Œ ì¡ì•„ì•¼í•œë‹¤. 440Hz ì‹ í˜¸ì˜ window sizeëŠ” 5*(1/440)ì´ ë©ë‹ˆë‹¤. \\(w(n)\\) : Window function ì¼ë°˜ì ìœ¼ë¡œ Hann windowê°€ ì“°ì…ë‹ˆë‹¤. \\(n\\) : Window size Window í•¨ìˆ˜ì— ë“¤ì–´ê°€ëŠ” Sampleì˜ ì–‘ì…ë‹ˆë‹¤. ì‘ì„ìˆ˜ë¡ Low-frequency resolutionì„ ê°€ì§€ê²Œ ë˜ê³ , high-time resolutionì„ ê°€ì§‘ë‹ˆë‹¤. ê¸¸ìˆ˜ë¡ High-frequency, low time resolutionì„ ê°€ì§‘ë‹ˆë‹¤. \\(H\\) : Hop size ìœˆë„ìš°ê°€ ê²¹ì¹˜ëŠ” ì‚¬ì´ì¦ˆì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” 1/4ì •ë„ë¥¼ ê²¹ì¹˜ê²Œ í•©ë‹ˆë‹¤. STFTì˜ ê²°ê³¼ëŠ” ì¦‰ ì‹œê°„ì˜ íë¦„(Window)ì— ë”°ë¥¸ Frequencyì˜ì—­ë³„ Amplitudeë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. 1234567sr = 16000 # sample rateT = 2.0 # secondst = np.linspace(0, T, int(T * sr), endpoint=False) # time variablex = 0.5 * np.sin(2 * np.pi * 440 * t) # pure sine wave at 440 Hz# y = 0.5*numpy.sin(2*numpy.pi*400*t)ipd.Audio(x, rate=sr) # load a NumPy array 12345678910111213141516print(len(y))D = librosa.stft(y)print(D.shape, D)# phase ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‚ ë¦°ë‹¤.D_mag = np.abs(D)print(D_mag)print(D_mag.shape)magnitude, phase = librosa.magphase(D)print(magnitude)print(magnitude.shape)print(magnitude-D_mag) 48944 (1025, 96) [[-2.1494275e-01+0.0000000e+00j -2.0992082e-01+0.0000000e+00j -2.0418610e-01+0.0000000e+00j ... -1.9438802e-01+0.0000000e+00j -1.9518623e-01+0.0000000e+00j -2.3163199e-01+0.0000000e+00j] [ 9.3493842e-02+6.7762636e-21j 1.2481287e-01+4.8880498e-03j 7.3961377e-02+1.3274251e-03j ... 7.7925511e-02-1.7781712e-02j 9.6285135e-02+1.7115690e-02j 1.1564651e-01-5.2810002e-02j] [ 1.9829417e-02+8.2818238e-19j -3.1706840e-02+1.5587136e-02j 5.7078212e-02-2.0519590e-02j ... 2.3265863e-02+7.6752454e-02j 3.0044108e-03-6.0352467e-02j 5.4616658e-03+6.8522707e-02j] ... [-5.3125373e-03-1.2618632e-18j 3.4157380e-03-1.7295172e-03j -1.8859134e-03-3.5993013e-04j ... -7.6227036e-04-9.3025468e-05j -1.8814437e-04-8.4138475e-05j 4.7763987e-04-5.3400453e-04j] [ 2.1248308e-03+1.5585406e-19j -1.4035926e-03+8.0862024e-05j 2.4144542e-03+3.4830419e-04j ... -2.3595782e-04+1.1687888e-03j 1.1331354e-04+1.2911476e-04j 2.8909228e-04+3.3650018e-04j] [-8.1756472e-04+0.0000000e+00j -9.3529455e-04+0.0000000e+00j -1.4104146e-03+0.0000000e+00j ... 1.3452002e-03+0.0000000e+00j -9.2299597e-06+0.0000000e+00j -4.9439305e-04+0.0000000e+00j]] [[2.1494275e-01 2.0992082e-01 2.0418610e-01 ... 1.9438802e-01 1.9518623e-01 2.3163199e-01] [9.3493842e-02 1.2490856e-01 7.3973291e-02 ... 7.9928555e-02 9.7794548e-02 1.2713383e-01] [1.9829417e-02 3.5331041e-02 6.0654562e-02 ... 8.0201246e-02 6.0427204e-02 6.8740025e-02] ... [5.3125373e-03 3.8286415e-03 1.9199529e-03 ... 7.6792570e-04 2.0610093e-04 7.1645004e-04] [2.1248308e-03 1.4059199e-03 2.4394479e-03 ... 1.1923688e-03 1.7178644e-04 4.4362905e-04] [8.1756472e-04 9.3529455e-04 1.4104146e-03 ... 1.3452002e-03 9.2299597e-06 4.9439305e-04]] (1025, 96) [[2.1494275e-01 2.0992082e-01 2.0418610e-01 ... 1.9438802e-01 1.9518623e-01 2.3163199e-01] [9.3493842e-02 1.2490856e-01 7.3973291e-02 ... 7.9928555e-02 9.7794548e-02 1.2713383e-01] [1.9829417e-02 3.5331041e-02 6.0654562e-02 ... 8.0201246e-02 6.0427204e-02 6.8740025e-02] ... [5.3125373e-03 3.8286415e-03 1.9199529e-03 ... 7.6792570e-04 2.0610093e-04 7.1645004e-04] [2.1248308e-03 1.4059199e-03 2.4394479e-03 ... 1.1923688e-03 1.7178644e-04 4.4362905e-04] [8.1756472e-04 9.3529455e-04 1.4104146e-03 ... 1.3452002e-03 9.2299597e-06 4.9439305e-04]] (1025, 96) [[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]] 123456S = librosa.core.stft(audio_np, n_fft=1024, hop_length=512, win_length=1024)D = np.abs(S)**2log_S = librosa.power_to_db(S, ref=np.max) #ì†Œë¦¬ì˜ ë‹¨ìœ„ë¥¼ dbë¡œ ë°”ê¿ˆ plt.figure(figsize=(12,4))librosa.display.specshow(log_S, sr=16000, x_axis='time') Window function?ìœ„ì—ì„œ Window functionê³¼ Window sizeë¼ëŠ” ì´ì•¼ê¸°ê°€ ë‚˜ì˜¤ê³  ìˆìŠµë‹ˆë‹¤. ìœˆë„ìš° Functionê³¼ SizeëŠ” ì™œ ì“°ëŠ” ê²ƒì´ë©° ì–´ë–¨ë•Œ ì“°ëŠ” ê²ƒì¼ê¹Œìš”? Window functionì˜ ì£¼ëœ ê¸°ëŠ¥ì€ main-lobeì˜ widthì™€ side-lobeì˜ ë ˆë²¨ì˜ Trade-off ë¥¼ ì œì–´í•´ ì¤€ë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ê¹ìŠ¤ í˜„ìƒì„ ë§‰ì•„ì£¼ëŠ” ê³ ë§ˆìš´ ì¹œêµ¬ì´ê¸°ë„ í•˜ì£ . ì§€ê¸ˆë‚˜ì˜¨ main-lobe, side-bloe, ê¹ìŠ¤í˜„ìƒì€ ë¬´ì—‡ì¼ê¹Œìš”? 123456789101112def frame_audio(audio, FFT_size=1024, hop_size=20, sample_rate = 22050): audio = np.pad(audio, int(FFT_size/2), mode='reflect') frame_len = np.round(sample_rate*hop_size / 1000).astype(int) frame_num = int((len(audio) - FFT_size) / frame_len) + 1 frames = np.zeros((frame_num, FFT_size)) for n in range(frame_num): frames[n] = audio[n*frame_len:n*frame_len+FFT_size] return framesaudio_framed = frame_audio(audio_np)print(&quot;Framed audio shape: {}&quot;.format(audio_framed.shape)) Framed audio shape: (469, 1024) 123456789101112131415161718from scipy import signalwindow = signal.get_window(&quot;hann&quot;, 1024, fftbins=True)audio_win = audio_framed * windowind = 2plt.figure(figsize=(15,6))plt.subplot(3,1,1)plt.plot(window)plt.grid(True)plt.subplot(3,1,2)plt.plot(audio_framed[ind])plt.grid(True)plt.subplot(3,1,3)plt.plot(audio_win[ind])plt.grid(True)plt.show() í”Œë¡¯ì„ ë³´ê²Œ ëœë‹¤ë©´ windowingì„ ì ìš©í•˜ê¸°ì „ plotì€ ëë¶€ë¶„ì´ ë‹¤ ë‹¤ë¥´ì§€ë§Œ, windowingì„ ì§€ë‚˜ê³  ë‚˜ì„œ ë‚˜ì˜¤ëŠ” plotì€ ëì´ 0 ìœ¼ë¡œ ì¼ì¹˜í•œë‹¤ëŠ” íŠ¹ì„±ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Window size?ìœˆë„ìš° ì‚¬ì´ì¦ˆëŠ” ì¼ë°˜ì ìœ¼ë¡œ timeê³¼ frequencyì˜ resolutionsì„ ì œì–´í•´ ì¤ë‹ˆë‹¤. short-window : ë‚®ì€ frequency resolution, ë†’ì€ time-resolutionì„ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤. Long-window : ë†’ì€ frequency resolutionì„ ê°€ì§€ë©°, ë‚®ì€ time-resolutionì„ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤. SpectrogramSpectrogramì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ ê³ ë¯¼í•´ë´…ì‹œë‹¤.ì¼ë°˜ì ìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ëŠ” ì…ë ¥ì‹ í˜¸ì— ëŒ€í•´ì„œ window functionì„ í†µê³¼í•˜ì—¬ window sizeë§Œí¼ sampling ëœ dataë¥¼ ë°›ì•„ì„œ Discrete Fourier Transformì„ ê±°ì¹˜ê²Œ ë©ë‹ˆë‹¤. DFTë¥¼ ê±°ì¹œ ì‹ í˜¸ë“¤ì€ Frequencyì™€ Amplitudeì˜ ì˜ì—­ì„ ê°€ì§€ëŠ” Spectrumì´ ë©ë‹ˆë‹¤. ì´í›„ ì´ë¥¼ 90ë„ë¡œ íšŒì „ì‹œì¼œì„œ, time domainìœ¼ë¡œ stackí•˜ê²Œ ë©ë‹ˆë‹¤. Spectrogramì€ Frequency Scaleì— ëŒ€í•´ì„œ Scalingì´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ì£¼íŒŒìˆ˜ ì˜ì—­ì— Scalingì„ í•˜ëŠ” ì´ìœ ëŠ”, ì¸ê°„ì˜ ì£¼íŒŒìˆ˜ë¥¼ ì¸ì‹í•˜ëŠ” ë°©ì‹ê³¼ ì—°ê´€ì´ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ëŒì€, ì¸ì ‘í•œ ì£¼íŒŒìˆ˜ë¥¼ í¬ê²Œ êµ¬ë³„í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ìš°ë¦¬ì˜ ì¸ì§€ê¸°ê´€ì´ categoricalí•œ êµ¬ë¶„ì„ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë•Œë¬¸ì— ìš°ë¦¬ëŠ” ì£¼íŒŒìˆ˜ë“¤ì˜ Binì˜ ê·¸ë£¹ì„ ë§Œë“¤ê³  ì´ë“¤ì„ í•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ì£¼íŒŒìˆ˜ ì˜ì—­ì—ì„œ ì–¼ë§ˆë§Œí¼ì˜ ì—ë„ˆì§€ê°€ ìˆëŠ”ì§€ë¥¼ ì°¾ì•„ë³¼ ê²ƒì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” ì¸ê°„ì´ ì ì€ ì£¼íŒŒìˆ˜ì— ë” í’ë¶€í•œ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ê¸°ë•Œë¬¸ì—, ì£¼íŒŒìˆ˜ê°€ ì˜¬ë¼ê°ˆìˆ˜ë¡ í•„í„°ì˜ í­ì´ ë†’ì•„ì§€ë©´ì„œ ê³ ì£¼íŒŒëŠ” ê±°ì˜ ê³ ë ¤ë¥¼ ì•ˆí•˜ê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ë˜ frequency scaleì€ ì–´ë–¤ ë°©ì‹ì„ í†µí•´ ì €ì£¼íŒŒìˆ˜ëŒ€ ì˜ì—­ì„ ê³ ë ¤í•  ê²ƒì´ê°€ì— ëŒ€í•œ ê³ ë¯¼ì´ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤. Linear frequency scaleì¼ë°˜ì ìœ¼ë¡œ single tone(ìˆœìŒ)ë“¤ì˜ ë°°ìŒ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ê¸° ì¢‹ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë¶„í¬ê°€ ì €ì£¼íŒŒìˆ˜ ì˜ì—­ì— ê¸°ìš¸ì–´ì ¸(skewed) ìˆìŠµë‹ˆë‹¤. Mel Scaleë©œ ìŠ¤í™íŠ¸ëŸ¼ì€ ì£¼íŒŒìˆ˜ ë‹¨ìœ„ë¥¼ ë‹¤ìŒ ê³µì‹ì— ë”°ë¼ ë©œ ë‹¨ìœ„ë¡œ ë°”ê¾¼ ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. $$m = 2595 \\log_{10}\\left(1 + \\frac{f}{700}\\right)$$ì¼ë°˜ì ìœ¼ë¡œëŠ” mel-scaled binì„ FFT sizeë³´ë‹¤ ì¡°ê¸ˆë” ì‘ê²Œ ë§Œë“œëŠ”ê²Œ ì¼ë°˜ì ì…ë‹ˆë‹¤. 1234# STFTS = librosa.core.stft(audio_np, n_fft=1024, hop_length=512, win_length=1024)# phase ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‚ ë¦°ë‹¤.D = np.abs(S)**2 1234# mel spectrogram (512 --&gt; 40)mel_basis = librosa.filters.mel(sr, 1024, n_mels=40)mel_S = np.dot(mel_basis, D)mel_S.shape (40, 404) 123456789import librosa.displayS = librosa.feature.melspectrogram(audio_np, sr=sr, n_mels = 128)log_S = librosa.power_to_db(S, ref=np.max)plt.figure(figsize=(12,4))librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')plt.title('Mel power sepctrogram')plt.colorbar(format='%+02.0f dB')plt.tight_layout() 123456789import librosa.displayS = librosa.feature.melspectrogram(audio_np, sr=sr, n_mels = 256)log_S = librosa.power_to_db(S, ref=np.max)plt.figure(figsize=(12,4))librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')plt.title('Mel power sepctrogram')plt.colorbar(format='%+02.0f dB')plt.tight_layout() Bark scaleê·€ê°€ ì¸ì‹í•˜ëŠ” ì£¼íŒŒìˆ˜ì˜ ì˜ì—­ì€ ëŒ€ëµ 20Hz~2000Hz ë¡œ ê°€ì •í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì£¼íŒŒìˆ˜ì— ëŒ€í•œ ì‚¬ëŒì˜ ì¸ì‹ì€ ë¹„ì„ í˜•ì ì…ë‹ˆë‹¤. ê·€ì™€ ë‡Œì˜ ê°€ì²­ëŒ€ì—­ì„ 24ê°œì˜ ëŒ€ì—­ìœ¼ë¡œ ë‚˜ëˆˆê²ƒì„ Barkë¼ê³  í•©ë‹ˆë‹¤! Bark scaleì€ 500Hzì´í•˜ì—ì„œëŠ” 100Hzì˜ ëŒ€ì—­í­ì„ ê°€ì§€ë©°, 500Hz ì´ìƒì—ì„œëŠ” ê° ëŒ€ì—­ì˜ ì¤‘ì‹¬ìˆ˜íŒŒìˆ˜ì˜ ëŒ€ëµ 20%ì— í•´ë‹¹í•˜ëŠ” ëŒ€ì—­í­ì„ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤. 20, 100, 200, 300, 400, 510, 630, 770, 920, 1080, 1270, 1480, 1720, 2000, 2320, 2700, 3150, 3700, 4400, 5300, 6400, 7700, 9500, 12000, 15500 ( Hz ) Log compression\\(10 * log10(\\frac{S}{ref})\\)ì˜ ë‹¨ìœ„ë¡œ ì‹ í˜¸ë¥¼ ìŠ¤ì¼€ì¼ë§ í•©ë‹ˆë‹¤. ì´ëŠ” spectrogramì„ ë°ì‹œë²¨ ìœ ë‹›ìœ¼ë¡œ ì „í™˜í•´ ì¤ë‹ˆë‹¤. 123#log compressionlog_mel_S = librosa.power_to_db(mel_S)log_mel_S.shape (40, 404) Discrete cosine transform (DCT)DCTëŠ” nê°œì˜ ë°ì´í„°ë¥¼ nê°œì˜ ì½”ì‚¬ì¸ í•¨ìˆ˜ì˜ í•©ìœ¼ë¡œ í‘œí˜„í•˜ì—¬ ë°ì´í„°ì˜ ì–‘ì„ ì¤„ì´ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì € ì£¼íŒŒìˆ˜ì— ì—ë„ˆì§€ê°€ ì§‘ì¤‘ë˜ê³  ê³  ì£¼íŒŒìˆ˜ ì˜ì—­ì— ì—ë„ˆì§€ê°€ ê°ì†Œí•©ë‹ˆë‹¤. Filter BankëŠ” ëª¨ë‘ Overlapping ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— Filter Bank ì—ë„ˆì§€ë“¤ ì‚¬ì´ì— ìƒê´€ê´€ê³„ê°€ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤. DCTëŠ” ì—ë„ˆì§€ë“¤ ì‚¬ì´ì— ì´ëŸ¬í•œ ìƒê´€ê´€ê³„ë¥¼ ë¶„ë¦¬ í•´ì£¼ëŠ” ì—­í™œì„ í•´ì¤ë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œ 26ê°œ DCT Coefficient ë“¤ ì¤‘ 12ë§Œ ë‚¨ê²¨ì•¼ í•˜ëŠ”ë°, ê·¸ ì´ìœ ëŠ” DCT Coefficient ê°€ ë§ìœ¼ë©´, Filter Bank ì—ë„ˆì§€ì˜ ë¹ ë¥¸ ë³€í™”ë¥¼ ë‚˜íƒ€ë‚´ê²Œ ë˜ê³ , ì´ê²ƒì€ ìŒì„±ì¸ì‹ì˜ ì„±ëŠ¥ì„ ë‚®ì¶”ê²Œ ë©ë‹ˆë‹¤. 12345# mfcc (DCT)mfcc = librosa.feature.mfcc(S=log_mel_S, n_mfcc=13)mfcc = mfcc.astype(np.float32) # to save the memory (64 to 32 bits)plt.figure(figsize=(12,4))librosa.display.specshow(mfcc) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3a8975cc88&gt; 1234567891011mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=13)delta2_mfcc = librosa.feature.delta(mfcc, order=2)print(delta2_mfcc.shape)plt.figure(figsize=(12,4))librosa.display.specshow(delta2_mfcc)plt.ylabel('MFCC coeffs')plt.xlabel('Time')plt.title('MFCC')plt.colorbar()plt.tight_layout() (13, 148) 12345678def change_pitch(data, sr): y_pitch = data.copy() bins_per_octave = 12 pitch_pm = 2 pitch_change = pitch_pm * 2 * (np.random.uniform()) y_pitch = librosa.effects.pitch_shift(y_pitch.astype('float64'), sr, n_steps=pitch_change, bins_per_octave=bins_per_octave) return y_pitch 123456def waveform_aug(waveform,sr): y = change_pitch(waveform, sr) fig = plt.figure(figsize = (14,5)) librosa.display.waveplot(y, sr=sr) ipd.display(ipd.Audio(data=y, rate=sr)) return y, sr 12ipd.display(ipd.Audio(data=audio_np, rate=sr))y, sr = waveform_aug(audio_np, 16000) 1234567S = librosa.feature.melspectrogram(audio_np, sr=sr, n_mels = 128)log_S = librosa.power_to_db(S, ref=np.max)plt.figure(figsize=(12,4))librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')plt.title('Mel power sepctrogram')plt.colorbar(format='%+02.0f dB')plt.tight_layout() 1np.random.uniform(low=1.5, high=3) 2.6892527992385458 12345def value_aug(data): y_aug = data.copy() dyn_change = np.random.uniform(low=1.5, high=3) y_aug = y_aug * dyn_change return y_aug 1234def add_noise(data): noise = np.random.randn(len(data)) data_noise = data + 0.005 * noise return data_noise 123def hpss(data): y_harmonic, y_percussive = librosa.effects.hpss(data.astype('float64')) return y_harmonic, y_percussive 12def shift(data): return np.roll(data, 1600) 123456789def stretch(data, rate=1): input_length = len(data) streching = librosa.effects.time_stretch(data, rate) if len(streching) &gt; input_length: streching = streching[:input_length] else: streching = np.pad(streching, (0, max(0, input_length - len(streching))), &quot;constant&quot;) return streching 12345678910def change_pitch_and_speed(data): y_pitch_speed = data.copy() # you can change low and high here length_change = np.random.uniform(low=0.8, high=1) speed_fac = 1.0 / length_change tmp = np.interp(np.arange(0, len(y_pitch_speed), speed_fac), np.arange(0, len(y_pitch_speed)), y_pitch_speed) minlen = min(y_pitch_speed.shape[0], tmp.shape[0]) y_pitch_speed *= 0 y_pitch_speed[0:minlen] = tmp[0:minlen] return y_pitch_speed 1234567data_noise = add_noise(audio_np)data_roll = shift(audio_np)data_stretch = stretch(audio_np)pitch_speed = change_pitch_and_speed(audio_np)value = value_aug(audio_np)y_harmonic, y_percussive = hpss(audio_np)y_shift = shift(audio_np) 1ipd.Audio(data_noise, rate=fs) 12345678import matplotlib.pyplot as pltlibrosa.display.specshow(librosa.amplitude_to_db(magnitude, ref=np.max), y_axis='log', x_axis='time')plt.title('Power spectrogram')plt.colorbar(format='%+2.0f dB')plt.tight_layout()plt.show() 1234567891011mel_s = librosa.feature.melspectrogram(y=y, sr=sr)print(mel_s.shape)import matplotlib.pyplot as pltlibrosa.display.specshow(librosa.amplitude_to_db(mel_s, ref=np.max), y_axis='log', x_axis='time')plt.title('Power spectrogram')plt.colorbar(format='%+2.0f dB')plt.tight_layout()plt.show() (128, 96) 1234567891011mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)print(mfccs.shape)import matplotlib.pyplot as pltplt.figure(figsize=(10, 4))librosa.display.specshow(mfccs, x_axis='time')plt.colorbar()plt.title('MFCC')plt.tight_layout()plt.show() (40, 96) 12345678import matplotlib.pyplot as pltlibrosa.display.specshow(librosa.amplitude_to_db(mfccs, ref=np.max), y_axis='log', x_axis='time')plt.title('Power spectrogram')plt.colorbar(format='%+2.0f dB')plt.tight_layout()plt.show() ë“¤ë¦¬ëŠ” ì†Œë¦¬ = ë°°ê²½ì¡ìŒ + ëª©ì†Œë¦¬12filename_wav = &quot;./wav/voice.wav&quot;filename_noise = &quot;./wav/cafe_noise.wav&quot; 12import IPython.display as ipdipd.Audio(filename_wav) 12import IPython.display as ipdipd.Audio(filename_noise) 123456789101112data_wav, sr_wav = librosa.load(filename_wav, mono=True, sr=16000)data_noise, sr_noise = librosa.load(filename_noise, mono=True, sr=16000)print(data_wav.shape)print(data_noise.shape)# ì „ì²´ì ìœ¼ë¡œ ë“¤ë¦¬ëŠ” ì†Œë¦¬ëŠ” ì†Œë¦¬ì˜ í•©( ë°°ê²½ìŒ + ëª©ì†Œë¦¬ )data_wav_noise = data_wav[:] + data_noise[:len(data_wav)]pos=10print(&quot;wav: {:.8f}, noise {:.8f}, wav+noise: {:.8f}&quot;.format(data_wav[pos], data_noise[pos], data_wav_noise[pos])) (48944,) (1044712,) wav: -0.00009155, noise -0.00003052, wav + noise: -0.00012207 1ipd.Audio(data_wav_noise, rate=16000)","link":"/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-9/"},{"title":"ë”¥ëŸ¬ë‹-ì…ë¬¸í•˜ê¸°-8","text":"ì˜¤ë””ì˜¤ ë”¥ëŸ¬ë‹ í•´ë³´ê¸°Reference Digital Signal Processing Lecture https://github.com/spatialaudio/digital-signal-processing-lecture Python for Signal Processing (unipingco) https://github.com/unpingco/Python-for-Signal-Processing Audio for Deep Learning (ë‚¨ê¸°í˜„ë‹˜) https://tykimos.github.io/2019/07/04/ISS_2nd_Deep_Learning_Conference_All_Together/ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì‘ì—…ì„ ìœ„í•œ ì—°ìŠµ (ë°•ìˆ˜ì² ë‹˜) https://github.com/scpark20/audio-preprocessing-practice Musical Applications of Machine Learning https://mac.kaist.ac.kr/~juhan/gct634/ Awesome audio study materials for Korean (ìµœê·¼ìš°ë‹˜) https://github.com/keunwoochoi/awesome-audio-study-materials-for-korean T Academy(ì¶œì²˜) https://tacademy.skplanet.com/live/player/onlineLectureDetail.action?seq=178 1. Digital Signal Processingì†Œë¦¬ signalë¥¼ ì–´ë– í•œ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ í‘œí˜„í•˜ë©°, ì†Œë¦¬ì™€ ê´€ë ¨ëœ taskë¥¼ í•´ê²°í•˜ëŠ”ë° ìˆìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì†Œë¦¬ëŠ” ì–´ë– í•œ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆì„ê¹Œìš”? Sound?ì†Œë¦¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì§„ë™ìœ¼ë¡œ ì¸í•œ ê³µê¸°ì˜ ì••ì¶•ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì••ì¶•ì´ ì–¼ë§ˆë‚˜ ë¬ëŠëƒì— ë”°ë¼ì„œ í‘œí˜„ë˜ê²ƒì´ ë°”ë¡œ Wave(íŒŒë™)ì¸ë°ìš”. íŒŒë™ì€ ì§„ë™í•˜ë©° ê³µê°„/ë§¤ì§ˆì„ ì „íŒŒí•´ ë‚˜ê°€ëŠ” í˜„ìƒì…ë‹ˆë‹¤. ì§ˆëŸ‰ì˜ ì´ë™ì€ ì—†ì§€ë§Œ ì—ë„ˆì§€/ìš´ë™ëŸ‰ì˜ ìš´ë°˜ì€ ì¡´ì¬í•©ë‹ˆë‹¤. Waveì—ì„œ ì €í¬ê°€ ì–»ì„ìˆ˜ ìˆëŠ” ì •ë³´ëŠ” í¬ê²Œ 3ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. Phase(Degress of displacement) : ìœ„ìƒ Amplitude(Intensity) : ì§„í­ Frequency : ì£¼íŒŒìˆ˜ Samplingìƒ˜í”Œë§ì€ ë¬´ì—‡ì¼ê¹Œìš”?? ì•„ë‚ ë¡œê·¸ ì •ë³´ë¥¼ ì˜ê²Œ ìª¼ê°œì„œ discreteí•œ ë””ì§€í„¸ ì •ë³´ë¡œ í‘œí˜„í•´ì•¼í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¬´í•œí•˜ê²Œ ìª¼ê°œì„œ ì €ì¥í• ìˆ˜ ì—†ìœ¼ë‹ˆ, ì–´ë–¤ ê¸°ì¤€ì„ ê°€ì§€ê³  ì•„ë‚ ë¡œê·¸ ì •ë³´ë¥¼ ìª¼ê°œì„œ ëŒ€í‘œê°’ì„ ì·¨í•˜ê²Œ ë©ë‹ˆë‹¤. Convert into a sqeuence of binary values via Sampling and Quantization 1.1. Time domainì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì•„ë‚ ë¡œê·¸ ì‹œê·¸ë„ì„ ìª¼ê°œê²Œ ë˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Samplingì„ í†µí•˜ì—¬ ì»´í“¨í„°ëŠ” ì†Œë¦¬ sequenceë¥¼ binary valueë¡œ ë°›ì•„ë“œë¦¬ê²Œ ë©ë‹ˆë‹¤. Sampling rate : ì–¼ë§ˆë‚˜ ì˜ê²Œ ìª¼ê°¤ ê²ƒì¸ê°€?ì˜ê°œ ìª¼ê°¤ìˆ˜ë¡ ì›ë³¸ ë°ì´í„°ì™€ ê±°ì´ ê°€ê¹Œì›Œì§€ê¸° ë–„ë¬¸ì— ì¢‹ì§€ë§Œ Dataì˜ ì–‘ì´ ì¦ê°€í•˜ê²Œ ë©ë‹ˆë‹¤. ë§Œì•½ ë„ˆë¬´ í¬ê²Œ ìª¼ê°œê²Œ ëœë‹¤ë©´, ì›ë³¸ ë°ì´í„°ë¡œ reconstructí•˜ê¸° í˜ë“¤ì–´ ì§ˆ ê²ƒì…ë‹ˆë‹¤. Sampling theoremìƒ˜í”Œë§ ë ˆì´íŠ¸ê°€ ìµœëŒ€ frequencyì˜ 2ë°° ë³´ë‹¤ ì»¤ì ¸ì•¼ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\\( f_{s} &gt; 2f_{m} \\) ì—¬ê¸°ì„œ \\(f_{s}\\)ëŠ” sampling rate, ê·¸ë¦¬ê³  \\(f_{m}\\)ì€ maximum frequencyë¥¼ ë§í•©ë‹ˆë‹¤. Nyqusit frequency = \\(f_{s}/2\\), sampling rateì˜ ì ˆë°˜ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ Samplingì€ ì¸ê°„ì˜ ì²­ê° ì˜ì—­ì— ë§ê²Œ í˜•ì„±ì´ ë©ë‹ˆë‹¤. Audio CD : 44.1 kHz(44100 sample/second) Speech communication : 8 kHz(8000 sample/second) 12345# library loadimport soundfile as sfimport librosaimport numpy as npimport matplotlib.pyplot as plt 1filename = &quot;./wav/voice.wav&quot; 12345678# íŒŒì¼ ë¡œë“œ ë°©ë²• 1y, sr = sf.read(filename, dtype='int16')print(&quot;Sample Rate: &quot;, sr)print(&quot;DATA: &quot;, type(y), y.shape, len(y), y)dur = len(y) / srprint(&quot;dur : &quot;, dur) Sample Rate: 16000 DATA: &lt;class 'numpy.ndarray'&gt; (48944,) 48944 [ -9 1 -5 ... -20 -16 -24] dur : 3.059 1234567# íŒŒì¼ ë¡œë“œ ë°©ë²• 2y, sr = librosa.load(filename, mono=True, sr=16000)print(&quot;Sample Rate: &quot;, sr)print(&quot;DATA: &quot;, type(y), y.shape, y)dur = len(y) / srprint(&quot;dur : &quot;, dur) Sample Rate: 16000 DATA: &lt;class 'numpy.ndarray'&gt; (48944,) [-2.7465820e-04 3.0517578e-05 -1.5258789e-04 ... -6.1035156e-04 -4.8828125e-04 -7.3242188e-04] dur : 3.059 Resamplingìƒ˜í”Œë§ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œê¸ˆ ë” ë†’ì€ sampling rate í˜¹ì€ ë” ë‚®ì€ sampling rateë¡œ ë‹¤ì‹œ ìƒ˜í”Œë§í• ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•ŒëŠ” ì¼ë°˜ì ìœ¼ë¡œ interpolation(ë³´ê°„)ì„ í• ë•ŒëŠ” low-pass filterë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.(Windowed sinc function) 12import IPython.display as ipdy_8k = librosa.resample(y, sr, 8000) 1ipd.Audio(y_8k, rate=8000) 1len(y_8k) 24472 12# durationlen(y_8k)/8000 3.059 12y_2k = librosa.resample(y, sr, 4000)ipd.Audio(y_2k, rate=2000) 1len(y_2k) 12236 Nomalization &amp; Quantizationì‹œê°„ì˜ ê¸°ì¤€ì´ ì•„ë‹Œ ì‹¤ì œ amplitudeì˜ real valued ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œê·¸ë„ì˜ ê°’ì„ ì¡°ì ˆí•©ë‹ˆë‹¤. Amplitudeë¥¼ ì´ì‚°ì ì¸ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê³ , signal ë°ì´í„°ì˜ Amplitudeë¥¼ ë°˜ì˜¬ë¦¼í•˜ê²Œ ë©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì´ì‚°ì ì¸ êµ¬ê°„ì€ ì–´ë–»ê²Œ ë‚˜ëˆŒìˆ˜ ìˆì„ê¹Œìš”?, bitì˜ ë¹„íŠ¸ì— ì˜í•´ì„œ ê²°ì •ë©ë‹ˆë‹¤. B bitì˜ Quantization : \\(-2^{B-1}\\) ~ \\(2^{B-1}-1\\) Audio CDì˜ Quantization (16 bits) : \\(-2^{15}\\) ~ \\(2^{15}-1\\) ìœ„ ê°’ë“¤ì€ ë³´í†µ -1.0 ~ 1.0 ì˜ì—­ìœ¼ë¡œ scalingë˜ê¸°ë„ í•©ë‹ˆë‹¤. 123# Normalizenormed_wav = y / max(np.abs(y))ipd.Audio(normed_wav, rate=sr) 12345678#quantization í•˜ë©´ ìŒì§ˆì€ ë–¨ì–´ì§€ì§€ë§Œ lightí•œ ìë£Œí˜•ì´ ëœë‹¤.Bit = 8max_value = 2 ** (Bit-1)quantized_8_wav = normed_wav * max_valuequantized_8_wav = np.round(quantized_8_wav).astype(int)quantized_8_wav = np.clip(quantized_8_wav, -max_value, max_value-1)ipd.Audio(quantized_8_wav, rate=sr) mu-law encodingì‚¬ëŒì˜ ê·€ëŠ” ì†Œë¦¬ì˜ amplitudeì— ëŒ€í•´ logì ìœ¼ë¡œ ë°˜ì‘í•©ë‹ˆë‹¤. ì¦‰, ì‘ì€ì†Œë¦¬ì˜ ì°¨ì´ëŠ” ì˜ì¡ì•„ë‚´ëŠ”ë° ë°˜í•´ ì†Œë¦¬ê°€ ì»¤ì§ˆìˆ˜ë¡ ê·¸ ì°¨ì´ë¥¼ ì˜ ëŠë¼ì§€ ëª»í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ íŠ¹ì„±ì„ waveê°’ì„ í‘œí˜„í•˜ëŠ”ë° ë°˜ì˜í•´ì„œ ì‘ì€ê°’ì—ëŠ” ë†’ì€ ë¶„ë³„ë ¥(high resolution)ì„, í°ê°’ë¼ë¦¬ëŠ” ë‚®ì€ ë¶„ë³„ë ¥(low resolution)ì„ ê°–ë„ë¡ í•©ë‹ˆë‹¤ 12def mu_law(x, mu=255): return np.sign(x) * np.log(1 + mu * np.abs(x)) / np.log(1 + mu) 1234567x = np.linspace(-1, 1, 1000)x_mu = mu_law(x)plt.figure(figsize=[6, 4])plt.plot(x)plt.plot(x_mu)plt.show() 12wav_mulaw = mu_law(normed_wav)ipd.Audio(wav_mulaw, rate=sr) to be continuedâ€¦ë’·ì¥ì—ì„œ ê³„ì†ë©ë‹ˆë‹¤.","link":"/2020/12/03/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%ED%95%98%EA%B8%B0-8/"},{"title":"ì¸ê³µì§€ëŠ¥ ê²½ì§„ëŒ€íšŒ í›„ê¸°","text":"ì¸ê³µì§€ëŠ¥ ë¬¸ì œí•´ê²° ê²½ì§„ëŒ€íšŒ ì°¸ê°€í›„ê¸° 2020, 2021 ğŸ”” ì •ë§ ì¹˜ì—´í–ˆë˜ ê²½ì§„ëŒ€íšŒì˜€ë‹¤. ì²˜ìŒì—ëŠ” ì´ë¯¸ì§€ ë©€í‹°ë¼ë²¨ ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œê°€ ë‚˜ì™”ì—ˆê³  ê·¸ ì´í›„ ë³¸ì„ ëŒ€íšŒì—ì„œëŠ” NLP, OCR, GAN ë“± ì—¬ëŸ¬ ë„ë©”ì¸ ë¬¸ì œê°€ ì¶œí˜„í–ˆì—ˆë‹¤ ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œ ê·¸ëŸ°ê±´ ì‚¬ì‹¤ìƒ ì—†ã…‹ìŒã…‹..ğŸ” ê·¸ëƒ¥ ë°ì´í„°ë¥¼ ë§ ê·¸ëŒ€ë¡œ í•´ì²´ ë¶„ì„ì„ í–ˆì–´ì•¼ í–ˆë‹¤. ì•„ë¬´ë¦¬ Data Clensingì„ í•´ë„ ê·¸ëŒ€ë¡œì¸ lossì™€ scoreâ€¦ 8:45ğŸ•› ê²°êµ­ ì•„ì˜ˆ ë¦¬ëª¨ë¸ë§, ë°ì´í„° Cleanize í•œ ë’¤ì— í•™ìŠµì„ ë‹¤ì‹œ ëŒë ¸ë‹¤. ì²œì‹ ë§Œê³  ëì—â€¦ ì°¸ê³ ë¡œ íŒ€ëª…ì€ í’ì„ ë„ìš°ê¸° ;; ëª…ë‹¨ì— ë°œí‘œë˜ê¸°ê¹Œì§€ ì—„ì²­ë‚œ ê¸´ì¥ê°ì´â€¦ğŸŒ¡ğŸ“ˆ","link":"/2020/12/09/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C-%ED%9B%84%EA%B8%B0/"},{"title":"ë”¥ëŸ¬ë‹-ì…ë¬¸ê³¼-ì¤€ë¹„7","text":"ë”¥ëŸ¬ë‹ ì‹œì‘í•´ë³´ê¸°-7í•©ì„±ê³±(Convolution) Convolution? ì •ì˜ í•©ì„±ê³± ì—°ì‚°ì€ ë‘ í•¨ìˆ˜ f, g ê°€ìš´ë° í•˜ë‚˜ì˜ í•¨ìˆ˜ë¥¼ ë°˜ì „(reverse), ì „ì´(shift)ì‹œí‚¨ ë‹¤ìŒ, ë‹¤ë¥¸ í•˜ë‚˜ì˜ í•¨ìˆ˜ì™€ ê³±í•œ ê²°ê³¼ë¥¼ ì ë¶„í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì´ë¥¼ ìˆ˜í•™ ê¸°í˜¸ë¡œ í‘œì‹œí•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. ë˜í•œ g í•¨ìˆ˜ ëŒ€ì‹ ì— f í•¨ìˆ˜ë¥¼ ë°˜ì „, ì „ì´ ì‹œí‚¤ëŠ” ê²½ìš° ë‹¤ìŒê³¼ ê°™ì´ í‘œì‹œí•  ìˆ˜ë„ ìˆë‹¤. ì´ ë‘ ì—°ì‚°ì€ í˜•íƒœëŠ” ë‹¤ë¥´ì§€ë§Œ ê°™ì€ ê²°ê³¼ê°’ì„ ê°–ëŠ”ë‹¤. ìœ„ì˜ ì ë¶„ì—ì„œ ì ë¶„ êµ¬ê°„ì€ í•¨ìˆ˜ fì™€ gê°€ ì •ì˜ëœ ë²”ìœ„ì— ë”°ë¼ì„œ ë‹¬ë¼ì§„ë‹¤. ë˜í•œ ë‘ í™•ë¥  ë³€ìˆ˜ Xì™€ Yê°€ ìˆì„ ë•Œ ê°ê°ì˜ í™•ë¥  ë°€ë„ í•¨ìˆ˜ë¥¼ fì™€ gë¼ê³  í•˜ë©´, X+Yì˜ í™•ë¥  ë°€ë„ í•¨ìˆ˜ëŠ” \\(f * g\\)ë¡œ í‘œì‹œí•  ìˆ˜ ìˆë‹¤. â€“ Wikipedia ë¬´ì—‡ì¸ì§€ ëª¨ë¥´ê² ì£ ? ì‰½ê²Œ ë§í•˜ìë©´ ê¸°ì¡´ MLPì—ì„œëŠ” ì´ë¯¸ì§€ê°€ ì‚´ì§ì´ë¼ë„ íšŒì „ì´ ë˜ê±°ë‚˜ ìœ„ì¹˜ ì´ë™ì´ ìˆë‹¤ë©´ ì‹ ê²½ë§ ìì²´ë¥¼ ë‹¤ì‹œ í•™ìŠµí•´ì•¼ í•˜ì§€ë§Œ CNNì€ ì´ë¯¸ì§€ì˜ ë³€í™”ê°€ ìˆì–´ë„ ì¬í•™ìŠµ ì—†ì–´ë„ ê°€ëŠ¥í•¨. ëª¨ë“  pixelì„ ë¹„êµí•  ê²Œ ì ˆëŒ€ ì•„ë‹˜. Feature ì¶”ì¶œì— ì¤‘ì ì„ ë‘ . ì‚¬ì§„ì— ë³´ì´ëŠ” \\(C_in * C_out\\)ë²ˆì˜ í•©ì„±ê³± ì—°ì‚° biasëŠ” í•˜ë‚˜ì˜ ë²¡í„° Filter(kernel)ì˜ í¬ê¸°ì— ë”°ë¼ ì˜ìƒì˜ í¬ê¸°ê°€ ì¤„ì–´ë“œëŠ” ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ paddingì„ ì“´ë‹¤. í¬ê¸°ê°€ (2N + 1)ì¸ ì»¤ë„ì— ìƒí•˜ì¢Œìš°ì— Nê°œ Zero paddingì„ í•´ì£¼ë©´ ëœë‹¤. Sliding Window ë°©ì‹ìœ¼ë¡œ ì»¤ë„ì´ ì´ë™ë˜ëŠ”ë° ê·¸ í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ë ¤ë©´ Strideë¥¼ ì“´ë‹¤. ë„ˆë¬´ í¬ë©´ ì¶œë ¥ Feature Mapì´ ê³¼ë„í•˜ê²Œ ì¤„ì–´ë“œëŠ” ê²½ìš°ê°€ ë°œìƒí•œë‹¤. ë³´ë‹¤ íš¨ìœ¨ì ì¸ Conv ì—°ì‚°ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” 1x1 Convë¥¼ ë„£ëŠ”ë‹¤ ì—°ì‚°ëŸ‰, íŒŒë¼ë¯¸í„° ê°œìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ BottleNeck êµ¬ì¡°ë¥¼ í™œìš©í•œë‹¤. í•˜í•„ 1x1 ?? 3x3 filter í•œê°œì™€ 1x1 + 3x3 parameter ë¹„êµ ê·¸ë˜ë„ ëª¨ë¥´ê² ë‹¤ë©´?? 12345678910111213141516import numpy as npnp.random((3, 3)).shape == (np.random((3, 1)) * np.random((1, 3)).shape)&gt;&gt; True# keras# k - kernel_size(ex. 3, 5, 7...)# n_filter - number of filters/channels í•„í„° ê°¯ìˆ˜conv1_1 = Conv(n_filters, (1, k))(input_1)conv1_2 = Conv(n_filters, (k, 1))(conv1_1)# ì™œ ë³‘ëª©?conv2 = Conv2D(96, (1, 1), ...)(conv1) # ì¤„ì˜€ë‹¤ê°€(receptive FieldëŠ” ê·¸ëŒ€ë¡œ, Feature mapì„ ë¯¸ë¦¬ ì¤„ì„.)conv3 = Conv2D(96, (3, 3), ...)(conv2) conv4 = Conv2D(128, (1, 1), ...)(conv3) # ë‹¤ì‹œ ëŠ˜ë¦¼ í•­ë“±í–‰ë ¬ì„ ë– ì˜¬ë¦¬ë©´ ì´í•´ê°€ ê°ˆê²ƒì´ë‹¤. CNN ë§Œë“¤ì—ˆëŠ”ë° ë„ˆë¬´ ëŠë¦¬ë„¤? ì–´ë–»ê²Œ í•˜ë©´ ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆì„ê¹Œâ€¦ Conv filterë¥¼ ë” ë„“ê²Œ ì“´ë‹¤. â€“&gt; GPU ì—°ì‚°ì´ ì‰¬ì›Œì§„ë‹¤ 12345# ì´ë ‡ê²Œ ë˜ì–´ìˆëŠ” ê±¸conv = Conv2D(96, (3, 3), ...)(conv)conv = Conv2D(96, (3, 3), ...)(conv)# ì•„ë˜ì²˜ëŸ¼ ë°”ê¾¼ë‹¤.conv = Conv2D(128, (3, 3), ...)(conv) GPUëŠ” ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— í•„í„° ê°¯ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ë”ìš± ë¹¨ë¼ì§„ë‹¤. ì‰½ê²Œ ë§í•˜ë©´ 96ê°œì”© ë‘ë²ˆë³´ë‹¤ 128ê°œì”© í•œë²ˆì´ ë” ë¹ ë¥´ë‹¤. ì„¤ëª…. 96 // 3 = 32 2- layerì„ 1- layerë¡œ ë°”ê¿€ë• 32 // 2 = 16 16^0.5 = 4 4 * 32 = 128 ë˜ ë‹¤ë¥¸ ë°©ë²• ê° ì±„ë„ì—ì„œ ë³„ë„ì˜ 2d convë¥¼ í•˜ëŠ” ë°©ë²• in_channels * channel_multipliter ì¤‘ê°„ì±„ë„ì€ ì—°ê²°ë˜ê³  1x1 convë¡œ out_channelsì— ë§¤í•‘ 1234# Kerasfrom keras.layers import SeparableConv2Dnet = SeparableConv2D(32, (3, 3))(net)# ê·¸ëƒ¥ 2d ì¶œì²˜ : source1 source2","link":"/2020/11/30/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/"},{"title":"ì†Œì†Œí•œ ì—°êµ¬ì•„ë‹Œ ì—°êµ¬","text":"ë„˜ì‚¬ë²½ ë‚œì´ë„ì¸ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ë§ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë³‘ë ¬ ì²˜ë¦¬í•œë‹¤ëŠ” ì ì—ì„œëŠ” ê°€ì¥ ì¢‹ì§€ë§Œ ê·¸ë§Œí¼ ëª¨ë¸ë§ í•˜ê¸°ê°€ ê¹Œë‹¤ë¡­ë‹¤. model input : (Batch, Sequence, feature_num) TransformerëŠ” input / outputì„ ì˜ëª» ì„¤ê³„í•˜ë‹¤ê°„ sequence ì •ë³´ê°€ ë‚ ì•„ê°ˆ ìˆ˜ë„ ìˆê³  ê·¸ëƒ¥ ë°ì´í„° ìì²´ê°€ 8:45 ê°€ ë ìˆ˜ê°€ ìˆë‹¤. ì…ë ¥ ë°ì´í„°ë¥¼ Seqneuceë¡œ Packing í•´ì£¼ì–´ì•¼ í•™ìŠµì´ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ëœë‹¤. Sequence Packing ë°©ë²•ì€ íŒŒì´ì¬ ë¬¸ë²•ìœ¼ë¡œ ë¹„êµì  ì‰½ê²Œ ë˜ì§€ë§Œâ€¦ 12345df_train = train.valuestrain_seq = []for i in range(seq_len, len(df_train)+1): train_seq.append(df_train[i - seq_len: i])train_seq = np.array(train_seq) Sequence unpacking ì€ numpy ì—°ì‚°ì„ ì¨ì•¼ ë¹„êµì  ì‰¬ì›€.. 12out = np.vstack(r for r in train_seq)out = out.unique(out, axis=0) ë‹¤ë³€ëŸ‰ íšŒê·€ì—ì„œ ì–»ì€ êµí›ˆ 1 ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ Time Embeddingâ€¦ ì‹œê°„ë³„ Positional Encoding ë°©ë²•ì„ ì†Œê°œí•˜ì§€. y = wx + b concat sin(wx+b) ì„ í˜•ì˜ ì‹œê°„ íŠ¹ì§•ê³¼ ì£¼ê¸°ì„±ì˜ íŠ¹ì§•ì—ë‹¤ sinì„ ì ìš©í•œ íŠ¹ì§•ì„. 12345678910111213141516171819202122232425262728293031323334353637class Time2Vector(Layer): def __init__(self, seq_len, **kwargs): super(Time2Vector, self).__init__() self.seq_len = seq_len def build(self, input_shape): '''shape (batch, seq_len) í˜•íƒœë¡œ ê°€ì¤‘ì¹˜ì™€ Bias ì´ˆê¸°í™” ''' self.weights_linear = self.add_weight(name='weight_linear', shape=(int(self.seq_len),), initializer='uniform', trainable=True) self.bias_linear = self.add_weight(name='bias_linear', shape=(int(self.seq_len),), initializer='uniform', trainable=True) self.weights_periodic = self.add_weight(name='weight_periodic', shape=(int(self.seq_len),), initializer='uniform', trainable=True) self.bias_periodic = self.add_weight(name='bias_periodic', shape=(int(self.seq_len),), initializer='uniform', trainable=True) def call(self, x): '''ì£¼ê¸°ì„±, ì„ í˜• ì‹œê°„ë³„ íŠ¹ì§•ì„ ê³„ì‚°''' x = tf.math.reduce_mean(x[:,:,:], axis=-1) # ì…ë ¥ Feature ì°¨ì› ìŠ¬ë¼ì´ì‹± time_linear = self.weights_linear * x + self.bias_linear # ì„ í˜• ì‹œê°„ íŠ¹ì§• time_linear = tf.expand_dims(time_linear, axis=-1) # ì°¨ì› ì¶”ê°€ (batch, seq_len, 1) time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic) time_periodic = tf.expand_dims(time_periodic, axis=-1) # ì°¨ì› ì¶”ê°€ (batch, seq_len, 1) return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2) íŠ¸ëœìŠ¤í¬ë¨¸ì˜ íŠ¹ì§• ì‹œê³„ì—´ ë°ì´í„° ë¶„ë¥˜, íšŒê·€ ë¬¸ì œì—ì„œëŠ” Decoderê°€ ë¹ ì ¸ìˆëŠ” Self Attentionì„ ì‚¬ìš©í•œë‹¤. (Fine Tunning) DecoderëŠ” ì±—ë´‡ì´ë‚˜ Auto-Encoderê°™ì€ Encoder-Decoder êµ¬ì¡°ì— ì‚¬ìš©ë˜ëŠ” ê²½í–¥ì´ ìˆë‹¤. ì°¸ê³  ë…¼ë¬¸ : https://openreview.net/forum?id=lE1AB4stmX ë…¼ë¬¸ì—ì„œëŠ” n_layer_num, padding_mask ë¥¼ ì“°ëŠ” íŠ¹ì§•ì´ ìˆë‹¤. ì´ íŒŒë¼ë¯¸í„°ê¹Œì§€ ì“¸ ìˆ˜ ìˆìœ¼ë©´ ì¢‹ê² ì§€ë§Œ GPU ë©”ëª¨ë¦¬ê°€ 8ê¸°ê°€ë°–ì— ì•ˆë˜ì„œ ë…¼ë¬¸ íŒŒë¼ë¯¸í„°ë³´ë‹¤ëŠ” ì‘ê²Œ ì„¤ì •ì„ í•´ì•¼ ì›í™œì´ ê°€ëŠ¥í•˜ë‹¤. ê·¸ë¦¬ê³  datasetì„ window í•˜ë©´ GPU RAM ì‚¬ìš©ëŸ‰ì„ ë” ë‚®ì¶œ ìˆ˜ ìˆë‹¤.","link":"/2021/01/03/%EC%86%8C%EC%86%8C%ED%95%9C-%EC%97%B0%EA%B5%AC%EC%95%84%EB%8B%8C-%EC%97%B0%EA%B5%AC/"}],"tags":[{"name":"Dacon","slug":"Dacon","link":"/tags/Dacon/"},{"name":"Try","slug":"Try","link":"/tags/Try/"},{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"DL","slug":"DL","link":"/tags/DL/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"XR","slug":"XR","link":"/tags/XR/"},{"name":"unity","slug":"unity","link":"/tags/unity/"},{"name":"Unity","slug":"Unity","link":"/tags/Unity/"},{"name":"Linear Algebra","slug":"Linear-Algebra","link":"/tags/Linear-Algebra/"},{"name":"Audio","slug":"Audio","link":"/tags/Audio/"},{"name":"competition","slug":"competition","link":"/tags/competition/"},{"name":"Dev","slug":"Dev","link":"/tags/Dev/"}],"categories":[{"name":"Study","slug":"Study","link":"/categories/Study/"},{"name":"Dacon","slug":"Study/Dacon","link":"/categories/Study/Dacon/"},{"name":"Kaggle, Dacon","slug":"Study/Kaggle-Dacon","link":"/categories/Study/Kaggle-Dacon/"},{"name":"Game","slug":"Study/Game","link":"/categories/Study/Game/"},{"name":"Game","slug":"Game","link":"/categories/Game/"},{"name":"Competition","slug":"Competition","link":"/categories/Competition/"}],"pages":[{"title":"About Xeoner","text":"Intro.HE11ow World!!ì•ˆë…•í•˜ì„¸ìš” ìŒì•…ê³¼ ê²Œì„ì„ ì¢‹ì•„í•˜ëŠ” ê°œë°œìì…ë‹ˆë‹¤ ğŸ•¹ ì €ëŠ” ì´ëŸ° ê²ƒë“¤ì„ ì¶”êµ¬í•˜ê³  ê³ ë¯¼í•©ë‹ˆë‹¤.ğŸ’» ì§ê´€ì ì¸ ì½”ë“œ, í˜¸ê¸°ì‹¬ê³¼ ìˆ˜í•™ì  ì¸ì‚¬ì´íŠ¸ë¡œ ë¬¸ì œí•´ê²°ì €ëŠ” ì´ê²ƒì„ ì¢‹ì•„í•©ë‹ˆë‹¤. ğŸ–¥ íŠ¸ëŸ¬ë¸” ìŠˆíŒ…(ê¸°ë°œí•œ ìˆ˜í•™ì íŠ¸ë¦­ìœ¼ë¡œ ì „ì²˜ë¦¬, ë³µì¡ë„ ë¬¸ì œ í•´ê²°)ì €ëŠ” ì´ê²ƒì„ ì¶”êµ¬í•˜ê³  ì—´ë§í•©ë‹ˆë‹¤. ğŸ–¥ LMM, Tensor Fusion Foundation Model Contacts ğŸ“« EMAIL : next.forr@gmail.com","link":"/about/index.html"}]}