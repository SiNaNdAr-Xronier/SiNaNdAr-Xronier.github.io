<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Category: Study - Xeoner&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Xeoner&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Xeoner&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="some description"><meta property="og:type" content="blog"><meta property="og:title" content="Xeoner&#039;s Blog"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Xeoner&#039;s Blog"><meta property="og:description" content="some description"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="Xeoner"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Xeoner's Blog","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Xeoner"},"publisher":{"@type":"Organization","name":"Xeoner's Blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"some description"}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link data-pjax rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-M6KHR911M6" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-M6KHR911M6');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Xeoner&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories/">Categories</a></li><li class="is-active"><a href="#" aria-current="page">Study</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-29T17:54:54.000Z" title="2020. 11. 30. 오전 2:54:54">2020-11-30</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-09-30T11:43:13.402Z" title="2023. 9. 30. 오후 8:43:13">2023-09-30</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">6 minutes read (About 902 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/30/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/">딥러닝-입문과-준비7</a></p><div class="content"><h2 id="딥러닝-시작해보기-7"><a href="#딥러닝-시작해보기-7" class="headerlink" title="딥러닝 시작해보기-7"></a>딥러닝 시작해보기-7</h2><h3 id="합성곱-Convolution"><a href="#합성곱-Convolution" class="headerlink" title="합성곱(Convolution)"></a>합성곱(Convolution)</h3><p>  Convolution?<br>  <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/conv_1.png" alt="conv">  </p>
<p>  <strong>정의</strong><br>  합성곱 연산은 두 함수 f, g 가운데 하나의 함수를 반전(reverse), 전이(shift)시킨 다음, 다른 하나의 함수와 곱한 결과를 적분하는 것을 의미한다. 이를 수학 기호로 표시하면 다음과 같다.<br>  <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/1.PNG" alt="1"></p>
<p>  또한 g 함수 대신에 f 함수를 반전, 전이 시키는 경우 다음과 같이 표시할 수도 있다. 이 두 연산은 형태는 다르지만 같은 결과값을 갖는다.<br>  <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/2.PNG" alt="2"></p>
<p>  위의 적분에서 적분 구간은 함수 f와 g가 정의된 범위에 따라서 달라진다.</p>
<p>  또한 두 확률 변수 X와 Y가 있을 때 각각의 확률 밀도 함수를 f와 g라고 하면, X+Y의 확률 밀도 함수는 \(f * g\)로 표시할 수 있다. – <a target="_blank" rel="noopener" href="https://ko.wikipedia.org/wiki/%ED%95%A9%EC%84%B1%EA%B3%B1">Wikipedia</a></p>
<blockquote>
<p>무엇인지 모르겠죠? 쉽게 말하자면<br>  기존 MLP에서는 이미지가 살짝이라도 회전이 되거나 위치 이동이 있다면 신경망 자체를 다시 학습해야 하지만<br>  CNN은 이미지의 변화가 있어도 재학습 없어도 가능함.</p>
</blockquote>
<p>  <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/conv_2.PNG" alt="conv2"><br>  모든 pixel을 비교할 게 절대 아님. Feature 추출에 중점을 둠.</p>
<p>  <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/conv_3.PNG" alt="conv3">   </p>
<blockquote>
<p>사진에 보이는 \(C_in * C_out\)번의 합성곱 연산 bias는 하나의 벡터</p>
</blockquote>
<p>  Filter(kernel)의 크기에 따라 영상의 크기가 줄어드는 문제점을 해결하기 위해 padding을 쓴다.<br>  <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/conv_4.PNG" alt="conv4"><br>  크기가 (2N + 1)인 커널에 상하좌우에 N개 Zero padding을 해주면 된다.</p>
<p>  Sliding Window 방식으로 커널이 이동되는데 그 크기를 조절하려면 Stride를 쓴다.<br>  너무 크면 출력 Feature Map이 과도하게 줄어드는 경우가 발생한다.</p>
<h3 id="보다-효율적인-Conv-연산을-하기-위해서는-1x1-Conv를-넣는다"><a href="#보다-효율적인-Conv-연산을-하기-위해서는-1x1-Conv를-넣는다" class="headerlink" title="보다 효율적인 Conv 연산을 하기 위해서는 1x1 Conv를 넣는다"></a>보다 효율적인 Conv 연산을 하기 위해서는 1x1 Conv를 넣는다</h3><p>  연산량, 파라미터 개수를 줄이기 위해 BottleNeck 구조를 활용한다.<br>  하필 1x1 ??<br>  <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%847/conv_5.PNG" alt="conv5"><br>  3x3 filter 한개와 1x1 + 3x3 parameter 비교</p>
<p>  그래도 모르겠다면??</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random((<span class="number">3</span>, <span class="number">3</span>)).shape == (np.random((<span class="number">3</span>, <span class="number">1</span>)) * np.random((<span class="number">1</span>, <span class="number">3</span>)).shape)</span><br><span class="line">&gt;&gt; <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># keras</span></span><br><span class="line"><span class="comment"># k - kernel_size(ex. 3, 5, 7...)</span></span><br><span class="line"><span class="comment"># n_filter - number of filters/channels 필터 갯수</span></span><br><span class="line">conv1_1 = Conv(n_filters, (<span class="number">1</span>, k))(input_1)</span><br><span class="line">conv1_2 = Conv(n_filters, (k, <span class="number">1</span>))(conv1_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 왜 병목?</span></span><br><span class="line">conv2 = Conv2D(<span class="number">96</span>, (<span class="number">1</span>, <span class="number">1</span>), ...)(conv1) </span><br><span class="line"><span class="comment"># 줄였다가(receptive Field는 그대로, Feature map을 미리 줄임.)</span></span><br><span class="line">conv3 = Conv2D(<span class="number">96</span>, (<span class="number">3</span>, <span class="number">3</span>), ...)(conv2) </span><br><span class="line">conv4 = Conv2D(<span class="number">128</span>, (<span class="number">1</span>, <span class="number">1</span>), ...)(conv3) <span class="comment"># 다시 늘림</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>  항등행렬을 떠올리면 이해가 갈것이다.</p>
<h3 id="CNN-만들었는데-너무-느리네-어떻게-하면-빠르게-할-수-있을까…"><a href="#CNN-만들었는데-너무-느리네-어떻게-하면-빠르게-할-수-있을까…" class="headerlink" title="CNN 만들었는데 너무 느리네? 어떻게 하면 빠르게 할 수 있을까…"></a>CNN 만들었는데 너무 느리네? 어떻게 하면 빠르게 할 수 있을까…</h3><p>  Conv filter를 더 넓게 쓴다. –&gt; GPU 연산이 쉬워진다  </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 이렇게 되어있는 걸</span></span><br><span class="line">conv = Conv2D(<span class="number">96</span>, (<span class="number">3</span>, <span class="number">3</span>), ...)(conv)</span><br><span class="line">conv = Conv2D(<span class="number">96</span>, (<span class="number">3</span>, <span class="number">3</span>), ...)(conv)</span><br><span class="line"><span class="comment"># 아래처럼 바꾼다.</span></span><br><span class="line">conv = Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), ...)(conv)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>GPU는 병렬로 처리하기 때문에 필터 갯수를 늘리면 더욱 빨라진다.<br>  쉽게 말하면 96개씩 두번보다 128개씩 한번이 더 빠르다.  </p>
</blockquote>
<p>  설명.</p>
<ol>
<li>96 &#x2F;&#x2F; 3 &#x3D; 32</li>
<li>2- layer을 1- layer로 바꿀땐</li>
<li>32 &#x2F;&#x2F; 2 &#x3D; 16</li>
<li>16^0.5 &#x3D; 4</li>
<li>4 * 32 &#x3D; 128</li>
</ol>
<p>  또 다른 방법<br>  각 채널에서 별도의 2d conv를 하는 방법<br>  in_channels * channel_multipliter 중간채널은 연결되고 1x1 conv로 out_channels에 매핑</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Keras</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> SeparableConv2D</span><br><span class="line">net = SeparableConv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>))(net)</span><br><span class="line"><span class="comment"># 그냥 2d</span></span><br></pre></td></tr></table></figure>

<p>  출처 :<br>  <a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1i0Fwh-d8kF05o4QRfJG5dZt_P7G85MCS">source1</a><br>  <a target="_blank" rel="noopener" href="https://towardsdatascience.com/speeding-up-convolutional-neural-networks-240beac5e30f">source2</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-27T13:54:12.000Z" title="2020. 11. 27. 오후 10:54:12">2020-11-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-09-11T23:26:22.015Z" title="2023. 9. 12. 오전 8:26:22">2023-09-12</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">8 minutes read (About 1209 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/27/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/">딥러닝-입문과-준비6</a></p><div class="content"><h2 id="딥러닝-시작해보기-6"><a href="#딥러닝-시작해보기-6" class="headerlink" title="딥러닝 시작해보기-6"></a>딥러닝 시작해보기-6</h2><h3 id="💥Remind-딥러닝에-비선형-활성화-함수를-사용하는-이유"><a href="#💥Remind-딥러닝에-비선형-활성화-함수를-사용하는-이유" class="headerlink" title="💥Remind!! 딥러닝에 비선형 활성화 함수를 사용하는 이유"></a>💥Remind!! 딥러닝에 비선형 활성화 함수를 사용하는 이유</h3><p>  선형 함수로는 XOR과 같은 non-linear한 문제는 해결이 안됨;;</p>
<blockquote>
<p>그러면 Hidden Layer를 늘리면 되지 않을까?<br>  $f(ax+by) &#x3D; af(x) + bf(y)$ 라는 특징 때문에<br>  N-layer 깊이를 아무리 쌓아도 1-Layer로 동작함.</p>
</blockquote>
<h3 id="최적화-Opt-알고리즘"><a href="#최적화-Opt-알고리즘" class="headerlink" title="최적화(Opt) 알고리즘"></a>최적화(Opt) 알고리즘</h3><ul>
<li><p>경사하강법(GD)<br>$$\theta &#x3D; \theta - \eta \nabla_\theta S(\theta)$$  </p>
<p>Network의 parameter&#x3D;$ \theta $ 로 할때 손실함수 $ J(\theta) $의 값을 최소화하기 위해 기울기<br>$ \nabla J(\theta)$를 이용하는 방법<br>GD에서는 Gradient의 반대 방향으로 일정 크기(lr)만큼 이동하는 것을 반복하여 loss function의 값을 최소화 하는 $ \theta $의 값을 찾음,  </p>
</li>
<li><p>lr(학습률) $ \eta $ 는 보통 1e-3 ~ 1e-4 사이에서 사용함.<br>  너무 크면 global minimum을 지나치고 너무 작으면 Local Minimum에 빠짐.</p>
</li>
<li><p>확률적 경사하강법(SGD)<br>전체 Training set을 사용하는 것을 batch Gradient Descent, 계산량이 많아지는 것을 방지하기 위해<br>mini-batch에 대해서만 손실함수를 계산하는 확률적 GD를 사용함.<br>같은 시간에 더 많은 step를 갈 수 있음, 여러번 반복할 경우 batch의 결과와 비슷함</p>
</li>
<li><p>GD vs SGD<br>GD : 확실한데 너무 느림 | SGD : 조금 헤메지만 빠름</p>
</li>
<li><p>Momentum : 현재 Gradient를 통해 이동하는 방향과 별개로 과거의 이동방식을 기억하면서 일종의 관성을 주는 방식</p>
</li>
<li><p>AdaGrad(Adaptive Gradient)</p>
<ol>
<li>많이 변화했던 변수들은 step size를 작게 하는 것<br>자주 등장하거나 변화를 많이 한 변수들은 optimum에 가까이 있을 확률이 높기 때문에 작은 크기로 이동하면서 미세조절</li>
<li>적게 변화한 변수들은 많이 이동해야할 확률이 높기 때문에 먼저 빠르게 loss값을 줄이는 방식으로 이동하는 방식<br>학습을 계속 진행하면 step size가 너무 줄어드는 단점이 있음.<br><img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_05.PNG" alt="AdaGrad"></li>
</ol>
</li>
<li><p>RMSProp<br>합을 지수평균으로 대체하여 Adagrad의 단점을 해결<br>G가 무한정 커지지는 않으면서 최근 변화량의 변수간 상대적인 크기 차이는 유지할 수 있음.<br><img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_04.PNG" alt="RMsprop"></p>
</li>
<li><p>Adam<br>Momentum + RMSProp</p>
<ol>
<li>지금까지 계산해온 기울기의 지수평균을 저장</li>
<li>rmsprop과 유사하게 Gradient의 제곱값의 지수평균을 저장<br><img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_06.PNG" alt="Adam"></li>
</ol>
</li>
</ul>
<h3 id="Overfitting-과적합"><a href="#Overfitting-과적합" class="headerlink" title="Overfitting(과적합)"></a>Overfitting(과적합)</h3><p>  Training Set의 지엽적인 특성까지 반영해 Variance High로 Training되어서 Training Set을 암기해버리는 현상<br>  <em>Test Set을 잘 예측하지 못함</em><br>  주로 표현력이 높은 모델, 즉 파라미터가 많은 모델에 발생</p>
<ol>
<li>정규화(Regularization)</li>
</ol>
<ul>
<li>손실함수에 가중치의 크기를 포함</li>
<li>가중치가 작아지도록 학습한다는 것은 Outlier(Noise)의 영향을 적게 받음</li>
</ul>
<h4 id="L2-정규화"><a href="#L2-정규화" class="headerlink" title="L2 정규화"></a>L2 정규화</h4><p><img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_08.PNG" alt="L2"></p>
<p><em>Rigde Regression</em></p>
<h4 id="L1-정규화"><a href="#L1-정규화" class="headerlink" title="L1 정규화"></a>L1 정규화</h4><p><img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_09.PNG" alt="L1"><br>Sparse Model에 알맞음.. 작은 가중치들이 거의 0으로 수렴하여 몇개의 중요한 가중치들만 남음. </p>
<p><em>Lasso Regression</em></p>
<p>미분 불가능한 점이 있기 때문에 Gradient-Base Learning에는 주의..<br><img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_10.PNG" alt="L1주의점"></p>
<h4 id="DropOut"><a href="#DropOut" class="headerlink" title="DropOut"></a>DropOut</h4><p>각 레이어의 일정 비율로 뉴런의 출력 값을 0으로 만들어 나머지 뉴런들로 학습하는 방법<br>과적합을 효과적으로 예방 가능(Network 내부의 Ensemble 학습으로 볼 수 있음)</p>
<p>역전파는 ReLU처럼 동작<br>Forward Propagation때 시그널을 통과시킨 뉴런은 Backward때도 통과시킴<br>drop된 뉴런은 Backward Propagation때도 시그널 차단</p>
<p>반면, TEST때는 모든 뉴런에 신호를 전달함</p>
<h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>학습하는 이전 층의 파라미터 변화로 현재층의 입력 분포가 바뀌는 현상을 내부 공분산 변화(Internal Covariate Shift)<br>이전 층의 작은 파라미터 변화가 증폭되어 뒷 레이어에 큰 영향을 받음.<br>그래서…</p>
<p>BN(2015)</p>
<ul>
<li>Gradient Vanishing, Exploding을 방지하는 대표적인 방법</li>
<li>직접적인 방법임.</li>
<li>Training 과정 자체를 안정화시켜 학습속도를 가속화</li>
<li>평균과 분산을 조절하는 과정이 <em>NN 안에 포함</em> 되어 있다는 것이 핵심적</li>
</ul>
<p>Training할때<br>각 Mini Batch마다 $ \gamma $ 와 $ \beta $를 구하고 저장해 둠</p>
<p>Test할때<br>구했던 $ \gamma $  와 $ \beta $ 의 평균을 사용</p>
<p><img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%846/ckpt_11.PNG" alt="BN"></p>
<h4 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h4><p>일종의 Regularization작업, 데이터가 적을 때 사용하면 매우 효과적<br>즉 데이터 변형</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-25T12:49:48.000Z" title="2020. 11. 25. 오후 9:49:48">2020-11-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-09-11T23:21:40.023Z" title="2023. 9. 12. 오전 8:21:40">2023-09-12</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">5 minutes read (About 796 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/25/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/">딥러닝-입문과-준비5</a></p><div class="content"><h2 id="딥러닝-시작해보기-5"><a href="#딥러닝-시작해보기-5" class="headerlink" title="딥러닝 시작해보기-5"></a>딥러닝 시작해보기-5</h2><h3 id="선형대수-배워보기-행렬을-아무리-곱하고-더해도-선모양"><a href="#선형대수-배워보기-행렬을-아무리-곱하고-더해도-선모양" class="headerlink" title="선형대수 배워보기(행렬을 아무리 곱하고 더해도 선모양)"></a>선형대수 배워보기(행렬을 아무리 곱하고 더해도 선모양)</h3><h4 id="Scala-크기만-존재하는-양"><a href="#Scala-크기만-존재하는-양" class="headerlink" title="Scala : 크기만 존재하는 양"></a>Scala : 크기만 존재하는 양</h4><h4 id="Vector-속도-위치이동-힘-공간뒤틀림과-같이-크기와-방향이-모두-존재하는-양"><a href="#Vector-속도-위치이동-힘-공간뒤틀림과-같이-크기와-방향이-모두-존재하는-양" class="headerlink" title="Vector : 속도, 위치이동, 힘, 공간뒤틀림과 같이 크기와 방향이 모두 존재하는 양"></a>Vector : 속도, 위치이동, 힘, 공간뒤틀림과 같이 크기와 방향이 모두 존재하는 양</h4><p><img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/slido51_1.PNG" alt="스칼라와벡터"></p>
<h4 id="Norm-n차원-벡터-vec-x-x-1-x-2-cdots-x-n"><a href="#Norm-n차원-벡터-vec-x-x-1-x-2-cdots-x-n" class="headerlink" title="Norm ? n차원 벡터 $$\vec{x} &#x3D; (x_1, x_2, \cdots x_n)$$"></a>Norm ? n차원 벡터 $$\vec{x} &#x3D; (x_1, x_2, \cdots x_n)$$</h4><p>Norm $$\lVert x \rVert &#x3D; \sqrt{x_1^1 + x_2^2 + \cdots + x_n^2}$$</p>
<blockquote>
<p>“원점 O에서 점\(x_1, x_2, \cdots, x_n\) 까지의 거리”</p>
</blockquote>
<h4 id="내적-Inner-product-Dot-product"><a href="#내적-Inner-product-Dot-product" class="headerlink" title="내적 ?  Inner product, Dot product"></a>내적 ?  Inner product, Dot product</h4><h4 id="행렬끼리-곱할-때는-차원을-주의한다"><a href="#행렬끼리-곱할-때는-차원을-주의한다" class="headerlink" title="행렬끼리 곱할 때는 차원을 주의한다."></a>행렬끼리 곱할 때는 차원을 주의한다.</h4><blockquote>
<p>A(m, n) * B(n, m) 만 가능</p>
</blockquote>
<h4 id="Transpose-전치행렬-행과-열을-뒤바꿈"><a href="#Transpose-전치행렬-행과-열을-뒤바꿈" class="headerlink" title="Transpose: 전치행렬(행과 열을 뒤바꿈)"></a>Transpose: 전치행렬(행과 열을 뒤바꿈)</h4><blockquote>
<p>A.T</p>
</blockquote>
<h3 id="numpy-연산-Element-wise-operation"><a href="#numpy-연산-Element-wise-operation" class="headerlink" title="numpy 연산(Element-wise operation)"></a>numpy 연산(Element-wise operation)</h3><blockquote>
<p>np.dot(x, y) (aka 내적, dot-product)와  x * y(element-wise)는 서로 다름.</p>
</blockquote>
<h3 id="numpy-비교-논리연산-element-wise-operation"><a href="#numpy-비교-논리연산-element-wise-operation" class="headerlink" title="numpy 비교, 논리연산(element-wise operation)"></a>numpy 비교, 논리연산(element-wise operation)</h3><h3 id="numpy-Reductions"><a href="#numpy-Reductions" class="headerlink" title="numpy Reductions"></a>numpy Reductions</h3><blockquote>
<p>argmax() : 최대값있는 인덱스를 리턴, argmin() : 최소값의 인덱스 리턴</p>
</blockquote>
<h3 id="np-all-np-any"><a href="#np-all-np-any" class="headerlink" title="np.all, np.any?"></a>np.all, np.any?</h3><blockquote>
<p>ALL : Array내 모든 값이 TRUE인가?<br>  any : Array내 값이 하나라도 TRUE인가?</p>
</blockquote>
<h3 id="np-mean-np-median-np-std-등-통계함수-사용-가능"><a href="#np-mean-np-median-np-std-등-통계함수-사용-가능" class="headerlink" title="np.mean, np.median, np.std 등 통계함수 사용 가능"></a>np.mean, np.median, np.std 등 통계함수 사용 가능</h3><h4 id="딥러닝에-대한-환상"><a href="#딥러닝에-대한-환상" class="headerlink" title="딥러닝에 대한 환상"></a>딥러닝에 대한 환상</h4><ol>
<li><p>복잡한 문제도 층을 깊고 넓게 쌓으면 해결된다 –&gt; Gradient Vanhshing, Initialize fault 으하하핰ㅋㅋㅋ</p>
</li>
<li><p>$$Sigmoid(z) &#x3D; \frac{1} {1 + e^{-z}}$$<br>  Sigmoid 도함수의 최대값은 1&#x2F;4 … –&gt; 그래서 Gradient Vanishing 나는거임 ㅇㅇ</p>
</li>
</ol>
<h4 id="가중치-초기화"><a href="#가중치-초기화" class="headerlink" title="가중치 초기화"></a>가중치 초기화</h4><ol>
<li><p>초기화의 중요성<br>  $ y &#x3D; Wx+b $ 에서<br>   w가 100, b가 50이라면 x가 0.01이더라도 y는 51이 됨<br>  역전파때 sigmoid 함수 통과시키면 $ Sigmoid’(51) $ 리턴됨<br>  하지만 y가 5만 넘어도 $ Sigmoid (y) $ 는 0에 수렴<br>  –&gt; 이것이 바로 Gradient Vanishing…</p>
</li>
<li><p>그래서 입력층의 가중치w를 모두 0으로 리셋!<br>  Forward Propagation때 두번째 층 뉴런에 모두 같은 값이 전달됨<br>  Backward Propagation때 두째 층 가중치가 모두 똑같이 업데이트 &#x3D;&#x3D;&gt; 신경망 표현력 제한</p>
</li>
<li><p>Bias는 0으로 초기화하는게 일반적으로 효율적</p>
</li>
</ol>
<h4 id="가중치-초기화-2"><a href="#가중치-초기화-2" class="headerlink" title="가중치 초기화 2"></a>가중치 초기화 2</h4><ol>
<li><p>표준 정규분포를 이용한 가중치 초기화<br>  Sigmoid함수의 출력값이 극단적으로(0 or 1)에 치우치는 현상 –&gt; Gradient Vanishing</p>
</li>
<li><p>표준편차를 0.01로 하는 정규분포로 초기화<br>  가중치가 모여 있음 &#x3D;&gt; 기울기 소실 문제 어느정도 완화됨</p>
</li>
</ol>
<h4 id="가중치-초기화-3"><a href="#가중치-초기화-3" class="headerlink" title="가중치 초기화 3"></a>가중치 초기화 3</h4><p>  Xavier초기화 방법(2010)<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn(n_input, n_output) / (n_input) ** <span class="number">0.5</span></span><br></pre></td></tr></table></figure><br>  Sigmoid와 같은 S자 함수의 경우 출력값들이 정규분포 형태이어야 안정적 학습 가능  </p>
<ul>
<li><p>Sigmoid function과 Xavier Init방법을 사용했을 경우 그래프<br>  <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/ckpt_01.PNG" alt="sigmodi"></p>
</li>
<li><p>ReLU 계열 함수에는 적절하지 않음<br>layer를 거쳐갈 수록 0에 수렴(converge)</p>
<p><img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/ckpt_02.PNG" alt="reluNo"></p>
</li>
</ul>
<h4 id="가중치-초기화-4"><a href="#가중치-초기화-4" class="headerlink" title="가중치 초기화 4"></a>가중치 초기화 4</h4><p>  He 초기화 방법(2015)<br>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = np.random.randn(n_input, n_output) / (n_input / <span class="number">2</span>) ** <span class="number">0.5</span></span><br></pre></td></tr></table></figure></p>
<p>  RELU + He init –&gt; 10 layer를 거쳐도 표준편차가 0으로 수렴하지 않음<br>  <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%845/ckpt_03.PNG" alt="relu"></p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>가중치 초기화는 너무나 중요함</li>
<li>tanh의 경우 Xavier Init 방법이 효율적</li>
<li>ReLU계열 함수에는 He Init 방법이 효율적</li>
<li>최근엔 대부분 He Init를 주로 사용</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-24T13:24:21.000Z" title="2020. 11. 24. 오후 10:24:21">2020-11-24</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-09-11T23:27:16.220Z" title="2023. 9. 12. 오전 8:27:16">2023-09-12</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">4 minutes read (About 591 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/">딥러닝-입문과-준비4</a></p><div class="content"><h2 id="딥러닝-시작해보기-4"><a href="#딥러닝-시작해보기-4" class="headerlink" title="딥러닝 시작해보기-4"></a>딥러닝 시작해보기-4</h2><h3 id="인공신경망과-손실함수"><a href="#인공신경망과-손실함수" class="headerlink" title="인공신경망과 손실함수"></a>인공신경망과 손실함수</h3><ol>
<li>인공신경망의 기본 구조</li>
</ol>
<ul>
<li>뇌의 학습방법을 수학적으로 모델링한 기계학습 알고리즘</li>
<li>기본 구조 : y &#x3D; Wx+b<br> <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_1.PNG" alt="인공신경망"><br> $ x_i $ : 입력, $ w_i $: 가중치, b : bias, f: 활성화함수<br> u : 결합(Net), z: 출력</li>
<li>뉴런에는 선형 결합과 활성화 함수 기능이 들어있음</li>
<li>입력층, 은닉층, 출력층으로 구성됨</li>
<li>각 노드의 뉴런 출력은 직접 전달되는 정보에만 의존할 뿐 다른 노드와는 무관</li>
<li>그래서? 병렬처리가 가능함.</li>
</ul>
<ol start="2">
<li>손실 함수(Loss or Cost function)</li>
</ol>
<ul>
<li>신경망의 출력값과 실제 결과값의 차이를 정의하는 함수</li>
<li>신경망 학습목표는 손실함수를 최소화 하는 방향으로 움직여야 함</li>
<li>SGD, Adam 등의 학습 최적화 알고리즘</li>
</ul>
<ol start="3">
<li>손실 함수</li>
</ol>
<ul>
<li><p>회귀(Regression)<br> 제곱 오차(MSE) 사용, 최근에는 rmse, mae의 장점이 있는 Huber Loss 사용하는 추세<br><img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_4.PNG" alt="회귀"></p>
</li>
<li><p>Huber Loss?<br>MAE + MSE -&gt; for Time Series Data!!<br><img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_5.PNG" alt="huber"><br><img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_7.PNG" alt="huber Loss"></p>
</li>
<li><p>분류(Classification)<br> 활성화 함수 : softmax, 손실함수 : cross-entropy<br><img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_2.PNG" alt="분류"></p>
</li>
</ul>
<h3 id="알고리즘과-역전파"><a href="#알고리즘과-역전파" class="headerlink" title="알고리즘과 역전파"></a>알고리즘과 역전파</h3><ol>
<li>학습 알고리즘</li>
</ol>
<ul>
<li>경사 하강법: 기울기를 이용하여 손실함수 S($ \theta $) 값을 최적화</li>
<li>gradient(기울기)의 반대 방향으로 일정 크기만큼 이동하는 것을 반복하여<br> 손실함수의 값을 최소화하는 $ \theta $의 값을 찾음</li>
<li>\[\theta &#x3D; \theta - \eta \nabla_\theta S(\theta)\]</li>
<li>이 떄 $ eta $ 는 미리 정해진 learning rate(step size) 이고 보통 1e-3 ~ 1e-4 정도를 사용</li>
</ul>
<ol start="2">
<li>역전파</li>
</ol>
<ul>
<li><p>계산 그래프</p>
</li>
<li><p>노드는 연산을, 엣지는 데이터의 흐름방향<br> <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_8.PNG" alt="Chain Rule"></p>
</li>
<li><p>sigmoid 함수 역전파<br> <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_9.PNG" alt="sigmoidBP"></p>
</li>
<li><p>합성함수 미분법(Chain Rule)<br> <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_10.PNG" alt="합성미분"></p>
</li>
<li><p>행렬연산과 역전파 1<br> <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_11.PNG" alt="행렬역전파"></p>
</li>
<li><p>이진분류 2-layer NN 역전파<br> <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%844/Letitgo_12.PNG" alt="이진역전파"></p>
</li>
</ul>
<blockquote>
<p>to be continued…</p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-22T19:44:55.000Z" title="2020. 11. 23. 오전 4:44:55">2020-11-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-05T06:19:36.732Z" title="2022. 4. 5. 오후 3:19:36">2022-04-05</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">a few seconds read (About 111 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/23/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%843/">딥러닝-입문과-준비3</a></p><div class="content"><h2 id="딥러닝-시작해보기-3"><a href="#딥러닝-시작해보기-3" class="headerlink" title="딥러닝 시작해보기-3"></a>딥러닝 시작해보기-3</h2><h3 id="차원수-늘리기-줄이기-TF2-x"><a href="#차원수-늘리기-줄이기-TF2-x" class="headerlink" title="차원수 늘리기, 줄이기(TF2.x)"></a>차원수 늘리기, 줄이기(TF2.x)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = tf.expand_dims(x, <span class="number">1</span>)</span><br><span class="line">x.shape <span class="comment"># (x.shape, 1)</span></span><br><span class="line"></span><br><span class="line">x[..., tf.newaxis].shape <span class="comment"># (x.shape, 1)</span></span><br><span class="line"></span><br><span class="line">np.squeeze(x[<span class="number">0</span>]).shape <span class="comment"># x.shape 차원 줄이기</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="TF2-x-Layers"><a href="#TF2-x-Layers" class="headerlink" title="TF2.x Layers"></a>TF2.x Layers</h3><p>Convolution</p>
<ul>
<li>filters : layer에서 출력될때 몇개의 filter</li>
<li>kernel_size : filter(weight) 의 사이즈</li>
<li>strides : 몇 개의 pixel만큼 skip하면서 sliding window 할 것인지</li>
<li>padding : same, zero</li>
<li>activation : 활성화 함수(<em>Linear function은 층을 쌓는 의미가 없다</em>)</li>
</ul>
<blockquote>
<p>to be Continued…</p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-22T10:44:36.000Z" title="2020. 11. 22. 오후 7:44:36">2020-11-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-05T06:19:34.439Z" title="2022. 4. 5. 오후 3:19:34">2022-04-05</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">a minute read (About 117 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%842/">딥러닝 입문과 준비2</a></p><div class="content"><h2 id="딥러닝-시작해보기-2"><a href="#딥러닝-시작해보기-2" class="headerlink" title="딥러닝 시작해보기-2"></a>딥러닝 시작해보기-2</h2><h3 id="Broadcast"><a href="#Broadcast" class="headerlink" title="Broadcast"></a>Broadcast</h3><p>두개의 행렬 shape가 서로 달라도<br>한쪽의 차원이 같거나, 연산하는 값이 한 개일때<br>shape에 맞게 복사해서 연산함</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">6</span>).reshape(-<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># [[0, 1, 2], </span></span><br><span class="line"><span class="comment">#  [3, 4, 5]]</span></span><br><span class="line"></span><br><span class="line">arr + <span class="number">3</span></span><br><span class="line"><span class="comment"># [[3, 4, 5],</span></span><br><span class="line"><span class="comment">#  [6, 7, 8]]</span></span><br><span class="line"></span><br><span class="line">arr * <span class="number">3</span></span><br><span class="line"><span class="comment"># [[0, 3, 6],</span></span><br><span class="line"><span class="comment">#  [9, 12 15]</span></span><br><span class="line"></span><br><span class="line">arr + np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># [[1, 3, 5],</span></span><br><span class="line"><span class="comment">#  [4, 6, 8]]</span></span><br><span class="line"></span><br><span class="line">np.add(arr, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 모든 원소에 1을 더함</span></span><br><span class="line"></span><br><span class="line">np.multiply(arr, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 모든 원소에 3을 곱함</span></span><br></pre></td></tr></table></figure>

<h3 id="argmax-argmin"><a href="#argmax-argmin" class="headerlink" title="argmax, argmin"></a>argmax, argmin</h3><p> 배열의 큰 값이나 작은 값의 index return<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arr = np.array([<span class="number">1</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">54</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">np.argmax(arr) <span class="comment"># 54</span></span><br><span class="line">np.argmin(arr) <span class="comment"># 1</span></span><br><span class="line">np.unique(arr) <span class="comment"># 유일한 값 출력</span></span><br></pre></td></tr></table></figure></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-22T09:45:13.000Z" title="2020. 11. 22. 오후 6:45:13">2020-11-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-05T06:19:24.272Z" title="2022. 4. 5. 오후 3:19:24">2022-04-05</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">2 minutes read (About 362 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/22/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%84/">딥러닝 입문과 준비</a></p><div class="content"><h2 id="딥러닝-시작해보기"><a href="#딥러닝-시작해보기" class="headerlink" title="딥러닝 시작해보기"></a>딥러닝 시작해보기</h2><h3 id="Tensor-이해하기"><a href="#Tensor-이해하기" class="headerlink" title="Tensor 이해하기"></a>Tensor 이해하기</h3><ol>
<li>차원</li>
</ol>
<ul>
<li>0차원(상수) : Scalar값</li>
<li>1차원(리스트 씌운 상수), 2차원(2d), 3차원(3d), 4차원(4-d), n차원(n-d) : Tensor</li>
<li>Numpy로 Tensor 표현과 응용이 가능</li>
</ul>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">arr = np.array([[<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>]])</span><br><span class="line"><span class="built_in">print</span>(arr.dtype) <span class="comment"># dtype(&#x27;float64&#x27;)</span></span><br><span class="line"><span class="built_in">print</span>(arr.shape) <span class="comment"># (2, 3)</span></span><br><span class="line"><span class="built_in">print</span>(arr.size) <span class="comment"># 2 * 3 = 6</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>차원 늘리기와 줄이기</li>
</ol>
<ul>
<li><p>reshape, -1 활용</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr.reshape(-<span class="number">1</span>) <span class="comment"># 1차원으로 펼치기</span></span><br><span class="line">arr.reshape(-<span class="number">1</span>, <span class="number">3</span>) <span class="comment"># 첫번째 차원은 알아서, 두번째 차원은 shape 3</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>Ravel() : arr의 차원을 1로 바꿈(&#x3D;&#x3D;&gt; Flatten)</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]) <span class="comment"># (2, 3)</span></span><br><span class="line">arr.ravel()</span><br><span class="line">arr.shape <span class="comment">#(6, )</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>np.expand_dims() : 값을 유지하고 차원만 늘릴때</p>
</li>
</ul>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr = np.expand_dims(arr, -<span class="number">1</span>) <span class="comment">#(6, 1)</span></span><br><span class="line">arr.shape</span><br></pre></td></tr></table></figure>

<ul>
<li>numpy array를 빠르게 채우는 방법!</li>
</ul>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0으로 채우기</span></span><br><span class="line">arr2 = np.zeros([<span class="number">3</span>, <span class="number">4</span>]) <span class="comment"># 3 * 4의 0이 채워진 배열</span></span><br><span class="line">one2 = np.ones([<span class="number">3</span>, <span class="number">4</span>]) <span class="comment"># 3 * 4의 1로 채워진 배열</span></span><br><span class="line"></span><br><span class="line">five2 = np.ones([<span class="number">3</span>, <span class="number">4</span>]) * <span class="number">5</span> <span class="comment"># 1로 채운 값에 5를 다 곱함</span></span><br><span class="line">arr2 = np.arange(n, m) <span class="comment"># n ~ m-1까지의  수로 배열 채우기</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># array([n ~ m-1])</span></span><br><span class="line">arr = np.arange(<span class="number">5</span>, <span class="number">11</span>).reshape(<span class="number">2</span>, -<span class="number">1</span>) </span><br><span class="line"><span class="comment"># 5 ~ 10 : 6개의 숫자, (2, 3)</span></span><br><span class="line"></span><br><span class="line">arr <span class="comment"># array([5, 6, 7]</span></span><br><span class="line">    <span class="comment">#       [8, 9, 10])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>모양이 맞지 않으면 Error…<br> 5, 6, 7, 8, 9는 5개의 숫자<br> 5 * 1 만 가능한.<br> <img src="/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9E%85%EB%AC%B8%EA%B3%BC-%EC%A4%80%EB%B9%84/slide_lect.PNG" alt="모양다름"></li>
</ul>
<ol start="3">
<li>Index &amp; slicing</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 리스트 인덱스 &amp; 슬라이싱</span></span><br><span class="line"></span><br><span class="line">nums = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">nums[:-<span class="number">1</span>] <span class="comment"># 마지막 숫자 전까지 표시</span></span><br><span class="line"></span><br><span class="line">nums[::-<span class="number">1</span>] <span class="comment"># 리스트 안의 숫자를 거꾸로 표현</span></span><br><span class="line"></span><br><span class="line">nums = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line"><span class="built_in">print</span>(nums[<span class="number">0</span>][<span class="number">1</span>]) = <span class="number">2</span> <span class="comment"># 첫번째 리스트 안의 인덱스가 1인 숫자</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = np.array([<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>])</span><br><span class="line"><span class="built_in">print</span>(arr[<span class="number">1</span>, <span class="number">2</span>]) <span class="comment"># 10 --&gt; 인덱싱 [행, 열]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(arr[<span class="number">1</span>:, <span class="number">1</span>:]) <span class="comment"># [[9, 10]]</span></span><br></pre></td></tr></table></figure>

<ol start="4">
<li>Boolean Indexing<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = np.random.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(data&lt;=<span class="number">0</span>) <span class="comment"># False, True로 나옴</span></span><br><span class="line"></span><br><span class="line">data[data &lt;=<span class="number">0</span>] = <span class="number">1</span> <span class="comment"># 0 이하인 것을 1로 채우다</span></span><br></pre></td></tr></table></figure></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-17T12:28:07.000Z" title="2020. 11. 17. 오후 9:28:07">2020-11-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-04-05T06:21:52.227Z" title="2022. 4. 5. 오후 3:21:52">2022-04-05</time></span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a></span><span class="level-item">5 minutes read (About 687 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/17/GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/">GAN프로젝트_try</a></p><div class="content"><h2 id="Style-GAN-toy-프로젝트"><a href="#Style-GAN-toy-프로젝트" class="headerlink" title="Style GAN toy 프로젝트"></a>Style GAN toy 프로젝트</h2><h3 id="StyleGAN의-특징"><a href="#StyleGAN의-특징" class="headerlink" title="StyleGAN의 특징"></a>StyleGAN의 특징</h3><ol>
<li>이미지를 Style의 조합으로 보고<br>Generator의 각 Layer마다 Style 정보를 입히는 방식으로 이미지 합성<br>이 때 각 Layer에서 추가되는 Style은 이미지의 Coarse Feature(포즈, 성별 등)부터<br>Fine Detail(머리색, 피부톤 등)까지<br>각기 다른 Level의 Visual 속성들을 조절 가능<br>StyleGAN은 생각보다 안정적이고 높은 퀄리티의 이미지 생성</li>
</ol>
<h3 id="네트워크-구조-Module"><a href="#네트워크-구조-Module" class="headerlink" title="네트워크 구조(Module)"></a>네트워크 구조(Module)</h3><ol>
<li>GAN이란 어떤 것일까???<br><img src="/./GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/GANdesc.png" alt="이것이 GAN"></li>
</ol>
<p><img src="/./GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/slide21.PNG" alt="GAN model"></p>
<ul>
<li>Instance Norm?<br><img src="/./GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/slide22.PNG" alt="Instance Norm"></li>
</ul>
<p><img src="/./GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/slide23.PNG" alt="Instance Norm2"></p>
<ul>
<li>Generator 구조 설명</li>
</ul>
<p><img src="/./GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/slide322.png" alt="model"></p>
<p>왼쪽이 Traditional Network, 오른쪽이 이 논문에서 제안한 Style-gased Generator. </p>
<p>왼쪽 네트워크와 오른쪽에 Synthesis Network가 똑같은 구조를 갖고 있지만,<br>이전 GAN에서는 Latent z를 바로 Input으로 넣어줬던 것과는 다르게,<br>StyleGAN에서는 학습된 Constant, (w) 값을 입력으로 사용함. </p>
<p>새롭게 Mapping Network와 Noise가 추가됨..</p>
<p>W를 Feature에 매핑하는 경우<br>W는 Z처럼 고정된 분포를 따르지 않음. </p>
<p>Sampling density는 학습된 Piecewise Continuous Mapping f(z)<br>(f는 Mapping Network 입니다)에 의해 정해짐. </p>
<p>따라서, Warping(틀어짐)이 많이 일어나지 않음.<br>그렇기 때문에 Factors of variation은 더욱 Linear하고, Disentangled (얽히지 않음).<br>이것이 바로 z를 곧바로 Feature에 매핑하는 것보다 w에 매핑하는 것의 장점입니다</p>
<p>기존의 Generator (a)는<br>Input Latent Vector (z)가 직접 Convolution, Upsampling 등을 거쳐 이미지로 변환되는 구조. </p>
<p>Style-based Generator (b) 의 경우,<br>(z)가  Fully-connected Layer로 구성된 Mapping Network을 거쳐<br>Intermediate Latent Vector (w) 먼저 변환. </p>
<p>(w)는 Constant Tensor가 이미지로 변환되는 과정에서<br>스타일을 입히는 역할을 수행.</p>
<p>다양한 스타일의 이미지를 생성.</p>
<ul>
<li>Style Transfer를 실시간으로 가능케하는 Adaptive Instance Norm</li>
</ul>
<p>Synthesis Network (합성 네트워크)<br>z를 중간 latent space W에 매핑을 한 뒤에 이 w는 “A”를 거쳐서 style, y&#x3D;(ys,yb)<br>로 변형됨. 이때 A는 학습된 affine transform 임. 그리고 이 style들은<br>AdaIN(adaptive instance normalization) opeartion을 control 함.<br><img src="/./GAN%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-try/Adain.png" alt="AdaIN"></p>
<p>AdaIN은 style transfer를 할 때 많이 쓰이는 방법으로, 임의의 style transfer를 실시간으로 가능하게 함.<br>여기서 feature map xi는<br>normalized 된 다음에, style로 변환된 두 y로 scaled, biased 됨. (style이 입혀짐)<br>이 과정을 매 layer 마다 반복함. 그리고 이러한 방법은 scale-specific control 을 가능하게 함.</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/Study/">Previous</a></div><div class="pagination-next is-invisible is-hidden-mobile"><a href="/categories/Study/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/Study/">1</a></li><li><a class="pagination-link is-current" href="/categories/Study/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="MELoDy-sigmax"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">MELoDy-sigmax</p><p class="is-size-6 is-block">Musical Data blog</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Seoul</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">23</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">12</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Competition/"><span class="level-start"><span class="level-item">Competition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Game/"><span class="level-start"><span class="level-item">Game</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/"><span class="level-start"><span class="level-item">Study</span></span><span class="level-end"><span class="level-item tag">18</span></span></a><ul><li><a class="level is-mobile" href="/categories/Study/Dacon/"><span class="level-start"><span class="level-item">Dacon</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/Game/"><span class="level-start"><span class="level-item">Game</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/Kaggle-Dacon/"><span class="level-start"><span class="level-item">Kaggle, Dacon</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-10-06T01:36:26.000Z">2023-10-06</time></p><p class="title"><a href="/2023/10/06/music2motion-project/">music2motion_project</a></p><p class="categories"><a href="/categories/Study/">Study</a> / <a href="/categories/Study/Game/">Game</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-15T23:28:46.000Z">2023-03-16</time></p><p class="title"><a href="/2023/03/16/%EA%B2%8C%EC%9E%84%EA%B0%9C%EB%B0%9C-%EC%9D%BC%EA%B8%B02/">게임개발-일기2</a></p><p class="categories"><a href="/categories/Game/">Game</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-16T09:23:41.000Z">2022-10-16</time></p><p class="title"><a href="/2022/10/16/rl-video-summ/">rl-video-summ</a></p><p class="categories"><a href="/categories/Study/">Study</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-05T07:26:50.000Z">2022-04-05</time></p><p class="title"><a href="/2022/04/05/hello-world/">Hello World</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-05T07:26:50.000Z">2022-04-05</time></p><p class="title"><a href="/2022/04/05/bbackcheem/">bbackcheem</a></p></div></article></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">September 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/07/"><span class="level-start"><span class="level-item">July 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/06/"><span class="level-start"><span class="level-item">June 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Audio/"><span class="tag">Audio</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DL/"><span class="tag">DL</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dacon/"><span class="tag">Dacon</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Dev/"><span class="tag">Dev</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Algebra/"><span class="tag">Linear Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Try/"><span class="tag">Try</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Unity/"><span class="tag">Unity</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/XR/"><span class="tag">XR</span><span class="tag">2</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Xeoner&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2024 Xeoner</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2024</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>